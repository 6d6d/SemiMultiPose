{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import cv2\n",
    "\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "# this makes our figures bigger\n",
    "pylab.rcParams['figure.figsize'] = 20, 12\n",
    "\n",
    "from maskrcnn_benchmark.config import cfg\n",
    "from maskrcnn_benchmark.structures.keypoint import keypoints_to_heat_map\n",
    "from maskrcnn_benchmark.modeling.roi_heads.keypoint_head.loss import project_keypoints_to_heatmap\n",
    "#from predictor import COCODemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before\n",
      "pup2.py\n",
      "combined\n",
      "version:  1\n",
      "alpha:  0\n",
      "loss type:  combined\n",
      "0\n",
      "all\n",
      "after\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python pup2.py \"combined\" \"all\" \"1\" \"0\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before\n",
      "train_bee_250.py\n",
      "standard\n",
      "250\n",
      "after\n",
      "2021-05-27 06:45:37,907 maskrcnn_benchmark INFO: Using 1 GPUs\n",
      "2021-05-27 06:45:37,907 maskrcnn_benchmark INFO: AMP_VERBOSE: False\n",
      "DATALOADER:\n",
      "  ASPECT_RATIO_GROUPING: True\n",
      "  NUM_WORKERS: 2\n",
      "  SIZE_DIVISIBILITY: 32\n",
      "DATASETS:\n",
      "  TEST: ('keypoints_coco_2014_minival',)\n",
      "  TRAIN: ('bee_train_cocostyle_small', 'bee_train_cocostyle_small')\n",
      "DATATYPE: pup\n",
      "DTYPE: float32\n",
      "INPUT:\n",
      "  BRIGHTNESS: 0.0\n",
      "  CONTRAST: 0.0\n",
      "  CROP_SIZE: 800\n",
      "  FLIP_PROB_TRAIN: 0.5\n",
      "  HUE: 0.0\n",
      "  MAX_SIZE_TEST: 1333\n",
      "  MAX_SIZE_TRAIN: 2666\n",
      "  MIN_SIZE_RANGE_TRAIN: (480, 1600)\n",
      "  MIN_SIZE_TEST: 800\n",
      "  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)\n",
      "  PIXEL_MEAN: [102.9801, 115.9465, 122.7717]\n",
      "  PIXEL_STD: [1.0, 1.0, 1.0]\n",
      "  SATURATION: 0.0\n",
      "  TO_BGR255: True\n",
      "MODEL:\n",
      "  ANIMAL_ON: False\n",
      "  BACKBONE:\n",
      "    CONV_BODY: R-50-FPN-RETINANET\n",
      "    FREEZE_CONV_BODY_AT: 2\n",
      "    USE_GN: False\n",
      "  CENTROID: False\n",
      "  CLS_AGNOSTIC_BBOX_REG: False\n",
      "  DEVICE: cuda\n",
      "  FBNET:\n",
      "    ARCH: default\n",
      "    ARCH_DEF: \n",
      "    BN_TYPE: bn\n",
      "    DET_HEAD_BLOCKS: []\n",
      "    DET_HEAD_LAST_SCALE: 1.0\n",
      "    DET_HEAD_STRIDE: 0\n",
      "    DW_CONV_SKIP_BN: True\n",
      "    DW_CONV_SKIP_RELU: True\n",
      "    KPTS_HEAD_BLOCKS: []\n",
      "    KPTS_HEAD_LAST_SCALE: 0.0\n",
      "    KPTS_HEAD_STRIDE: 0\n",
      "    MASK_HEAD_BLOCKS: []\n",
      "    MASK_HEAD_LAST_SCALE: 0.0\n",
      "    MASK_HEAD_STRIDE: 0\n",
      "    RPN_BN_TYPE: \n",
      "    RPN_HEAD_BLOCKS: 0\n",
      "    SCALE_FACTOR: 1.0\n",
      "    WIDTH_DIVISOR: 1\n",
      "  FCOS:\n",
      "    FPN_STRIDES: [8, 16, 32, 64, 128]\n",
      "    INFERENCE_TH: 0.05\n",
      "    LOSS_ALPHA: 0.25\n",
      "    LOSS_GAMMA: 2.0\n",
      "    NMS_TH: 0.4\n",
      "    NUM_CLASSES: 2\n",
      "    NUM_CONVS: 4\n",
      "    PRE_NMS_TOP_N: 1000\n",
      "    PRIOR_PROB: 0.01\n",
      "  FCOS_ON: True\n",
      "  FPN:\n",
      "    USE_GN: False\n",
      "    USE_RELU: False\n",
      "  GROUP_NORM:\n",
      "    DIM_PER_GP: -1\n",
      "    EPSILON: 1e-05\n",
      "    NUM_GROUPS: 32\n",
      "  HEATMAPS_LOSS_WEIGHT: 4.0\n",
      "  KEYPOINT_ON: True\n",
      "  MASK_ON: False\n",
      "  META_ARCHITECTURE: GeneralizedRCNN\n",
      "  RESNETS:\n",
      "    BACKBONE_OUT_CHANNELS: 256\n",
      "    DEFORMABLE_GROUPS: 1\n",
      "    NUM_GROUPS: 1\n",
      "    RES2_OUT_CHANNELS: 256\n",
      "    RES5_DILATION: 1\n",
      "    STAGE_WITH_DCN: (False, False, False, False)\n",
      "    STEM_FUNC: StemWithFixedBatchNorm\n",
      "    STEM_OUT_CHANNELS: 64\n",
      "    STRIDE_IN_1X1: True\n",
      "    TRANS_FUNC: BottleneckWithFixedBatchNorm\n",
      "    WIDTH_PER_GROUP: 64\n",
      "    WITH_MODULATED_DCN: False\n",
      "  RETINANET:\n",
      "    ANCHOR_SIZES: (32, 64, 128, 256, 512)\n",
      "    ANCHOR_STRIDES: (8, 16, 32, 64, 128)\n",
      "    ASPECT_RATIOS: (0.5, 1.0, 2.0)\n",
      "    BBOX_REG_BETA: 0.11\n",
      "    BBOX_REG_WEIGHT: 4.0\n",
      "    BG_IOU_THRESHOLD: 0.4\n",
      "    FG_IOU_THRESHOLD: 0.5\n",
      "    INFERENCE_TH: 0.05\n",
      "    LOSS_ALPHA: 0.25\n",
      "    LOSS_GAMMA: 2.0\n",
      "    NMS_TH: 0.4\n",
      "    NUM_CLASSES: 81\n",
      "    NUM_CONVS: 4\n",
      "    OCTAVE: 2.0\n",
      "    PRE_NMS_TOP_N: 1000\n",
      "    PRIOR_PROB: 0.01\n",
      "    SCALES_PER_OCTAVE: 3\n",
      "    STRADDLE_THRESH: 0\n",
      "    USE_C5: False\n",
      "  RETINANET_ON: False\n",
      "  ROI_BOX_HEAD:\n",
      "    CONV_HEAD_DIM: 256\n",
      "    DILATION: 1\n",
      "    FEATURE_EXTRACTOR: ResNet50Conv5ROIFeatureExtractor\n",
      "    MLP_HEAD_DIM: 1024\n",
      "    NUM_CLASSES: 81\n",
      "    NUM_STACKED_CONVS: 4\n",
      "    POOLER_RESOLUTION: 14\n",
      "    POOLER_SAMPLING_RATIO: 0\n",
      "    POOLER_SCALES: (0.0625,)\n",
      "    PREDICTOR: FastRCNNPredictor\n",
      "    USE_GN: False\n",
      "  ROI_HEADS:\n",
      "    BATCH_SIZE_PER_IMAGE: 1\n",
      "    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)\n",
      "    BG_IOU_THRESHOLD: 0.5\n",
      "    DETECTIONS_PER_IMG: 100\n",
      "    FG_IOU_THRESHOLD: 0.5\n",
      "    NMS: 0.5\n",
      "    POSITIVE_FRACTION: 0.25\n",
      "    SCORE_THRESH: 0.05\n",
      "    USE_FPN: False\n",
      "  ROI_KEYPOINT_HEAD:\n",
      "    CONV_LAYERS: (512, 512, 512, 512, 512, 512, 512, 512)\n",
      "    FEATURE_EXTRACTOR: KeypointRCNNFeatureExtractor\n",
      "    MLP_HEAD_DIM: 1024\n",
      "    NUM_CLASSES: 17\n",
      "    POOLER_RESOLUTION: 14\n",
      "    POOLER_SAMPLING_RATIO: 0\n",
      "    POOLER_SCALES: (0.0625,)\n",
      "    PREDICTOR: KeypointRCNNPredictor\n",
      "    RESOLUTION: 14\n",
      "    SHARE_BOX_FEATURE_EXTRACTOR: True\n",
      "  ROI_MASK_HEAD:\n",
      "    CONV_LAYERS: (256, 256, 256, 256)\n",
      "    DILATION: 1\n",
      "    FEATURE_EXTRACTOR: ResNet50Conv5ROIFeatureExtractor\n",
      "    MLP_HEAD_DIM: 1024\n",
      "    POOLER_RESOLUTION: 14\n",
      "    POOLER_SAMPLING_RATIO: 0\n",
      "    POOLER_SCALES: (0.0625,)\n",
      "    POSTPROCESS_MASKS: False\n",
      "    POSTPROCESS_MASKS_THRESHOLD: 0.5\n",
      "    PREDICTOR: MaskRCNNC4Predictor\n",
      "    RESOLUTION: 14\n",
      "    SHARE_BOX_FEATURE_EXTRACTOR: True\n",
      "    USE_GN: False\n",
      "  RPN:\n",
      "    ANCHOR_SIZES: (32, 64, 128, 256, 512)\n",
      "    ANCHOR_STRIDE: (16,)\n",
      "    ASPECT_RATIOS: (0.5, 1.0, 2.0)\n",
      "    BATCH_SIZE_PER_IMAGE: 1\n",
      "    BG_IOU_THRESHOLD: 0.3\n",
      "    FG_IOU_THRESHOLD: 0.7\n",
      "    FPN_POST_NMS_PER_BATCH: True\n",
      "    FPN_POST_NMS_TOP_N_TEST: 2000\n",
      "    FPN_POST_NMS_TOP_N_TRAIN: 2000\n",
      "    MIN_SIZE: 0\n",
      "    NMS_THRESH: 0.7\n",
      "    POSITIVE_FRACTION: 0.5\n",
      "    POST_NMS_TOP_N_TEST: 1000\n",
      "    POST_NMS_TOP_N_TRAIN: 2000\n",
      "    PRE_NMS_TOP_N_TEST: 6000\n",
      "    PRE_NMS_TOP_N_TRAIN: 12000\n",
      "    RPN_HEAD: SingleConvRPNHead\n",
      "    STRADDLE_THRESH: 0\n",
      "    USE_FPN: False\n",
      "  RPN_ONLY: True\n",
      "  WEIGHT: ../../.torch/models/resnet50_lpf3.pth\n",
      "OUTPUT_DIR: training_dir/250/standardwhee/1/fcos_kps_ms_training_R_50_FPN_1x_bee\n",
      "PATHS_CATALOG: /share/ctn/users/bsb2144/directpose/maskrcnn_benchmark/config/paths_catalog.py\n",
      "SOLVER:\n",
      "  BASE_LR: 0.01\n",
      "  BIAS_LR_FACTOR: 2\n",
      "  CHECKPOINT_PERIOD: 2500\n",
      "  GAMMA: 0.1\n",
      "  IMS_PER_BATCH: 3\n",
      "  KPS_GRAD_MULT: 10.0\n",
      "  MAX_GRAD_NORM: 5.0\n",
      "  MAX_ITER: 180000\n",
      "  MOMENTUM: 0.9\n",
      "  POWER: 1.0\n",
      "  STEPS: (60000, 80000)\n",
      "  WARMUP_FACTOR: 0.3333333333333333\n",
      "  WARMUP_ITERS: 500\n",
      "  WARMUP_METHOD: constant\n",
      "  WEIGHT_DECAY: 0.0001\n",
      "  WEIGHT_DECAY_BIAS: 0\n",
      "TEST:\n",
      "  DETECTIONS_PER_IMG: 100\n",
      "  EXPECTED_RESULTS: []\n",
      "  EXPECTED_RESULTS_SIGMA_TOL: 4\n",
      "  IMS_PER_BATCH: 8\n",
      "2021-05-27 06:45:37,910 maskrcnn_benchmark INFO: Collecting env info (might take some time)\n",
      "2021-05-27 06:45:42,390 maskrcnn_benchmark INFO: \n",
      "PyTorch version: 1.0.0.dev20190328\n",
      "Is debug build: No\n",
      "CUDA used to build PyTorch: 10.0.130\n",
      "\n",
      "OS: CentOS Linux 7 (Core)\n",
      "GCC version: (GCC) 5.2.0\n",
      "CMake version: Could not collect\n",
      "\n",
      "Python version: 3.7\n",
      "Is CUDA available: Yes\n",
      "CUDA runtime version: Could not collect\n",
      "GPU models and configuration: GPU 0: GeForce GTX 1080 Ti\n",
      "Nvidia driver version: 460.27.04\n",
      "cuDNN version: Could not collect\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] numpy==1.19.2\n",
      "[pip3] torch==1.0.0.dev20190328\n",
      "[pip3] torchvision==0.2.2\n",
      "[conda] blas                      1.0                         mkl  \n",
      "[conda] mkl                       2020.2                      256  \n",
      "[conda] mkl-service               2.3.0            py37he8ac12f_0  \n",
      "[conda] mkl_fft                   1.2.0            py37h23d657b_0  \n",
      "[conda] mkl_random                1.1.1            py37h0573a6f_0  \n",
      "[conda] pytorch-nightly           1.0.0.dev20190328 py3.7_cuda10.0.130_cudnn7.4.2_0    pytorch\n",
      "[conda] torchvision               0.2.2                    pypi_0    pypi\n",
      "        Pillow (8.0.1)\n",
      "2021-05-27 06:45:42,391 maskrcnn_benchmark INFO: Loaded configuration file /home/bsb2144/directpose/configs/fcos/fcos_kps_ms_training_R_50_FPN_1x.yaml\n",
      "2021-05-27 06:45:42,391 maskrcnn_benchmark INFO: \n",
      "MODEL:\n",
      "  META_ARCHITECTURE: \"GeneralizedRCNN\"\n",
      "  WEIGHT: \"catalog://ImageNetPretrained/MSRA/R-50\"\n",
      "  RPN_ONLY: True\n",
      "  FCOS_ON: True\n",
      "  KEYPOINT_ON: True\n",
      "  BACKBONE:\n",
      "    CONV_BODY: \"R-50-FPN-RETINANET\"\n",
      "  RESNETS:\n",
      "    BACKBONE_OUT_CHANNELS: 256\n",
      "    STAGE_WITH_DCN: (False, False, False, False)\n",
      "    WITH_MODULATED_DCN: False\n",
      "    DEFORMABLE_GROUPS: 1\n",
      "  RETINANET:\n",
      "    USE_C5: False # FCOS uses P5 instead of C5\n",
      "  FCOS:\n",
      "    NUM_CLASSES: 2\n",
      "DATASETS:\n",
      "  TRAIN: (\"keypoints_coco_2014_train\", \"keypoints_coco_2014_valminusminival\")\n",
      "  TEST: (\"keypoints_coco_2014_minival\",)\n",
      "INPUT:\n",
      "  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)\n",
      "  MAX_SIZE_TRAIN: 1333\n",
      "  MIN_SIZE_TEST: 800\n",
      "  MAX_SIZE_TEST: 1333\n",
      "DATALOADER:\n",
      "  SIZE_DIVISIBILITY: 32\n",
      "SOLVER:\n",
      "  BASE_LR: 0.01\n",
      "  WEIGHT_DECAY: 0.0001\n",
      "  STEPS: (60000, 80000)\n",
      "  MAX_ITER: 90000\n",
      "  IMS_PER_BATCH: 16\n",
      "  WARMUP_METHOD: \"constant\"\n",
      "\n",
      "2021-05-27 06:45:42,392 maskrcnn_benchmark INFO: Running with config:\n",
      "AMP_VERBOSE: False\n",
      "DATALOADER:\n",
      "  ASPECT_RATIO_GROUPING: True\n",
      "  NUM_WORKERS: 2\n",
      "  SIZE_DIVISIBILITY: 32\n",
      "DATASETS:\n",
      "  TEST: ('keypoints_coco_2014_minival',)\n",
      "  TRAIN: ('bee_train_cocostyle_small', 'bee_train_cocostyle_small')\n",
      "DATATYPE: pup\n",
      "DTYPE: float32\n",
      "INPUT:\n",
      "  BRIGHTNESS: 0.0\n",
      "  CONTRAST: 0.0\n",
      "  CROP_SIZE: 800\n",
      "  FLIP_PROB_TRAIN: 0.5\n",
      "  HUE: 0.0\n",
      "  MAX_SIZE_TEST: 1333\n",
      "  MAX_SIZE_TRAIN: 2666\n",
      "  MIN_SIZE_RANGE_TRAIN: (480, 1600)\n",
      "  MIN_SIZE_TEST: 800\n",
      "  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)\n",
      "  PIXEL_MEAN: [102.9801, 115.9465, 122.7717]\n",
      "  PIXEL_STD: [1.0, 1.0, 1.0]\n",
      "  SATURATION: 0.0\n",
      "  TO_BGR255: True\n",
      "MODEL:\n",
      "  ANIMAL_ON: False\n",
      "  BACKBONE:\n",
      "    CONV_BODY: R-50-FPN-RETINANET\n",
      "    FREEZE_CONV_BODY_AT: 2\n",
      "    USE_GN: False\n",
      "  CENTROID: False\n",
      "  CLS_AGNOSTIC_BBOX_REG: False\n",
      "  DEVICE: cuda\n",
      "  FBNET:\n",
      "    ARCH: default\n",
      "    ARCH_DEF: \n",
      "    BN_TYPE: bn\n",
      "    DET_HEAD_BLOCKS: []\n",
      "    DET_HEAD_LAST_SCALE: 1.0\n",
      "    DET_HEAD_STRIDE: 0\n",
      "    DW_CONV_SKIP_BN: True\n",
      "    DW_CONV_SKIP_RELU: True\n",
      "    KPTS_HEAD_BLOCKS: []\n",
      "    KPTS_HEAD_LAST_SCALE: 0.0\n",
      "    KPTS_HEAD_STRIDE: 0\n",
      "    MASK_HEAD_BLOCKS: []\n",
      "    MASK_HEAD_LAST_SCALE: 0.0\n",
      "    MASK_HEAD_STRIDE: 0\n",
      "    RPN_BN_TYPE: \n",
      "    RPN_HEAD_BLOCKS: 0\n",
      "    SCALE_FACTOR: 1.0\n",
      "    WIDTH_DIVISOR: 1\n",
      "  FCOS:\n",
      "    FPN_STRIDES: [8, 16, 32, 64, 128]\n",
      "    INFERENCE_TH: 0.05\n",
      "    LOSS_ALPHA: 0.25\n",
      "    LOSS_GAMMA: 2.0\n",
      "    NMS_TH: 0.4\n",
      "    NUM_CLASSES: 2\n",
      "    NUM_CONVS: 4\n",
      "    PRE_NMS_TOP_N: 1000\n",
      "    PRIOR_PROB: 0.01\n",
      "  FCOS_ON: True\n",
      "  FPN:\n",
      "    USE_GN: False\n",
      "    USE_RELU: False\n",
      "  GROUP_NORM:\n",
      "    DIM_PER_GP: -1\n",
      "    EPSILON: 1e-05\n",
      "    NUM_GROUPS: 32\n",
      "  HEATMAPS_LOSS_WEIGHT: 4.0\n",
      "  KEYPOINT_ON: True\n",
      "  MASK_ON: False\n",
      "  META_ARCHITECTURE: GeneralizedRCNN\n",
      "  RESNETS:\n",
      "    BACKBONE_OUT_CHANNELS: 256\n",
      "    DEFORMABLE_GROUPS: 1\n",
      "    NUM_GROUPS: 1\n",
      "    RES2_OUT_CHANNELS: 256\n",
      "    RES5_DILATION: 1\n",
      "    STAGE_WITH_DCN: (False, False, False, False)\n",
      "    STEM_FUNC: StemWithFixedBatchNorm\n",
      "    STEM_OUT_CHANNELS: 64\n",
      "    STRIDE_IN_1X1: True\n",
      "    TRANS_FUNC: BottleneckWithFixedBatchNorm\n",
      "    WIDTH_PER_GROUP: 64\n",
      "    WITH_MODULATED_DCN: False\n",
      "  RETINANET:\n",
      "    ANCHOR_SIZES: (32, 64, 128, 256, 512)\n",
      "    ANCHOR_STRIDES: (8, 16, 32, 64, 128)\n",
      "    ASPECT_RATIOS: (0.5, 1.0, 2.0)\n",
      "    BBOX_REG_BETA: 0.11\n",
      "    BBOX_REG_WEIGHT: 4.0\n",
      "    BG_IOU_THRESHOLD: 0.4\n",
      "    FG_IOU_THRESHOLD: 0.5\n",
      "    INFERENCE_TH: 0.05\n",
      "    LOSS_ALPHA: 0.25\n",
      "    LOSS_GAMMA: 2.0\n",
      "    NMS_TH: 0.4\n",
      "    NUM_CLASSES: 81\n",
      "    NUM_CONVS: 4\n",
      "    OCTAVE: 2.0\n",
      "    PRE_NMS_TOP_N: 1000\n",
      "    PRIOR_PROB: 0.01\n",
      "    SCALES_PER_OCTAVE: 3\n",
      "    STRADDLE_THRESH: 0\n",
      "    USE_C5: False\n",
      "  RETINANET_ON: False\n",
      "  ROI_BOX_HEAD:\n",
      "    CONV_HEAD_DIM: 256\n",
      "    DILATION: 1\n",
      "    FEATURE_EXTRACTOR: ResNet50Conv5ROIFeatureExtractor\n",
      "    MLP_HEAD_DIM: 1024\n",
      "    NUM_CLASSES: 81\n",
      "    NUM_STACKED_CONVS: 4\n",
      "    POOLER_RESOLUTION: 14\n",
      "    POOLER_SAMPLING_RATIO: 0\n",
      "    POOLER_SCALES: (0.0625,)\n",
      "    PREDICTOR: FastRCNNPredictor\n",
      "    USE_GN: False\n",
      "  ROI_HEADS:\n",
      "    BATCH_SIZE_PER_IMAGE: 1\n",
      "    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)\n",
      "    BG_IOU_THRESHOLD: 0.5\n",
      "    DETECTIONS_PER_IMG: 100\n",
      "    FG_IOU_THRESHOLD: 0.5\n",
      "    NMS: 0.5\n",
      "    POSITIVE_FRACTION: 0.25\n",
      "    SCORE_THRESH: 0.05\n",
      "    USE_FPN: False\n",
      "  ROI_KEYPOINT_HEAD:\n",
      "    CONV_LAYERS: (512, 512, 512, 512, 512, 512, 512, 512)\n",
      "    FEATURE_EXTRACTOR: KeypointRCNNFeatureExtractor\n",
      "    MLP_HEAD_DIM: 1024\n",
      "    NUM_CLASSES: 17\n",
      "    POOLER_RESOLUTION: 14\n",
      "    POOLER_SAMPLING_RATIO: 0\n",
      "    POOLER_SCALES: (0.0625,)\n",
      "    PREDICTOR: KeypointRCNNPredictor\n",
      "    RESOLUTION: 14\n",
      "    SHARE_BOX_FEATURE_EXTRACTOR: True\n",
      "  ROI_MASK_HEAD:\n",
      "    CONV_LAYERS: (256, 256, 256, 256)\n",
      "    DILATION: 1\n",
      "    FEATURE_EXTRACTOR: ResNet50Conv5ROIFeatureExtractor\n",
      "    MLP_HEAD_DIM: 1024\n",
      "    POOLER_RESOLUTION: 14\n",
      "    POOLER_SAMPLING_RATIO: 0\n",
      "    POOLER_SCALES: (0.0625,)\n",
      "    POSTPROCESS_MASKS: False\n",
      "    POSTPROCESS_MASKS_THRESHOLD: 0.5\n",
      "    PREDICTOR: MaskRCNNC4Predictor\n",
      "    RESOLUTION: 14\n",
      "    SHARE_BOX_FEATURE_EXTRACTOR: True\n",
      "    USE_GN: False\n",
      "  RPN:\n",
      "    ANCHOR_SIZES: (32, 64, 128, 256, 512)\n",
      "    ANCHOR_STRIDE: (16,)\n",
      "    ASPECT_RATIOS: (0.5, 1.0, 2.0)\n",
      "    BATCH_SIZE_PER_IMAGE: 1\n",
      "    BG_IOU_THRESHOLD: 0.3\n",
      "    FG_IOU_THRESHOLD: 0.7\n",
      "    FPN_POST_NMS_PER_BATCH: True\n",
      "    FPN_POST_NMS_TOP_N_TEST: 2000\n",
      "    FPN_POST_NMS_TOP_N_TRAIN: 2000\n",
      "    MIN_SIZE: 0\n",
      "    NMS_THRESH: 0.7\n",
      "    POSITIVE_FRACTION: 0.5\n",
      "    POST_NMS_TOP_N_TEST: 1000\n",
      "    POST_NMS_TOP_N_TRAIN: 2000\n",
      "    PRE_NMS_TOP_N_TEST: 6000\n",
      "    PRE_NMS_TOP_N_TRAIN: 12000\n",
      "    RPN_HEAD: SingleConvRPNHead\n",
      "    STRADDLE_THRESH: 0\n",
      "    USE_FPN: False\n",
      "  RPN_ONLY: True\n",
      "  WEIGHT: ../../.torch/models/resnet50_lpf3.pth\n",
      "OUTPUT_DIR: training_dir/250/standardwhee/1/fcos_kps_ms_training_R_50_FPN_1x_bee\n",
      "PATHS_CATALOG: /share/ctn/users/bsb2144/directpose/maskrcnn_benchmark/config/paths_catalog.py\n",
      "SOLVER:\n",
      "  BASE_LR: 0.01\n",
      "  BIAS_LR_FACTOR: 2\n",
      "  CHECKPOINT_PERIOD: 2500\n",
      "  GAMMA: 0.1\n",
      "  IMS_PER_BATCH: 3\n",
      "  KPS_GRAD_MULT: 10.0\n",
      "  MAX_GRAD_NORM: 5.0\n",
      "  MAX_ITER: 180000\n",
      "  MOMENTUM: 0.9\n",
      "  POWER: 1.0\n",
      "  STEPS: (60000, 80000)\n",
      "  WARMUP_FACTOR: 0.3333333333333333\n",
      "  WARMUP_ITERS: 500\n",
      "  WARMUP_METHOD: constant\n",
      "  WEIGHT_DECAY: 0.0001\n",
      "  WEIGHT_DECAY_BIAS: 0\n",
      "TEST:\n",
      "  DETECTIONS_PER_IMG: 100\n",
      "  EXPECTED_RESULTS: []\n",
      "  EXPECTED_RESULTS_SIGMA_TOL: 4\n",
      "  IMS_PER_BATCH: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.cls_tower.0.weight\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.cls_tower.0.bias\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.cls_tower.1.weight\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.cls_tower.1.bias\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.cls_tower.3.weight\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.cls_tower.3.bias\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.cls_tower.4.weight\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.cls_tower.4.bias\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.cls_tower.6.weight\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.cls_tower.6.bias\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.cls_tower.7.weight\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.cls_tower.7.bias\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.cls_tower.9.weight\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.cls_tower.9.bias\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.cls_tower.10.weight\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.cls_tower.10.bias\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.kps_tower.0.weight\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.kps_tower.0.bias\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.kps_tower.1.weight\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.kps_tower.1.bias\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.kps_tower.3.weight\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.kps_tower.3.bias\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.kps_tower.4.weight\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.kps_tower.4.bias\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.kps_tower.6.weight\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.kps_tower.6.bias\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.kps_tower.7.weight\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.kps_tower.7.bias\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.kps_tower.9.weight\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.kps_tower.9.bias\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.kps_tower.10.weight\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.kps_tower.10.bias\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.kps_bases.weight\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.kps_bases.bias\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.sample_features_conv.0.weight\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.sample_features_conv.0.bias\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.sample_features_conv.1.weight\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.sample_features_conv.1.bias\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.kps_offsets.0.kps_offsets.weight\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.kps_offsets.0.kps_offsets.bias\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.cls_logits.weight\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.cls_logits.bias\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.centerness.weight\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.centerness.bias\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to heatmaps.deeplab.seg_head.0.weight\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to heatmaps.deeplab.seg_head.1.weight\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to heatmaps.deeplab.seg_head.1.bias\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to heatmaps.deeplab.seg_head.3.weight\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to heatmaps.deeplab.seg_head.4.weight\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to heatmaps.deeplab.seg_head.4.bias\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to heatmaps.deeplab.heatmaps.weight\n",
      "in starts with\n",
      "apply SOLVER.KPS_GRAD_MULT to heatmaps.deeplab.heatmaps.bias\n",
      "IN LR SCHEDULER\n",
      "2021-05-27 06:45:45,728 maskrcnn_benchmark.utils.checkpoint INFO: Loading checkpoint from training_dir/250/standardwhee/1/fcos_kps_ms_training_R_50_FPN_1x_bee/model_0002000.pth\n",
      "2021-05-27 06:45:45,946 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.bn1.bias                                  loaded from backbone.body.bn1.bias                                  of shape (64,)\n",
      "2021-05-27 06:45:45,946 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.bn1.num_batches_tracked                   loaded from backbone.body.bn1.num_batches_tracked                   of shape ()\n",
      "2021-05-27 06:45:45,946 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.bn1.running_mean                          loaded from backbone.body.bn1.running_mean                          of shape (64,)\n",
      "2021-05-27 06:45:45,946 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.bn1.running_var                           loaded from backbone.body.bn1.running_var                           of shape (64,)\n",
      "2021-05-27 06:45:45,946 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.bn1.weight                                loaded from backbone.body.bn1.weight                                of shape (64,)\n",
      "2021-05-27 06:45:45,946 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.conv1.weight                              loaded from backbone.body.conv1.weight                              of shape (64, 3, 7, 7)\n",
      "2021-05-27 06:45:45,946 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.0.bn1.bias                         loaded from backbone.body.layer1.0.bn1.bias                         of shape (64,)\n",
      "2021-05-27 06:45:45,947 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.0.bn1.num_batches_tracked          loaded from backbone.body.layer1.0.bn1.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,947 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.0.bn1.running_mean                 loaded from backbone.body.layer1.0.bn1.running_mean                 of shape (64,)\n",
      "2021-05-27 06:45:45,947 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.0.bn1.running_var                  loaded from backbone.body.layer1.0.bn1.running_var                  of shape (64,)\n",
      "2021-05-27 06:45:45,947 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.0.bn1.weight                       loaded from backbone.body.layer1.0.bn1.weight                       of shape (64,)\n",
      "2021-05-27 06:45:45,947 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.0.bn2.bias                         loaded from backbone.body.layer1.0.bn2.bias                         of shape (64,)\n",
      "2021-05-27 06:45:45,947 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.0.bn2.num_batches_tracked          loaded from backbone.body.layer1.0.bn2.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,947 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.0.bn2.running_mean                 loaded from backbone.body.layer1.0.bn2.running_mean                 of shape (64,)\n",
      "2021-05-27 06:45:45,947 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.0.bn2.running_var                  loaded from backbone.body.layer1.0.bn2.running_var                  of shape (64,)\n",
      "2021-05-27 06:45:45,947 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.0.bn2.weight                       loaded from backbone.body.layer1.0.bn2.weight                       of shape (64,)\n",
      "2021-05-27 06:45:45,947 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.0.bn3.bias                         loaded from backbone.body.layer1.0.bn3.bias                         of shape (256,)\n",
      "2021-05-27 06:45:45,948 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.0.bn3.num_batches_tracked          loaded from backbone.body.layer1.0.bn3.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,948 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.0.bn3.running_mean                 loaded from backbone.body.layer1.0.bn3.running_mean                 of shape (256,)\n",
      "2021-05-27 06:45:45,948 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.0.bn3.running_var                  loaded from backbone.body.layer1.0.bn3.running_var                  of shape (256,)\n",
      "2021-05-27 06:45:45,948 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.0.bn3.weight                       loaded from backbone.body.layer1.0.bn3.weight                       of shape (256,)\n",
      "2021-05-27 06:45:45,948 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.0.conv1.weight                     loaded from backbone.body.layer1.0.conv1.weight                     of shape (64, 64, 1, 1)\n",
      "2021-05-27 06:45:45,948 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.0.conv2.weight                     loaded from backbone.body.layer1.0.conv2.weight                     of shape (64, 64, 3, 3)\n",
      "2021-05-27 06:45:45,948 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.0.conv3.weight                     loaded from backbone.body.layer1.0.conv3.weight                     of shape (256, 64, 1, 1)\n",
      "2021-05-27 06:45:45,948 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.0.downsample.0.weight              loaded from backbone.body.layer1.0.downsample.0.weight              of shape (256, 64, 1, 1)\n",
      "2021-05-27 06:45:45,948 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.0.downsample.1.bias                loaded from backbone.body.layer1.0.downsample.1.bias                of shape (256,)\n",
      "2021-05-27 06:45:45,948 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.0.downsample.1.num_batches_tracked loaded from backbone.body.layer1.0.downsample.1.num_batches_tracked of shape ()\n",
      "2021-05-27 06:45:45,949 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.0.downsample.1.running_mean        loaded from backbone.body.layer1.0.downsample.1.running_mean        of shape (256,)\n",
      "2021-05-27 06:45:45,949 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.0.downsample.1.running_var         loaded from backbone.body.layer1.0.downsample.1.running_var         of shape (256,)\n",
      "2021-05-27 06:45:45,949 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.0.downsample.1.weight              loaded from backbone.body.layer1.0.downsample.1.weight              of shape (256,)\n",
      "2021-05-27 06:45:45,949 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.1.bn1.bias                         loaded from backbone.body.layer1.1.bn1.bias                         of shape (64,)\n",
      "2021-05-27 06:45:45,949 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.1.bn1.num_batches_tracked          loaded from backbone.body.layer1.1.bn1.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,949 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.1.bn1.running_mean                 loaded from backbone.body.layer1.1.bn1.running_mean                 of shape (64,)\n",
      "2021-05-27 06:45:45,949 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.1.bn1.running_var                  loaded from backbone.body.layer1.1.bn1.running_var                  of shape (64,)\n",
      "2021-05-27 06:45:45,949 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.1.bn1.weight                       loaded from backbone.body.layer1.1.bn1.weight                       of shape (64,)\n",
      "2021-05-27 06:45:45,949 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.1.bn2.bias                         loaded from backbone.body.layer1.1.bn2.bias                         of shape (64,)\n",
      "2021-05-27 06:45:45,950 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.1.bn2.num_batches_tracked          loaded from backbone.body.layer1.1.bn2.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,950 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.1.bn2.running_mean                 loaded from backbone.body.layer1.1.bn2.running_mean                 of shape (64,)\n",
      "2021-05-27 06:45:45,950 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.1.bn2.running_var                  loaded from backbone.body.layer1.1.bn2.running_var                  of shape (64,)\n",
      "2021-05-27 06:45:45,950 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.1.bn2.weight                       loaded from backbone.body.layer1.1.bn2.weight                       of shape (64,)\n",
      "2021-05-27 06:45:45,950 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.1.bn3.bias                         loaded from backbone.body.layer1.1.bn3.bias                         of shape (256,)\n",
      "2021-05-27 06:45:45,950 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.1.bn3.num_batches_tracked          loaded from backbone.body.layer1.1.bn3.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,950 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.1.bn3.running_mean                 loaded from backbone.body.layer1.1.bn3.running_mean                 of shape (256,)\n",
      "2021-05-27 06:45:45,950 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.1.bn3.running_var                  loaded from backbone.body.layer1.1.bn3.running_var                  of shape (256,)\n",
      "2021-05-27 06:45:45,950 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.1.bn3.weight                       loaded from backbone.body.layer1.1.bn3.weight                       of shape (256,)\n",
      "2021-05-27 06:45:45,950 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.1.conv1.weight                     loaded from backbone.body.layer1.1.conv1.weight                     of shape (64, 256, 1, 1)\n",
      "2021-05-27 06:45:45,950 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.1.conv2.weight                     loaded from backbone.body.layer1.1.conv2.weight                     of shape (64, 64, 3, 3)\n",
      "2021-05-27 06:45:45,951 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.1.conv3.weight                     loaded from backbone.body.layer1.1.conv3.weight                     of shape (256, 64, 1, 1)\n",
      "2021-05-27 06:45:45,951 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.2.bn1.bias                         loaded from backbone.body.layer1.2.bn1.bias                         of shape (64,)\n",
      "2021-05-27 06:45:45,951 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.2.bn1.num_batches_tracked          loaded from backbone.body.layer1.2.bn1.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,951 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.2.bn1.running_mean                 loaded from backbone.body.layer1.2.bn1.running_mean                 of shape (64,)\n",
      "2021-05-27 06:45:45,951 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.2.bn1.running_var                  loaded from backbone.body.layer1.2.bn1.running_var                  of shape (64,)\n",
      "2021-05-27 06:45:45,951 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.2.bn1.weight                       loaded from backbone.body.layer1.2.bn1.weight                       of shape (64,)\n",
      "2021-05-27 06:45:45,951 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.2.bn2.bias                         loaded from backbone.body.layer1.2.bn2.bias                         of shape (64,)\n",
      "2021-05-27 06:45:45,951 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.2.bn2.num_batches_tracked          loaded from backbone.body.layer1.2.bn2.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,951 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.2.bn2.running_mean                 loaded from backbone.body.layer1.2.bn2.running_mean                 of shape (64,)\n",
      "2021-05-27 06:45:45,952 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.2.bn2.running_var                  loaded from backbone.body.layer1.2.bn2.running_var                  of shape (64,)\n",
      "2021-05-27 06:45:45,952 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.2.bn2.weight                       loaded from backbone.body.layer1.2.bn2.weight                       of shape (64,)\n",
      "2021-05-27 06:45:45,952 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.2.bn3.bias                         loaded from backbone.body.layer1.2.bn3.bias                         of shape (256,)\n",
      "2021-05-27 06:45:45,952 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.2.bn3.num_batches_tracked          loaded from backbone.body.layer1.2.bn3.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,952 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.2.bn3.running_mean                 loaded from backbone.body.layer1.2.bn3.running_mean                 of shape (256,)\n",
      "2021-05-27 06:45:45,952 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.2.bn3.running_var                  loaded from backbone.body.layer1.2.bn3.running_var                  of shape (256,)\n",
      "2021-05-27 06:45:45,952 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.2.bn3.weight                       loaded from backbone.body.layer1.2.bn3.weight                       of shape (256,)\n",
      "2021-05-27 06:45:45,952 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.2.conv1.weight                     loaded from backbone.body.layer1.2.conv1.weight                     of shape (64, 256, 1, 1)\n",
      "2021-05-27 06:45:45,952 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.2.conv2.weight                     loaded from backbone.body.layer1.2.conv2.weight                     of shape (64, 64, 3, 3)\n",
      "2021-05-27 06:45:45,952 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer1.2.conv3.weight                     loaded from backbone.body.layer1.2.conv3.weight                     of shape (256, 64, 1, 1)\n",
      "2021-05-27 06:45:45,953 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.0.bn1.bias                         loaded from backbone.body.layer2.0.bn1.bias                         of shape (128,)\n",
      "2021-05-27 06:45:45,953 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.0.bn1.num_batches_tracked          loaded from backbone.body.layer2.0.bn1.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,953 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.0.bn1.running_mean                 loaded from backbone.body.layer2.0.bn1.running_mean                 of shape (128,)\n",
      "2021-05-27 06:45:45,953 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.0.bn1.running_var                  loaded from backbone.body.layer2.0.bn1.running_var                  of shape (128,)\n",
      "2021-05-27 06:45:45,953 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.0.bn1.weight                       loaded from backbone.body.layer2.0.bn1.weight                       of shape (128,)\n",
      "2021-05-27 06:45:45,953 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.0.bn2.bias                         loaded from backbone.body.layer2.0.bn2.bias                         of shape (128,)\n",
      "2021-05-27 06:45:45,953 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.0.bn2.num_batches_tracked          loaded from backbone.body.layer2.0.bn2.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,953 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.0.bn2.running_mean                 loaded from backbone.body.layer2.0.bn2.running_mean                 of shape (128,)\n",
      "2021-05-27 06:45:45,953 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.0.bn2.running_var                  loaded from backbone.body.layer2.0.bn2.running_var                  of shape (128,)\n",
      "2021-05-27 06:45:45,953 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.0.bn2.weight                       loaded from backbone.body.layer2.0.bn2.weight                       of shape (128,)\n",
      "2021-05-27 06:45:45,954 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.0.bn3.bias                         loaded from backbone.body.layer2.0.bn3.bias                         of shape (512,)\n",
      "2021-05-27 06:45:45,954 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.0.bn3.num_batches_tracked          loaded from backbone.body.layer2.0.bn3.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,954 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.0.bn3.running_mean                 loaded from backbone.body.layer2.0.bn3.running_mean                 of shape (512,)\n",
      "2021-05-27 06:45:45,954 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.0.bn3.running_var                  loaded from backbone.body.layer2.0.bn3.running_var                  of shape (512,)\n",
      "2021-05-27 06:45:45,954 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.0.bn3.weight                       loaded from backbone.body.layer2.0.bn3.weight                       of shape (512,)\n",
      "2021-05-27 06:45:45,954 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.0.conv1.weight                     loaded from backbone.body.layer2.0.conv1.weight                     of shape (128, 256, 1, 1)\n",
      "2021-05-27 06:45:45,954 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.0.conv2.weight                     loaded from backbone.body.layer2.0.conv2.weight                     of shape (128, 128, 3, 3)\n",
      "2021-05-27 06:45:45,954 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.0.conv3.0.filt                     loaded from backbone.body.layer2.0.conv3.0.filt                     of shape (128, 1, 3, 3)\n",
      "2021-05-27 06:45:45,954 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.0.conv3.1.weight                   loaded from backbone.body.layer2.0.conv3.1.weight                   of shape (512, 128, 1, 1)\n",
      "2021-05-27 06:45:45,954 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.0.downsample.0.filt                loaded from backbone.body.layer2.0.downsample.0.filt                of shape (256, 1, 3, 3)\n",
      "2021-05-27 06:45:45,955 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.0.downsample.1.weight              loaded from backbone.body.layer2.0.downsample.1.weight              of shape (512, 256, 1, 1)\n",
      "2021-05-27 06:45:45,955 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.0.downsample.2.bias                loaded from backbone.body.layer2.0.downsample.2.bias                of shape (512,)\n",
      "2021-05-27 06:45:45,955 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.0.downsample.2.num_batches_tracked loaded from backbone.body.layer2.0.downsample.2.num_batches_tracked of shape ()\n",
      "2021-05-27 06:45:45,955 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.0.downsample.2.running_mean        loaded from backbone.body.layer2.0.downsample.2.running_mean        of shape (512,)\n",
      "2021-05-27 06:45:45,955 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.0.downsample.2.running_var         loaded from backbone.body.layer2.0.downsample.2.running_var         of shape (512,)\n",
      "2021-05-27 06:45:45,955 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.0.downsample.2.weight              loaded from backbone.body.layer2.0.downsample.2.weight              of shape (512,)\n",
      "2021-05-27 06:45:45,955 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.1.bn1.bias                         loaded from backbone.body.layer2.1.bn1.bias                         of shape (128,)\n",
      "2021-05-27 06:45:45,955 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.1.bn1.num_batches_tracked          loaded from backbone.body.layer2.1.bn1.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,955 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.1.bn1.running_mean                 loaded from backbone.body.layer2.1.bn1.running_mean                 of shape (128,)\n",
      "2021-05-27 06:45:45,955 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.1.bn1.running_var                  loaded from backbone.body.layer2.1.bn1.running_var                  of shape (128,)\n",
      "2021-05-27 06:45:45,956 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.1.bn1.weight                       loaded from backbone.body.layer2.1.bn1.weight                       of shape (128,)\n",
      "2021-05-27 06:45:45,956 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.1.bn2.bias                         loaded from backbone.body.layer2.1.bn2.bias                         of shape (128,)\n",
      "2021-05-27 06:45:45,956 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.1.bn2.num_batches_tracked          loaded from backbone.body.layer2.1.bn2.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,956 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.1.bn2.running_mean                 loaded from backbone.body.layer2.1.bn2.running_mean                 of shape (128,)\n",
      "2021-05-27 06:45:45,956 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.1.bn2.running_var                  loaded from backbone.body.layer2.1.bn2.running_var                  of shape (128,)\n",
      "2021-05-27 06:45:45,956 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.1.bn2.weight                       loaded from backbone.body.layer2.1.bn2.weight                       of shape (128,)\n",
      "2021-05-27 06:45:45,956 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.1.bn3.bias                         loaded from backbone.body.layer2.1.bn3.bias                         of shape (512,)\n",
      "2021-05-27 06:45:45,956 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.1.bn3.num_batches_tracked          loaded from backbone.body.layer2.1.bn3.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,956 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.1.bn3.running_mean                 loaded from backbone.body.layer2.1.bn3.running_mean                 of shape (512,)\n",
      "2021-05-27 06:45:45,956 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.1.bn3.running_var                  loaded from backbone.body.layer2.1.bn3.running_var                  of shape (512,)\n",
      "2021-05-27 06:45:45,957 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.1.bn3.weight                       loaded from backbone.body.layer2.1.bn3.weight                       of shape (512,)\n",
      "2021-05-27 06:45:45,957 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.1.conv1.weight                     loaded from backbone.body.layer2.1.conv1.weight                     of shape (128, 512, 1, 1)\n",
      "2021-05-27 06:45:45,957 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.1.conv2.weight                     loaded from backbone.body.layer2.1.conv2.weight                     of shape (128, 128, 3, 3)\n",
      "2021-05-27 06:45:45,957 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.1.conv3.weight                     loaded from backbone.body.layer2.1.conv3.weight                     of shape (512, 128, 1, 1)\n",
      "2021-05-27 06:45:45,957 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.2.bn1.bias                         loaded from backbone.body.layer2.2.bn1.bias                         of shape (128,)\n",
      "2021-05-27 06:45:45,957 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.2.bn1.num_batches_tracked          loaded from backbone.body.layer2.2.bn1.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,957 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.2.bn1.running_mean                 loaded from backbone.body.layer2.2.bn1.running_mean                 of shape (128,)\n",
      "2021-05-27 06:45:45,957 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.2.bn1.running_var                  loaded from backbone.body.layer2.2.bn1.running_var                  of shape (128,)\n",
      "2021-05-27 06:45:45,957 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.2.bn1.weight                       loaded from backbone.body.layer2.2.bn1.weight                       of shape (128,)\n",
      "2021-05-27 06:45:45,957 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.2.bn2.bias                         loaded from backbone.body.layer2.2.bn2.bias                         of shape (128,)\n",
      "2021-05-27 06:45:45,958 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.2.bn2.num_batches_tracked          loaded from backbone.body.layer2.2.bn2.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,958 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.2.bn2.running_mean                 loaded from backbone.body.layer2.2.bn2.running_mean                 of shape (128,)\n",
      "2021-05-27 06:45:45,958 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.2.bn2.running_var                  loaded from backbone.body.layer2.2.bn2.running_var                  of shape (128,)\n",
      "2021-05-27 06:45:45,958 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.2.bn2.weight                       loaded from backbone.body.layer2.2.bn2.weight                       of shape (128,)\n",
      "2021-05-27 06:45:45,958 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.2.bn3.bias                         loaded from backbone.body.layer2.2.bn3.bias                         of shape (512,)\n",
      "2021-05-27 06:45:45,958 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.2.bn3.num_batches_tracked          loaded from backbone.body.layer2.2.bn3.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,958 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.2.bn3.running_mean                 loaded from backbone.body.layer2.2.bn3.running_mean                 of shape (512,)\n",
      "2021-05-27 06:45:45,958 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.2.bn3.running_var                  loaded from backbone.body.layer2.2.bn3.running_var                  of shape (512,)\n",
      "2021-05-27 06:45:45,958 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.2.bn3.weight                       loaded from backbone.body.layer2.2.bn3.weight                       of shape (512,)\n",
      "2021-05-27 06:45:45,958 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.2.conv1.weight                     loaded from backbone.body.layer2.2.conv1.weight                     of shape (128, 512, 1, 1)\n",
      "2021-05-27 06:45:45,959 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.2.conv2.weight                     loaded from backbone.body.layer2.2.conv2.weight                     of shape (128, 128, 3, 3)\n",
      "2021-05-27 06:45:45,959 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.2.conv3.weight                     loaded from backbone.body.layer2.2.conv3.weight                     of shape (512, 128, 1, 1)\n",
      "2021-05-27 06:45:45,959 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.3.bn1.bias                         loaded from backbone.body.layer2.3.bn1.bias                         of shape (128,)\n",
      "2021-05-27 06:45:45,959 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.3.bn1.num_batches_tracked          loaded from backbone.body.layer2.3.bn1.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,959 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.3.bn1.running_mean                 loaded from backbone.body.layer2.3.bn1.running_mean                 of shape (128,)\n",
      "2021-05-27 06:45:45,959 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.3.bn1.running_var                  loaded from backbone.body.layer2.3.bn1.running_var                  of shape (128,)\n",
      "2021-05-27 06:45:45,959 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.3.bn1.weight                       loaded from backbone.body.layer2.3.bn1.weight                       of shape (128,)\n",
      "2021-05-27 06:45:45,959 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.3.bn2.bias                         loaded from backbone.body.layer2.3.bn2.bias                         of shape (128,)\n",
      "2021-05-27 06:45:45,959 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.3.bn2.num_batches_tracked          loaded from backbone.body.layer2.3.bn2.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,960 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.3.bn2.running_mean                 loaded from backbone.body.layer2.3.bn2.running_mean                 of shape (128,)\n",
      "2021-05-27 06:45:45,960 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.3.bn2.running_var                  loaded from backbone.body.layer2.3.bn2.running_var                  of shape (128,)\n",
      "2021-05-27 06:45:45,960 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.3.bn2.weight                       loaded from backbone.body.layer2.3.bn2.weight                       of shape (128,)\n",
      "2021-05-27 06:45:45,960 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.3.bn3.bias                         loaded from backbone.body.layer2.3.bn3.bias                         of shape (512,)\n",
      "2021-05-27 06:45:45,960 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.3.bn3.num_batches_tracked          loaded from backbone.body.layer2.3.bn3.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,960 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.3.bn3.running_mean                 loaded from backbone.body.layer2.3.bn3.running_mean                 of shape (512,)\n",
      "2021-05-27 06:45:45,960 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.3.bn3.running_var                  loaded from backbone.body.layer2.3.bn3.running_var                  of shape (512,)\n",
      "2021-05-27 06:45:45,960 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.3.bn3.weight                       loaded from backbone.body.layer2.3.bn3.weight                       of shape (512,)\n",
      "2021-05-27 06:45:45,960 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.3.conv1.weight                     loaded from backbone.body.layer2.3.conv1.weight                     of shape (128, 512, 1, 1)\n",
      "2021-05-27 06:45:45,960 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.3.conv2.weight                     loaded from backbone.body.layer2.3.conv2.weight                     of shape (128, 128, 3, 3)\n",
      "2021-05-27 06:45:45,961 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer2.3.conv3.weight                     loaded from backbone.body.layer2.3.conv3.weight                     of shape (512, 128, 1, 1)\n",
      "2021-05-27 06:45:45,961 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.0.bn1.bias                         loaded from backbone.body.layer3.0.bn1.bias                         of shape (256,)\n",
      "2021-05-27 06:45:45,961 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.0.bn1.num_batches_tracked          loaded from backbone.body.layer3.0.bn1.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,961 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.0.bn1.running_mean                 loaded from backbone.body.layer3.0.bn1.running_mean                 of shape (256,)\n",
      "2021-05-27 06:45:45,961 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.0.bn1.running_var                  loaded from backbone.body.layer3.0.bn1.running_var                  of shape (256,)\n",
      "2021-05-27 06:45:45,961 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.0.bn1.weight                       loaded from backbone.body.layer3.0.bn1.weight                       of shape (256,)\n",
      "2021-05-27 06:45:45,961 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.0.bn2.bias                         loaded from backbone.body.layer3.0.bn2.bias                         of shape (256,)\n",
      "2021-05-27 06:45:45,961 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.0.bn2.num_batches_tracked          loaded from backbone.body.layer3.0.bn2.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,961 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.0.bn2.running_mean                 loaded from backbone.body.layer3.0.bn2.running_mean                 of shape (256,)\n",
      "2021-05-27 06:45:45,961 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.0.bn2.running_var                  loaded from backbone.body.layer3.0.bn2.running_var                  of shape (256,)\n",
      "2021-05-27 06:45:45,962 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.0.bn2.weight                       loaded from backbone.body.layer3.0.bn2.weight                       of shape (256,)\n",
      "2021-05-27 06:45:45,962 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.0.bn3.bias                         loaded from backbone.body.layer3.0.bn3.bias                         of shape (1024,)\n",
      "2021-05-27 06:45:45,962 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.0.bn3.num_batches_tracked          loaded from backbone.body.layer3.0.bn3.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,962 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.0.bn3.running_mean                 loaded from backbone.body.layer3.0.bn3.running_mean                 of shape (1024,)\n",
      "2021-05-27 06:45:45,962 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.0.bn3.running_var                  loaded from backbone.body.layer3.0.bn3.running_var                  of shape (1024,)\n",
      "2021-05-27 06:45:45,962 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.0.bn3.weight                       loaded from backbone.body.layer3.0.bn3.weight                       of shape (1024,)\n",
      "2021-05-27 06:45:45,962 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.0.conv1.weight                     loaded from backbone.body.layer3.0.conv1.weight                     of shape (256, 512, 1, 1)\n",
      "2021-05-27 06:45:45,962 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.0.conv2.weight                     loaded from backbone.body.layer3.0.conv2.weight                     of shape (256, 256, 3, 3)\n",
      "2021-05-27 06:45:45,962 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.0.conv3.0.filt                     loaded from backbone.body.layer3.0.conv3.0.filt                     of shape (256, 1, 3, 3)\n",
      "2021-05-27 06:45:45,962 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.0.conv3.1.weight                   loaded from backbone.body.layer3.0.conv3.1.weight                   of shape (1024, 256, 1, 1)\n",
      "2021-05-27 06:45:45,963 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.0.downsample.0.filt                loaded from backbone.body.layer3.0.downsample.0.filt                of shape (512, 1, 3, 3)\n",
      "2021-05-27 06:45:45,963 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.0.downsample.1.weight              loaded from backbone.body.layer3.0.downsample.1.weight              of shape (1024, 512, 1, 1)\n",
      "2021-05-27 06:45:45,963 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.0.downsample.2.bias                loaded from backbone.body.layer3.0.downsample.2.bias                of shape (1024,)\n",
      "2021-05-27 06:45:45,963 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.0.downsample.2.num_batches_tracked loaded from backbone.body.layer3.0.downsample.2.num_batches_tracked of shape ()\n",
      "2021-05-27 06:45:45,963 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.0.downsample.2.running_mean        loaded from backbone.body.layer3.0.downsample.2.running_mean        of shape (1024,)\n",
      "2021-05-27 06:45:45,963 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.0.downsample.2.running_var         loaded from backbone.body.layer3.0.downsample.2.running_var         of shape (1024,)\n",
      "2021-05-27 06:45:45,963 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.0.downsample.2.weight              loaded from backbone.body.layer3.0.downsample.2.weight              of shape (1024,)\n",
      "2021-05-27 06:45:45,963 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.1.bn1.bias                         loaded from backbone.body.layer3.1.bn1.bias                         of shape (256,)\n",
      "2021-05-27 06:45:45,963 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.1.bn1.num_batches_tracked          loaded from backbone.body.layer3.1.bn1.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,963 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.1.bn1.running_mean                 loaded from backbone.body.layer3.1.bn1.running_mean                 of shape (256,)\n",
      "2021-05-27 06:45:45,964 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.1.bn1.running_var                  loaded from backbone.body.layer3.1.bn1.running_var                  of shape (256,)\n",
      "2021-05-27 06:45:45,964 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.1.bn1.weight                       loaded from backbone.body.layer3.1.bn1.weight                       of shape (256,)\n",
      "2021-05-27 06:45:45,964 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.1.bn2.bias                         loaded from backbone.body.layer3.1.bn2.bias                         of shape (256,)\n",
      "2021-05-27 06:45:45,964 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.1.bn2.num_batches_tracked          loaded from backbone.body.layer3.1.bn2.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,964 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.1.bn2.running_mean                 loaded from backbone.body.layer3.1.bn2.running_mean                 of shape (256,)\n",
      "2021-05-27 06:45:45,964 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.1.bn2.running_var                  loaded from backbone.body.layer3.1.bn2.running_var                  of shape (256,)\n",
      "2021-05-27 06:45:45,964 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.1.bn2.weight                       loaded from backbone.body.layer3.1.bn2.weight                       of shape (256,)\n",
      "2021-05-27 06:45:45,964 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.1.bn3.bias                         loaded from backbone.body.layer3.1.bn3.bias                         of shape (1024,)\n",
      "2021-05-27 06:45:45,964 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.1.bn3.num_batches_tracked          loaded from backbone.body.layer3.1.bn3.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,964 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.1.bn3.running_mean                 loaded from backbone.body.layer3.1.bn3.running_mean                 of shape (1024,)\n",
      "2021-05-27 06:45:45,965 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.1.bn3.running_var                  loaded from backbone.body.layer3.1.bn3.running_var                  of shape (1024,)\n",
      "2021-05-27 06:45:45,965 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.1.bn3.weight                       loaded from backbone.body.layer3.1.bn3.weight                       of shape (1024,)\n",
      "2021-05-27 06:45:45,965 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.1.conv1.weight                     loaded from backbone.body.layer3.1.conv1.weight                     of shape (256, 1024, 1, 1)\n",
      "2021-05-27 06:45:45,965 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.1.conv2.weight                     loaded from backbone.body.layer3.1.conv2.weight                     of shape (256, 256, 3, 3)\n",
      "2021-05-27 06:45:45,965 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.1.conv3.weight                     loaded from backbone.body.layer3.1.conv3.weight                     of shape (1024, 256, 1, 1)\n",
      "2021-05-27 06:45:45,965 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.2.bn1.bias                         loaded from backbone.body.layer3.2.bn1.bias                         of shape (256,)\n",
      "2021-05-27 06:45:45,965 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.2.bn1.num_batches_tracked          loaded from backbone.body.layer3.2.bn1.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,965 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.2.bn1.running_mean                 loaded from backbone.body.layer3.2.bn1.running_mean                 of shape (256,)\n",
      "2021-05-27 06:45:45,965 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.2.bn1.running_var                  loaded from backbone.body.layer3.2.bn1.running_var                  of shape (256,)\n",
      "2021-05-27 06:45:45,965 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.2.bn1.weight                       loaded from backbone.body.layer3.2.bn1.weight                       of shape (256,)\n",
      "2021-05-27 06:45:45,966 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.2.bn2.bias                         loaded from backbone.body.layer3.2.bn2.bias                         of shape (256,)\n",
      "2021-05-27 06:45:45,966 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.2.bn2.num_batches_tracked          loaded from backbone.body.layer3.2.bn2.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,966 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.2.bn2.running_mean                 loaded from backbone.body.layer3.2.bn2.running_mean                 of shape (256,)\n",
      "2021-05-27 06:45:45,966 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.2.bn2.running_var                  loaded from backbone.body.layer3.2.bn2.running_var                  of shape (256,)\n",
      "2021-05-27 06:45:45,966 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.2.bn2.weight                       loaded from backbone.body.layer3.2.bn2.weight                       of shape (256,)\n",
      "2021-05-27 06:45:45,966 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.2.bn3.bias                         loaded from backbone.body.layer3.2.bn3.bias                         of shape (1024,)\n",
      "2021-05-27 06:45:45,966 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.2.bn3.num_batches_tracked          loaded from backbone.body.layer3.2.bn3.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,966 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.2.bn3.running_mean                 loaded from backbone.body.layer3.2.bn3.running_mean                 of shape (1024,)\n",
      "2021-05-27 06:45:45,966 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.2.bn3.running_var                  loaded from backbone.body.layer3.2.bn3.running_var                  of shape (1024,)\n",
      "2021-05-27 06:45:45,967 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.2.bn3.weight                       loaded from backbone.body.layer3.2.bn3.weight                       of shape (1024,)\n",
      "2021-05-27 06:45:45,967 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.2.conv1.weight                     loaded from backbone.body.layer3.2.conv1.weight                     of shape (256, 1024, 1, 1)\n",
      "2021-05-27 06:45:45,967 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.2.conv2.weight                     loaded from backbone.body.layer3.2.conv2.weight                     of shape (256, 256, 3, 3)\n",
      "2021-05-27 06:45:45,967 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.2.conv3.weight                     loaded from backbone.body.layer3.2.conv3.weight                     of shape (1024, 256, 1, 1)\n",
      "2021-05-27 06:45:45,967 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.3.bn1.bias                         loaded from backbone.body.layer3.3.bn1.bias                         of shape (256,)\n",
      "2021-05-27 06:45:45,967 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.3.bn1.num_batches_tracked          loaded from backbone.body.layer3.3.bn1.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,967 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.3.bn1.running_mean                 loaded from backbone.body.layer3.3.bn1.running_mean                 of shape (256,)\n",
      "2021-05-27 06:45:45,967 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.3.bn1.running_var                  loaded from backbone.body.layer3.3.bn1.running_var                  of shape (256,)\n",
      "2021-05-27 06:45:45,967 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.3.bn1.weight                       loaded from backbone.body.layer3.3.bn1.weight                       of shape (256,)\n",
      "2021-05-27 06:45:45,967 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.3.bn2.bias                         loaded from backbone.body.layer3.3.bn2.bias                         of shape (256,)\n",
      "2021-05-27 06:45:45,968 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.3.bn2.num_batches_tracked          loaded from backbone.body.layer3.3.bn2.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,968 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.3.bn2.running_mean                 loaded from backbone.body.layer3.3.bn2.running_mean                 of shape (256,)\n",
      "2021-05-27 06:45:45,968 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.3.bn2.running_var                  loaded from backbone.body.layer3.3.bn2.running_var                  of shape (256,)\n",
      "2021-05-27 06:45:45,968 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.3.bn2.weight                       loaded from backbone.body.layer3.3.bn2.weight                       of shape (256,)\n",
      "2021-05-27 06:45:45,968 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.3.bn3.bias                         loaded from backbone.body.layer3.3.bn3.bias                         of shape (1024,)\n",
      "2021-05-27 06:45:45,968 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.3.bn3.num_batches_tracked          loaded from backbone.body.layer3.3.bn3.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,968 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.3.bn3.running_mean                 loaded from backbone.body.layer3.3.bn3.running_mean                 of shape (1024,)\n",
      "2021-05-27 06:45:45,968 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.3.bn3.running_var                  loaded from backbone.body.layer3.3.bn3.running_var                  of shape (1024,)\n",
      "2021-05-27 06:45:45,968 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.3.bn3.weight                       loaded from backbone.body.layer3.3.bn3.weight                       of shape (1024,)\n",
      "2021-05-27 06:45:45,968 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.3.conv1.weight                     loaded from backbone.body.layer3.3.conv1.weight                     of shape (256, 1024, 1, 1)\n",
      "2021-05-27 06:45:45,969 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.3.conv2.weight                     loaded from backbone.body.layer3.3.conv2.weight                     of shape (256, 256, 3, 3)\n",
      "2021-05-27 06:45:45,969 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.3.conv3.weight                     loaded from backbone.body.layer3.3.conv3.weight                     of shape (1024, 256, 1, 1)\n",
      "2021-05-27 06:45:45,969 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.4.bn1.bias                         loaded from backbone.body.layer3.4.bn1.bias                         of shape (256,)\n",
      "2021-05-27 06:45:45,969 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.4.bn1.num_batches_tracked          loaded from backbone.body.layer3.4.bn1.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,969 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.4.bn1.running_mean                 loaded from backbone.body.layer3.4.bn1.running_mean                 of shape (256,)\n",
      "2021-05-27 06:45:45,969 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.4.bn1.running_var                  loaded from backbone.body.layer3.4.bn1.running_var                  of shape (256,)\n",
      "2021-05-27 06:45:45,969 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.4.bn1.weight                       loaded from backbone.body.layer3.4.bn1.weight                       of shape (256,)\n",
      "2021-05-27 06:45:45,969 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.4.bn2.bias                         loaded from backbone.body.layer3.4.bn2.bias                         of shape (256,)\n",
      "2021-05-27 06:45:45,969 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.4.bn2.num_batches_tracked          loaded from backbone.body.layer3.4.bn2.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,969 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.4.bn2.running_mean                 loaded from backbone.body.layer3.4.bn2.running_mean                 of shape (256,)\n",
      "2021-05-27 06:45:45,970 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.4.bn2.running_var                  loaded from backbone.body.layer3.4.bn2.running_var                  of shape (256,)\n",
      "2021-05-27 06:45:45,970 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.4.bn2.weight                       loaded from backbone.body.layer3.4.bn2.weight                       of shape (256,)\n",
      "2021-05-27 06:45:45,970 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.4.bn3.bias                         loaded from backbone.body.layer3.4.bn3.bias                         of shape (1024,)\n",
      "2021-05-27 06:45:45,970 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.4.bn3.num_batches_tracked          loaded from backbone.body.layer3.4.bn3.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,970 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.4.bn3.running_mean                 loaded from backbone.body.layer3.4.bn3.running_mean                 of shape (1024,)\n",
      "2021-05-27 06:45:45,970 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.4.bn3.running_var                  loaded from backbone.body.layer3.4.bn3.running_var                  of shape (1024,)\n",
      "2021-05-27 06:45:45,970 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.4.bn3.weight                       loaded from backbone.body.layer3.4.bn3.weight                       of shape (1024,)\n",
      "2021-05-27 06:45:45,970 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.4.conv1.weight                     loaded from backbone.body.layer3.4.conv1.weight                     of shape (256, 1024, 1, 1)\n",
      "2021-05-27 06:45:45,970 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.4.conv2.weight                     loaded from backbone.body.layer3.4.conv2.weight                     of shape (256, 256, 3, 3)\n",
      "2021-05-27 06:45:45,970 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.4.conv3.weight                     loaded from backbone.body.layer3.4.conv3.weight                     of shape (1024, 256, 1, 1)\n",
      "2021-05-27 06:45:45,971 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.5.bn1.bias                         loaded from backbone.body.layer3.5.bn1.bias                         of shape (256,)\n",
      "2021-05-27 06:45:45,971 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.5.bn1.num_batches_tracked          loaded from backbone.body.layer3.5.bn1.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,971 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.5.bn1.running_mean                 loaded from backbone.body.layer3.5.bn1.running_mean                 of shape (256,)\n",
      "2021-05-27 06:45:45,971 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.5.bn1.running_var                  loaded from backbone.body.layer3.5.bn1.running_var                  of shape (256,)\n",
      "2021-05-27 06:45:45,971 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.5.bn1.weight                       loaded from backbone.body.layer3.5.bn1.weight                       of shape (256,)\n",
      "2021-05-27 06:45:45,971 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.5.bn2.bias                         loaded from backbone.body.layer3.5.bn2.bias                         of shape (256,)\n",
      "2021-05-27 06:45:45,971 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.5.bn2.num_batches_tracked          loaded from backbone.body.layer3.5.bn2.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,971 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.5.bn2.running_mean                 loaded from backbone.body.layer3.5.bn2.running_mean                 of shape (256,)\n",
      "2021-05-27 06:45:45,971 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.5.bn2.running_var                  loaded from backbone.body.layer3.5.bn2.running_var                  of shape (256,)\n",
      "2021-05-27 06:45:45,971 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.5.bn2.weight                       loaded from backbone.body.layer3.5.bn2.weight                       of shape (256,)\n",
      "2021-05-27 06:45:45,972 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.5.bn3.bias                         loaded from backbone.body.layer3.5.bn3.bias                         of shape (1024,)\n",
      "2021-05-27 06:45:45,972 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.5.bn3.num_batches_tracked          loaded from backbone.body.layer3.5.bn3.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,972 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.5.bn3.running_mean                 loaded from backbone.body.layer3.5.bn3.running_mean                 of shape (1024,)\n",
      "2021-05-27 06:45:45,972 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.5.bn3.running_var                  loaded from backbone.body.layer3.5.bn3.running_var                  of shape (1024,)\n",
      "2021-05-27 06:45:45,972 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.5.bn3.weight                       loaded from backbone.body.layer3.5.bn3.weight                       of shape (1024,)\n",
      "2021-05-27 06:45:45,972 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.5.conv1.weight                     loaded from backbone.body.layer3.5.conv1.weight                     of shape (256, 1024, 1, 1)\n",
      "2021-05-27 06:45:45,972 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.5.conv2.weight                     loaded from backbone.body.layer3.5.conv2.weight                     of shape (256, 256, 3, 3)\n",
      "2021-05-27 06:45:45,972 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer3.5.conv3.weight                     loaded from backbone.body.layer3.5.conv3.weight                     of shape (1024, 256, 1, 1)\n",
      "2021-05-27 06:45:45,972 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.0.bn1.bias                         loaded from backbone.body.layer4.0.bn1.bias                         of shape (512,)\n",
      "2021-05-27 06:45:45,973 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.0.bn1.num_batches_tracked          loaded from backbone.body.layer4.0.bn1.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,973 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.0.bn1.running_mean                 loaded from backbone.body.layer4.0.bn1.running_mean                 of shape (512,)\n",
      "2021-05-27 06:45:45,973 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.0.bn1.running_var                  loaded from backbone.body.layer4.0.bn1.running_var                  of shape (512,)\n",
      "2021-05-27 06:45:45,973 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.0.bn1.weight                       loaded from backbone.body.layer4.0.bn1.weight                       of shape (512,)\n",
      "2021-05-27 06:45:45,973 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.0.bn2.bias                         loaded from backbone.body.layer4.0.bn2.bias                         of shape (512,)\n",
      "2021-05-27 06:45:45,973 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.0.bn2.num_batches_tracked          loaded from backbone.body.layer4.0.bn2.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,973 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.0.bn2.running_mean                 loaded from backbone.body.layer4.0.bn2.running_mean                 of shape (512,)\n",
      "2021-05-27 06:45:45,973 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.0.bn2.running_var                  loaded from backbone.body.layer4.0.bn2.running_var                  of shape (512,)\n",
      "2021-05-27 06:45:45,973 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.0.bn2.weight                       loaded from backbone.body.layer4.0.bn2.weight                       of shape (512,)\n",
      "2021-05-27 06:45:45,973 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.0.bn3.bias                         loaded from backbone.body.layer4.0.bn3.bias                         of shape (2048,)\n",
      "2021-05-27 06:45:45,973 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.0.bn3.num_batches_tracked          loaded from backbone.body.layer4.0.bn3.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,974 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.0.bn3.running_mean                 loaded from backbone.body.layer4.0.bn3.running_mean                 of shape (2048,)\n",
      "2021-05-27 06:45:45,974 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.0.bn3.running_var                  loaded from backbone.body.layer4.0.bn3.running_var                  of shape (2048,)\n",
      "2021-05-27 06:45:45,974 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.0.bn3.weight                       loaded from backbone.body.layer4.0.bn3.weight                       of shape (2048,)\n",
      "2021-05-27 06:45:45,974 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.0.conv1.weight                     loaded from backbone.body.layer4.0.conv1.weight                     of shape (512, 1024, 1, 1)\n",
      "2021-05-27 06:45:45,974 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.0.conv2.weight                     loaded from backbone.body.layer4.0.conv2.weight                     of shape (512, 512, 3, 3)\n",
      "2021-05-27 06:45:45,974 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.0.conv3.0.filt                     loaded from backbone.body.layer4.0.conv3.0.filt                     of shape (512, 1, 3, 3)\n",
      "2021-05-27 06:45:45,974 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.0.conv3.1.weight                   loaded from backbone.body.layer4.0.conv3.1.weight                   of shape (2048, 512, 1, 1)\n",
      "2021-05-27 06:45:45,974 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.0.downsample.0.filt                loaded from backbone.body.layer4.0.downsample.0.filt                of shape (1024, 1, 3, 3)\n",
      "2021-05-27 06:45:45,974 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.0.downsample.1.weight              loaded from backbone.body.layer4.0.downsample.1.weight              of shape (2048, 1024, 1, 1)\n",
      "2021-05-27 06:45:45,974 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.0.downsample.2.bias                loaded from backbone.body.layer4.0.downsample.2.bias                of shape (2048,)\n",
      "2021-05-27 06:45:45,975 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.0.downsample.2.num_batches_tracked loaded from backbone.body.layer4.0.downsample.2.num_batches_tracked of shape ()\n",
      "2021-05-27 06:45:45,975 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.0.downsample.2.running_mean        loaded from backbone.body.layer4.0.downsample.2.running_mean        of shape (2048,)\n",
      "2021-05-27 06:45:45,975 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.0.downsample.2.running_var         loaded from backbone.body.layer4.0.downsample.2.running_var         of shape (2048,)\n",
      "2021-05-27 06:45:45,975 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.0.downsample.2.weight              loaded from backbone.body.layer4.0.downsample.2.weight              of shape (2048,)\n",
      "2021-05-27 06:45:45,975 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.1.bn1.bias                         loaded from backbone.body.layer4.1.bn1.bias                         of shape (512,)\n",
      "2021-05-27 06:45:45,975 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.1.bn1.num_batches_tracked          loaded from backbone.body.layer4.1.bn1.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,975 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.1.bn1.running_mean                 loaded from backbone.body.layer4.1.bn1.running_mean                 of shape (512,)\n",
      "2021-05-27 06:45:45,975 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.1.bn1.running_var                  loaded from backbone.body.layer4.1.bn1.running_var                  of shape (512,)\n",
      "2021-05-27 06:45:45,975 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.1.bn1.weight                       loaded from backbone.body.layer4.1.bn1.weight                       of shape (512,)\n",
      "2021-05-27 06:45:45,976 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.1.bn2.bias                         loaded from backbone.body.layer4.1.bn2.bias                         of shape (512,)\n",
      "2021-05-27 06:45:45,976 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.1.bn2.num_batches_tracked          loaded from backbone.body.layer4.1.bn2.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,976 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.1.bn2.running_mean                 loaded from backbone.body.layer4.1.bn2.running_mean                 of shape (512,)\n",
      "2021-05-27 06:45:45,976 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.1.bn2.running_var                  loaded from backbone.body.layer4.1.bn2.running_var                  of shape (512,)\n",
      "2021-05-27 06:45:45,976 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.1.bn2.weight                       loaded from backbone.body.layer4.1.bn2.weight                       of shape (512,)\n",
      "2021-05-27 06:45:45,976 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.1.bn3.bias                         loaded from backbone.body.layer4.1.bn3.bias                         of shape (2048,)\n",
      "2021-05-27 06:45:45,976 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.1.bn3.num_batches_tracked          loaded from backbone.body.layer4.1.bn3.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,976 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.1.bn3.running_mean                 loaded from backbone.body.layer4.1.bn3.running_mean                 of shape (2048,)\n",
      "2021-05-27 06:45:45,976 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.1.bn3.running_var                  loaded from backbone.body.layer4.1.bn3.running_var                  of shape (2048,)\n",
      "2021-05-27 06:45:45,976 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.1.bn3.weight                       loaded from backbone.body.layer4.1.bn3.weight                       of shape (2048,)\n",
      "2021-05-27 06:45:45,976 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.1.conv1.weight                     loaded from backbone.body.layer4.1.conv1.weight                     of shape (512, 2048, 1, 1)\n",
      "2021-05-27 06:45:45,977 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.1.conv2.weight                     loaded from backbone.body.layer4.1.conv2.weight                     of shape (512, 512, 3, 3)\n",
      "2021-05-27 06:45:45,977 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.1.conv3.weight                     loaded from backbone.body.layer4.1.conv3.weight                     of shape (2048, 512, 1, 1)\n",
      "2021-05-27 06:45:45,977 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.2.bn1.bias                         loaded from backbone.body.layer4.2.bn1.bias                         of shape (512,)\n",
      "2021-05-27 06:45:45,977 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.2.bn1.num_batches_tracked          loaded from backbone.body.layer4.2.bn1.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,977 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.2.bn1.running_mean                 loaded from backbone.body.layer4.2.bn1.running_mean                 of shape (512,)\n",
      "2021-05-27 06:45:45,977 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.2.bn1.running_var                  loaded from backbone.body.layer4.2.bn1.running_var                  of shape (512,)\n",
      "2021-05-27 06:45:45,977 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.2.bn1.weight                       loaded from backbone.body.layer4.2.bn1.weight                       of shape (512,)\n",
      "2021-05-27 06:45:45,977 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.2.bn2.bias                         loaded from backbone.body.layer4.2.bn2.bias                         of shape (512,)\n",
      "2021-05-27 06:45:45,977 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.2.bn2.num_batches_tracked          loaded from backbone.body.layer4.2.bn2.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,978 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.2.bn2.running_mean                 loaded from backbone.body.layer4.2.bn2.running_mean                 of shape (512,)\n",
      "2021-05-27 06:45:45,978 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.2.bn2.running_var                  loaded from backbone.body.layer4.2.bn2.running_var                  of shape (512,)\n",
      "2021-05-27 06:45:45,978 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.2.bn2.weight                       loaded from backbone.body.layer4.2.bn2.weight                       of shape (512,)\n",
      "2021-05-27 06:45:45,978 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.2.bn3.bias                         loaded from backbone.body.layer4.2.bn3.bias                         of shape (2048,)\n",
      "2021-05-27 06:45:45,978 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.2.bn3.num_batches_tracked          loaded from backbone.body.layer4.2.bn3.num_batches_tracked          of shape ()\n",
      "2021-05-27 06:45:45,978 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.2.bn3.running_mean                 loaded from backbone.body.layer4.2.bn3.running_mean                 of shape (2048,)\n",
      "2021-05-27 06:45:45,978 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.2.bn3.running_var                  loaded from backbone.body.layer4.2.bn3.running_var                  of shape (2048,)\n",
      "2021-05-27 06:45:45,978 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.2.bn3.weight                       loaded from backbone.body.layer4.2.bn3.weight                       of shape (2048,)\n",
      "2021-05-27 06:45:45,978 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.2.conv1.weight                     loaded from backbone.body.layer4.2.conv1.weight                     of shape (512, 2048, 1, 1)\n",
      "2021-05-27 06:45:45,978 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.2.conv2.weight                     loaded from backbone.body.layer4.2.conv2.weight                     of shape (512, 512, 3, 3)\n",
      "2021-05-27 06:45:45,979 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.layer4.2.conv3.weight                     loaded from backbone.body.layer4.2.conv3.weight                     of shape (2048, 512, 1, 1)\n",
      "2021-05-27 06:45:45,979 maskrcnn_benchmark.utils.model_serialization INFO: backbone.body.maxpool.1.filt                            loaded from backbone.body.maxpool.1.filt                            of shape (64, 1, 3, 3)\n",
      "2021-05-27 06:45:45,979 maskrcnn_benchmark.utils.model_serialization INFO: backbone.fpn.fpn_inner2.bias                            loaded from backbone.fpn.fpn_inner2.bias                            of shape (256,)\n",
      "2021-05-27 06:45:45,979 maskrcnn_benchmark.utils.model_serialization INFO: backbone.fpn.fpn_inner2.weight                          loaded from backbone.fpn.fpn_inner2.weight                          of shape (256, 512, 1, 1)\n",
      "2021-05-27 06:45:45,979 maskrcnn_benchmark.utils.model_serialization INFO: backbone.fpn.fpn_inner3.bias                            loaded from backbone.fpn.fpn_inner3.bias                            of shape (256,)\n",
      "2021-05-27 06:45:45,979 maskrcnn_benchmark.utils.model_serialization INFO: backbone.fpn.fpn_inner3.weight                          loaded from backbone.fpn.fpn_inner3.weight                          of shape (256, 1024, 1, 1)\n",
      "2021-05-27 06:45:45,979 maskrcnn_benchmark.utils.model_serialization INFO: backbone.fpn.fpn_inner4.bias                            loaded from backbone.fpn.fpn_inner4.bias                            of shape (256,)\n",
      "2021-05-27 06:45:45,979 maskrcnn_benchmark.utils.model_serialization INFO: backbone.fpn.fpn_inner4.weight                          loaded from backbone.fpn.fpn_inner4.weight                          of shape (256, 2048, 1, 1)\n",
      "2021-05-27 06:45:45,979 maskrcnn_benchmark.utils.model_serialization INFO: backbone.fpn.fpn_layer2.bias                            loaded from backbone.fpn.fpn_layer2.bias                            of shape (256,)\n",
      "2021-05-27 06:45:45,979 maskrcnn_benchmark.utils.model_serialization INFO: backbone.fpn.fpn_layer2.weight                          loaded from backbone.fpn.fpn_layer2.weight                          of shape (256, 256, 3, 3)\n",
      "2021-05-27 06:45:45,980 maskrcnn_benchmark.utils.model_serialization INFO: backbone.fpn.fpn_layer3.bias                            loaded from backbone.fpn.fpn_layer3.bias                            of shape (256,)\n",
      "2021-05-27 06:45:45,980 maskrcnn_benchmark.utils.model_serialization INFO: backbone.fpn.fpn_layer3.weight                          loaded from backbone.fpn.fpn_layer3.weight                          of shape (256, 256, 3, 3)\n",
      "2021-05-27 06:45:45,980 maskrcnn_benchmark.utils.model_serialization INFO: backbone.fpn.fpn_layer4.bias                            loaded from backbone.fpn.fpn_layer4.bias                            of shape (256,)\n",
      "2021-05-27 06:45:45,980 maskrcnn_benchmark.utils.model_serialization INFO: backbone.fpn.fpn_layer4.weight                          loaded from backbone.fpn.fpn_layer4.weight                          of shape (256, 256, 3, 3)\n",
      "2021-05-27 06:45:45,980 maskrcnn_benchmark.utils.model_serialization INFO: backbone.fpn.top_blocks.p6.bias                         loaded from backbone.fpn.top_blocks.p6.bias                         of shape (256,)\n",
      "2021-05-27 06:45:45,980 maskrcnn_benchmark.utils.model_serialization INFO: backbone.fpn.top_blocks.p6.weight                       loaded from backbone.fpn.top_blocks.p6.weight                       of shape (256, 256, 3, 3)\n",
      "2021-05-27 06:45:45,980 maskrcnn_benchmark.utils.model_serialization INFO: backbone.fpn.top_blocks.p7.bias                         loaded from backbone.fpn.top_blocks.p7.bias                         of shape (256,)\n",
      "2021-05-27 06:45:45,980 maskrcnn_benchmark.utils.model_serialization INFO: backbone.fpn.top_blocks.p7.weight                       loaded from backbone.fpn.top_blocks.p7.weight                       of shape (256, 256, 3, 3)\n",
      "2021-05-27 06:45:45,980 maskrcnn_benchmark.utils.model_serialization INFO: heatmaps.deeplab.heatmaps.bias                          loaded from heatmaps.deeplab.heatmaps.bias                          of shape (3,)\n",
      "2021-05-27 06:45:45,980 maskrcnn_benchmark.utils.model_serialization INFO: heatmaps.deeplab.heatmaps.weight                        loaded from heatmaps.deeplab.heatmaps.weight                        of shape (3, 128, 3, 3)\n",
      "2021-05-27 06:45:45,981 maskrcnn_benchmark.utils.model_serialization INFO: heatmaps.deeplab.seg_head.0.weight                      loaded from heatmaps.deeplab.seg_head.0.weight                      of shape (128, 256, 3, 3)\n",
      "2021-05-27 06:45:45,981 maskrcnn_benchmark.utils.model_serialization INFO: heatmaps.deeplab.seg_head.1.bias                        loaded from heatmaps.deeplab.seg_head.1.bias                        of shape (128,)\n",
      "2021-05-27 06:45:45,981 maskrcnn_benchmark.utils.model_serialization INFO: heatmaps.deeplab.seg_head.1.num_batches_tracked         loaded from heatmaps.deeplab.seg_head.1.num_batches_tracked         of shape ()\n",
      "2021-05-27 06:45:45,981 maskrcnn_benchmark.utils.model_serialization INFO: heatmaps.deeplab.seg_head.1.running_mean                loaded from heatmaps.deeplab.seg_head.1.running_mean                of shape (128,)\n",
      "2021-05-27 06:45:45,981 maskrcnn_benchmark.utils.model_serialization INFO: heatmaps.deeplab.seg_head.1.running_var                 loaded from heatmaps.deeplab.seg_head.1.running_var                 of shape (128,)\n",
      "2021-05-27 06:45:45,981 maskrcnn_benchmark.utils.model_serialization INFO: heatmaps.deeplab.seg_head.1.weight                      loaded from heatmaps.deeplab.seg_head.1.weight                      of shape (128,)\n",
      "2021-05-27 06:45:45,981 maskrcnn_benchmark.utils.model_serialization INFO: heatmaps.deeplab.seg_head.3.weight                      loaded from heatmaps.deeplab.seg_head.3.weight                      of shape (128, 128, 3, 3)\n",
      "2021-05-27 06:45:45,981 maskrcnn_benchmark.utils.model_serialization INFO: heatmaps.deeplab.seg_head.4.bias                        loaded from heatmaps.deeplab.seg_head.4.bias                        of shape (128,)\n",
      "2021-05-27 06:45:45,981 maskrcnn_benchmark.utils.model_serialization INFO: heatmaps.deeplab.seg_head.4.num_batches_tracked         loaded from heatmaps.deeplab.seg_head.4.num_batches_tracked         of shape ()\n",
      "2021-05-27 06:45:45,981 maskrcnn_benchmark.utils.model_serialization INFO: heatmaps.deeplab.seg_head.4.running_mean                loaded from heatmaps.deeplab.seg_head.4.running_mean                of shape (128,)\n",
      "2021-05-27 06:45:45,982 maskrcnn_benchmark.utils.model_serialization INFO: heatmaps.deeplab.seg_head.4.running_var                 loaded from heatmaps.deeplab.seg_head.4.running_var                 of shape (128,)\n",
      "2021-05-27 06:45:45,982 maskrcnn_benchmark.utils.model_serialization INFO: heatmaps.deeplab.seg_head.4.weight                      loaded from heatmaps.deeplab.seg_head.4.weight                      of shape (128,)\n",
      "2021-05-27 06:45:45,982 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.centerness.bias                                loaded from rpn.head.centerness.bias                                of shape (1,)\n",
      "2021-05-27 06:45:45,982 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.centerness.weight                              loaded from rpn.head.centerness.weight                              of shape (1, 256, 3, 3)\n",
      "2021-05-27 06:45:45,982 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.cls_logits.bias                                loaded from rpn.head.cls_logits.bias                                of shape (1,)\n",
      "2021-05-27 06:45:45,982 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.cls_logits.weight                              loaded from rpn.head.cls_logits.weight                              of shape (1, 256, 3, 3)\n",
      "2021-05-27 06:45:45,982 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.cls_tower.0.bias                               loaded from rpn.head.cls_tower.0.bias                               of shape (256,)\n",
      "2021-05-27 06:45:45,982 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.cls_tower.0.weight                             loaded from rpn.head.cls_tower.0.weight                             of shape (256, 256, 3, 3)\n",
      "2021-05-27 06:45:45,982 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.cls_tower.1.bias                               loaded from rpn.head.cls_tower.1.bias                               of shape (256,)\n",
      "2021-05-27 06:45:45,982 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.cls_tower.1.weight                             loaded from rpn.head.cls_tower.1.weight                             of shape (256,)\n",
      "2021-05-27 06:45:45,983 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.cls_tower.10.bias                              loaded from rpn.head.cls_tower.10.bias                              of shape (256,)\n",
      "2021-05-27 06:45:45,983 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.cls_tower.10.weight                            loaded from rpn.head.cls_tower.10.weight                            of shape (256,)\n",
      "2021-05-27 06:45:45,983 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.cls_tower.3.bias                               loaded from rpn.head.cls_tower.3.bias                               of shape (256,)\n",
      "2021-05-27 06:45:45,983 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.cls_tower.3.weight                             loaded from rpn.head.cls_tower.3.weight                             of shape (256, 256, 3, 3)\n",
      "2021-05-27 06:45:45,983 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.cls_tower.4.bias                               loaded from rpn.head.cls_tower.4.bias                               of shape (256,)\n",
      "2021-05-27 06:45:45,983 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.cls_tower.4.weight                             loaded from rpn.head.cls_tower.4.weight                             of shape (256,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-27 06:45:45,983 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.cls_tower.6.bias                               loaded from rpn.head.cls_tower.6.bias                               of shape (256,)\n",
      "2021-05-27 06:45:45,983 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.cls_tower.6.weight                             loaded from rpn.head.cls_tower.6.weight                             of shape (256, 256, 3, 3)\n",
      "2021-05-27 06:45:45,983 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.cls_tower.7.bias                               loaded from rpn.head.cls_tower.7.bias                               of shape (256,)\n",
      "2021-05-27 06:45:45,984 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.cls_tower.7.weight                             loaded from rpn.head.cls_tower.7.weight                             of shape (256,)\n",
      "2021-05-27 06:45:45,984 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.cls_tower.9.bias                               loaded from rpn.head.cls_tower.9.bias                               of shape (256,)\n",
      "2021-05-27 06:45:45,984 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.cls_tower.9.weight                             loaded from rpn.head.cls_tower.9.weight                             of shape (256, 256, 3, 3)\n",
      "2021-05-27 06:45:45,984 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.kps_bases.bias                                 loaded from rpn.head.kps_bases.bias                                 of shape (2,)\n",
      "2021-05-27 06:45:45,984 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.kps_bases.weight                               loaded from rpn.head.kps_bases.weight                               of shape (2, 64, 3, 3)\n",
      "2021-05-27 06:45:45,984 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.kps_offsets.0.kps_offsets.bias                 loaded from rpn.head.kps_offsets.0.kps_offsets.bias                 of shape (6,)\n",
      "2021-05-27 06:45:45,984 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.kps_offsets.0.kps_offsets.weight               loaded from rpn.head.kps_offsets.0.kps_offsets.weight               of shape (6, 64, 1, 1)\n",
      "2021-05-27 06:45:45,984 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.kps_tower.0.bias                               loaded from rpn.head.kps_tower.0.bias                               of shape (64,)\n",
      "2021-05-27 06:45:45,984 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.kps_tower.0.weight                             loaded from rpn.head.kps_tower.0.weight                             of shape (64, 256, 3, 3)\n",
      "2021-05-27 06:45:45,984 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.kps_tower.1.bias                               loaded from rpn.head.kps_tower.1.bias                               of shape (64,)\n",
      "2021-05-27 06:45:45,985 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.kps_tower.1.weight                             loaded from rpn.head.kps_tower.1.weight                             of shape (64,)\n",
      "2021-05-27 06:45:45,985 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.kps_tower.10.bias                              loaded from rpn.head.kps_tower.10.bias                              of shape (64,)\n",
      "2021-05-27 06:45:45,985 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.kps_tower.10.weight                            loaded from rpn.head.kps_tower.10.weight                            of shape (64,)\n",
      "2021-05-27 06:45:45,985 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.kps_tower.3.bias                               loaded from rpn.head.kps_tower.3.bias                               of shape (64,)\n",
      "2021-05-27 06:45:45,985 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.kps_tower.3.weight                             loaded from rpn.head.kps_tower.3.weight                             of shape (64, 64, 3, 3)\n",
      "2021-05-27 06:45:45,985 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.kps_tower.4.bias                               loaded from rpn.head.kps_tower.4.bias                               of shape (64,)\n",
      "2021-05-27 06:45:45,985 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.kps_tower.4.weight                             loaded from rpn.head.kps_tower.4.weight                             of shape (64,)\n",
      "2021-05-27 06:45:45,985 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.kps_tower.6.bias                               loaded from rpn.head.kps_tower.6.bias                               of shape (64,)\n",
      "2021-05-27 06:45:45,985 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.kps_tower.6.weight                             loaded from rpn.head.kps_tower.6.weight                             of shape (64, 64, 3, 3)\n",
      "2021-05-27 06:45:45,986 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.kps_tower.7.bias                               loaded from rpn.head.kps_tower.7.bias                               of shape (64,)\n",
      "2021-05-27 06:45:45,986 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.kps_tower.7.weight                             loaded from rpn.head.kps_tower.7.weight                             of shape (64,)\n",
      "2021-05-27 06:45:45,986 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.kps_tower.9.bias                               loaded from rpn.head.kps_tower.9.bias                               of shape (64,)\n",
      "2021-05-27 06:45:45,986 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.kps_tower.9.weight                             loaded from rpn.head.kps_tower.9.weight                             of shape (64, 64, 3, 3)\n",
      "2021-05-27 06:45:45,986 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.sample_features_conv.0.bias                    loaded from rpn.head.sample_features_conv.0.bias                    of shape (64,)\n",
      "2021-05-27 06:45:45,986 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.sample_features_conv.0.weight                  loaded from rpn.head.sample_features_conv.0.weight                  of shape (64, 64, 3, 3)\n",
      "2021-05-27 06:45:45,986 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.sample_features_conv.1.bias                    loaded from rpn.head.sample_features_conv.1.bias                    of shape (64,)\n",
      "2021-05-27 06:45:45,986 maskrcnn_benchmark.utils.model_serialization INFO: rpn.head.sample_features_conv.1.weight                  loaded from rpn.head.sample_features_conv.1.weight                  of shape (64,)\n",
      "2021-05-27 06:45:46,063 maskrcnn_benchmark.utils.checkpoint INFO: Loading optimizer from training_dir/250/standardwhee/1/fcos_kps_ms_training_R_50_FPN_1x_bee/model_0002000.pth\n",
      "2021-05-27 06:45:46,199 maskrcnn_benchmark.utils.checkpoint INFO: Loading scheduler from training_dir/250/standardwhee/1/fcos_kps_ms_training_R_50_FPN_1x_bee/model_0002000.pth\n",
      "train san check: True\n",
      "im p gpu:  3\n",
      "2021-05-27 06:45:46,200 maskrcnn_benchmark.data.build WARNING: When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14\n",
      "min train range(480, 1601)\n",
      "max train: 2666\n",
      "max test: 1333\n",
      "min test : 800\n",
      "crop size,,, 800\n",
      "name?\n",
      "datasets/pup/annotations/pup_train2.json\n",
      "datasets/pup/images\n",
      "PupKeypoints\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "len ids pre  798\n",
      "name?\n",
      "datasets/pup/annotations/pup_train2.json\n",
      "datasets/pup/images\n",
      "PupKeypoints\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "len ids pre  798\n",
      "DatasetCatalog <class 'maskrcnn_benchmark.config.paths_catalog.DatasetCatalog'>\n",
      "dataset_l ([Dataset COCODataset\n",
      "    Number of datapoints: 798\n",
      "    Root Location: datasets/pup/images\n",
      "    Transforms (if any): None\n",
      "    Target Transforms (if any): None], {0: 0, 1: 1, 2: 2, 3: 4, 4: 5, 5: 6, 6: 7, 7: 8, 8: 9, 9: 11, 10: 12, 11: 13, 12: 14, 13: 15, 14: 17, 15: 18, 16: 19, 17: 21, 18: 23, 19: 24, 20: 25, 21: 27, 22: 29, 23: 30, 24: 31, 25: 32, 26: 33, 27: 34, 28: 35, 29: 36, 30: 37, 31: 38, 32: 39, 33: 40, 34: 41, 35: 42, 36: 43, 37: 44, 38: 45, 39: 46, 40: 47, 41: 48, 42: 49, 43: 50, 44: 51, 45: 52, 46: 53, 47: 54, 48: 55, 49: 56, 50: 57, 51: 59, 52: 60, 53: 61, 54: 63, 55: 65, 56: 66, 57: 67, 58: 68, 59: 69, 60: 71, 61: 73, 62: 74, 63: 75, 64: 76, 65: 78, 66: 83, 67: 85, 68: 86, 69: 87, 70: 88, 71: 90, 72: 91, 73: 92, 74: 93, 75: 94, 76: 95, 77: 97, 78: 98, 79: 99, 80: 100, 81: 101, 82: 102, 83: 103, 84: 104, 85: 105, 86: 106, 87: 107, 88: 108, 89: 109, 90: 110, 91: 111, 92: 112, 93: 113, 94: 114, 95: 115, 96: 116, 97: 117, 98: 118, 99: 119, 100: 125, 101: 126, 102: 127, 103: 128, 104: 129, 105: 130, 106: 131, 107: 132, 108: 136, 109: 138, 110: 139, 111: 140, 112: 141, 113: 142, 114: 143, 115: 145, 116: 146, 117: 147, 118: 148, 119: 151, 120: 152, 121: 153, 122: 156, 123: 157, 124: 158, 125: 170, 126: 171, 127: 172, 128: 176, 129: 177, 130: 180, 131: 183, 132: 185, 133: 186, 134: 187, 135: 190, 136: 191, 137: 193, 138: 194, 139: 196, 140: 197, 141: 199, 142: 203, 143: 204, 144: 205, 145: 206, 146: 207, 147: 208, 148: 209, 149: 210, 150: 211, 151: 212, 152: 213, 153: 214, 154: 215, 155: 216, 156: 217, 157: 218, 158: 219, 159: 220, 160: 223, 161: 224, 162: 225, 163: 226, 164: 227, 165: 229, 166: 230, 167: 232, 168: 233, 169: 234, 170: 237, 171: 249, 172: 250, 173: 251, 174: 252, 175: 254, 176: 255, 177: 256, 178: 257, 179: 258, 180: 259, 181: 260, 182: 261, 183: 264, 184: 265, 185: 266, 186: 267, 187: 268, 188: 269, 189: 270, 190: 271, 191: 272, 192: 281, 193: 282, 194: 285, 195: 287, 196: 288, 197: 289, 198: 290, 199: 291, 200: 292, 201: 293, 202: 294, 203: 296, 204: 297, 205: 298, 206: 299, 207: 300, 208: 302, 209: 304, 210: 305, 211: 306, 212: 307, 213: 308, 214: 309, 215: 310, 216: 312, 217: 313, 218: 314, 219: 316, 220: 317, 221: 318, 222: 331, 223: 333, 224: 334, 225: 336, 226: 337, 227: 338, 228: 339, 229: 343, 230: 344, 231: 345, 232: 346, 233: 347, 234: 348, 235: 350, 236: 352, 237: 358, 238: 360, 239: 361, 240: 362, 241: 363, 242: 364, 243: 366, 244: 367, 245: 368, 246: 369, 247: 371, 248: 372, 249: 373, 250: 374, 251: 375, 252: 376, 253: 377, 254: 378, 255: 380, 256: 381, 257: 382, 258: 383, 259: 384, 260: 385, 261: 386, 262: 387, 263: 389, 264: 390, 265: 391, 266: 392, 267: 394, 268: 395, 269: 396, 270: 397, 271: 398, 272: 399, 273: 401, 274: 402, 275: 403, 276: 404, 277: 405, 278: 406, 279: 408, 280: 410, 281: 411, 282: 414, 283: 415, 284: 416, 285: 417, 286: 419, 287: 420, 288: 423, 289: 425, 290: 427, 291: 429, 292: 431, 293: 433, 294: 434, 295: 435, 296: 436, 297: 437, 298: 438, 299: 439, 300: 448, 301: 449, 302: 450, 303: 451, 304: 453, 305: 454, 306: 455, 307: 457, 308: 458, 309: 459, 310: 460, 311: 461, 312: 462, 313: 463, 314: 466, 315: 467, 316: 468, 317: 471, 318: 473, 319: 474, 320: 475, 321: 476, 322: 477, 323: 478, 324: 479, 325: 484, 326: 485, 327: 486, 328: 487, 329: 489, 330: 491, 331: 492, 332: 493, 333: 494, 334: 495, 335: 496, 336: 497, 337: 498, 338: 499, 339: 500, 340: 501, 341: 503, 342: 504, 343: 505, 344: 506, 345: 507, 346: 508, 347: 509, 348: 512, 349: 513, 350: 514, 351: 517, 352: 518, 353: 519, 354: 525, 355: 526, 356: 527, 357: 528, 358: 529, 359: 530, 360: 531, 361: 534, 362: 535, 363: 537, 364: 538, 365: 540, 366: 541, 367: 542, 368: 544, 369: 546, 370: 551, 371: 554, 372: 555, 373: 558, 374: 559, 375: 560, 376: 561, 377: 562, 378: 563, 379: 564, 380: 565, 381: 567, 382: 570, 383: 571, 384: 572, 385: 574, 386: 575, 387: 576, 388: 577, 389: 579, 390: 583, 391: 584, 392: 585, 393: 586, 394: 587, 395: 588, 396: 589, 397: 590, 398: 591, 399: 592, 400: 594, 401: 595, 402: 597, 403: 598, 404: 599, 405: 603, 406: 604, 407: 605, 408: 606, 409: 607, 410: 608, 411: 609, 412: 610, 413: 611, 414: 612, 415: 613, 416: 614, 417: 615, 418: 618, 419: 619, 420: 620, 421: 621, 422: 622, 423: 623, 424: 624, 425: 625, 426: 627, 427: 628, 428: 629, 429: 630, 430: 631, 431: 632, 432: 633, 433: 637, 434: 638, 435: 643, 436: 644, 437: 645, 438: 647, 439: 648, 440: 649, 441: 651, 442: 653, 443: 654, 444: 655, 445: 656, 446: 658, 447: 660, 448: 661, 449: 662, 450: 664, 451: 665, 452: 666, 453: 667, 454: 669, 455: 670, 456: 672, 457: 675, 458: 676, 459: 677, 460: 678, 461: 679, 462: 680, 463: 684, 464: 685, 465: 686, 466: 690, 467: 691, 468: 692, 469: 693, 470: 694, 471: 696, 472: 697, 473: 698, 474: 699, 475: 700, 476: 701, 477: 702, 478: 703, 479: 704, 480: 705, 481: 707, 482: 708, 483: 709, 484: 710, 485: 711, 486: 717, 487: 719, 488: 727, 489: 733, 490: 734, 491: 735, 492: 737, 493: 738, 494: 739, 495: 740, 496: 741, 497: 743, 498: 744, 499: 746, 500: 747, 501: 748, 502: 751, 503: 752, 504: 754, 505: 755, 506: 763, 507: 764, 508: 765, 509: 766, 510: 767, 511: 768, 512: 769, 513: 770, 514: 772, 515: 773, 516: 774, 517: 775, 518: 776, 519: 777, 520: 778, 521: 779, 522: 780, 523: 782, 524: 783, 525: 784, 526: 786, 527: 787, 528: 788, 529: 789, 530: 791, 531: 792, 532: 794, 533: 795, 534: 797, 535: 798, 536: 799, 537: 802, 538: 804, 539: 805, 540: 806, 541: 807, 542: 808, 543: 809, 544: 810, 545: 811, 546: 812, 547: 813, 548: 814, 549: 815, 550: 816, 551: 817, 552: 818, 553: 819, 554: 820, 555: 822, 556: 823, 557: 824, 558: 825, 559: 826, 560: 827, 561: 828, 562: 829, 563: 830, 564: 831, 565: 832, 566: 834, 567: 835, 568: 836, 569: 838, 570: 839, 571: 850, 572: 851, 573: 852, 574: 854, 575: 855, 576: 858, 577: 859, 578: 860, 579: 861, 580: 862, 581: 863, 582: 865, 583: 866, 584: 868, 585: 869, 586: 870, 587: 871, 588: 873, 589: 874, 590: 875, 591: 876, 592: 878, 593: 886, 594: 887, 595: 888, 596: 890, 597: 891, 598: 892, 599: 893, 600: 894, 601: 895, 602: 896, 603: 898, 604: 899, 605: 900, 606: 901, 607: 902, 608: 903, 609: 904, 610: 906, 611: 908, 612: 909, 613: 910, 614: 911, 615: 912, 616: 913, 617: 914, 618: 915, 619: 916, 620: 918, 621: 932, 622: 933, 623: 934, 624: 935, 625: 936, 626: 942, 627: 945, 628: 946, 629: 947, 630: 948, 631: 949, 632: 950, 633: 951, 634: 952, 635: 953, 636: 954, 637: 955, 638: 960, 639: 961, 640: 962, 641: 963, 642: 964, 643: 965, 644: 966, 645: 967, 646: 968, 647: 969, 648: 970, 649: 971, 650: 972, 651: 973, 652: 974, 653: 976, 654: 977, 655: 979, 656: 980, 657: 982, 658: 983, 659: 984, 660: 985, 661: 986, 662: 987, 663: 988, 664: 989, 665: 990, 666: 991, 667: 992, 668: 993, 669: 995, 670: 996, 671: 1004, 672: 1005, 673: 1008, 674: 1013, 675: 1014, 676: 1016, 677: 1017, 678: 1019, 679: 1020, 680: 1021, 681: 1022, 682: 1023, 683: 1024, 684: 1025, 685: 1026, 686: 1027, 687: 1028, 688: 1031, 689: 1033, 690: 1034, 691: 1035, 692: 1036, 693: 1037, 694: 1038, 695: 1039, 696: 1041, 697: 1042, 698: 1043, 699: 1044, 700: 1045, 701: 1047, 702: 1048, 703: 1051, 704: 1052, 705: 1053, 706: 1054, 707: 1055, 708: 1056, 709: 1057, 710: 1058, 711: 1059, 712: 1060, 713: 1061, 714: 1062, 715: 1063, 716: 1064, 717: 1065, 718: 1066, 719: 1067, 720: 1068, 721: 1071, 722: 1072, 723: 1073, 724: 1074, 725: 1075, 726: 1076, 727: 1079, 728: 1092, 729: 1093, 730: 1095, 731: 1096, 732: 1097, 733: 1098, 734: 1099, 735: 1100, 736: 1101, 737: 1102, 738: 1104, 739: 1105, 740: 1106, 741: 1108, 742: 1109, 743: 1110, 744: 1111, 745: 1112, 746: 1123, 747: 1124, 748: 1125, 749: 1127, 750: 1129, 751: 1130, 752: 1131, 753: 1132, 754: 1133, 755: 1134, 756: 1135, 757: 1137, 758: 1138, 759: 1139, 760: 1140, 761: 1141, 762: 1143, 763: 1144, 764: 1145, 765: 1146, 766: 1148, 767: 1149, 768: 1152, 769: 1153, 770: 1154, 771: 1155, 772: 1157, 773: 1158, 774: 1163, 775: 1164, 776: 1165, 777: 1166, 778: 1167, 779: 1168, 780: 1169, 781: 1170, 782: 1171, 783: 1173, 784: 1174, 785: 1176, 786: 1177, 787: 1178, 788: 1179, 789: 1180, 790: 1181, 791: 1182, 792: 1183, 793: 1185, 794: 1186, 795: 1187, 796: 1190, 797: 1192}) 2 1 798\n",
      "len data l 1\n",
      "data yo\n",
      "Dataset COCODataset\n",
      "    Number of datapoints: 798\n",
      "    Root Location: datasets/pup/images\n",
      "    Transforms (if any): None\n",
      "    Target Transforms (if any): None\n",
      "samp 1 [254, 432, 476, 318, 703, 513, 668, 334, 445, 188, 697, 533, 396, 504, 657, 674, 417, 202, 103, 368, 470, 518, 14, 98, 375, 676, 21, 5, 266, 353, 165, 512, 521, 431, 413, 516, 406, 323, 461, 39, 87, 176, 651, 420, 363, 154, 579, 760, 105, 230, 437, 492, 301, 499, 149, 273, 696, 109, 104, 24, 716, 434, 450, 395, 171, 794, 670, 596, 731, 333, 770, 398, 637, 414, 190, 206, 782, 549, 325, 130, 238, 542, 594, 305, 711, 374, 383, 573, 454, 140, 605, 655, 564, 68, 319, 302, 291, 541, 181, 539, 252, 463, 295, 210, 77, 121, 79, 191, 91, 142, 653, 638, 456, 617, 349, 792, 60, 195, 83, 277, 449, 723, 120, 702, 462, 679, 441, 606, 783, 748, 54, 776, 86, 624, 640, 196, 710, 427, 403, 773, 143, 448, 35, 740, 352, 665, 289, 298, 267, 717, 661, 373, 790, 49, 316, 192, 218, 370, 89, 217, 280, 712, 426, 734, 429, 354, 718, 479, 34, 544, 532, 675, 124, 389, 164, 440, 345, 369, 623, 93, 508, 50, 158, 88, 46, 646, 115, 300, 255, 405, 356, 161, 438, 767, 614, 148, 581, 598, 787, 747, 211, 706, 37, 411, 335, 677, 350, 713, 608, 197, 15, 65, 589, 0, 418, 742, 648, 493, 99, 730, 399, 639, 540, 177, 756, 170, 694, 451, 81, 157, 737, 642, 447, 649, 565, 555, 496, 108, 13, 755, 534, 465, 772, 481, 669, 361, 704, 471, 262, 488, 662, 402, 174, 66, 241, 691, 392, 339, 720, 152, 117, 276, 725, 421, 162, 699, 55, 726, 690, 543, 219, 365, 546, 166, 178, 338, 695, 380, 337, 44, 753, 393, 610, 327, 341, 248, 271, 391, 483, 172, 443, 739, 390, 138, 290, 43, 213, 10, 460, 523, 789, 663, 796, 477, 194, 371, 547, 671, 221, 505, 386, 714, 436, 260, 385, 425, 480, 664, 700, 724, 387, 527, 58, 346, 63, 613, 182, 360, 382, 253, 475, 689, 572, 682, 469, 322, 222, 16, 208, 287, 752, 281, 67, 96, 304, 401, 693, 107, 321, 635, 484, 240, 244, 467, 328, 529, 136, 793, 641, 269, 70, 23, 312, 515, 478, 153, 616, 444, 759, 52, 628, 343, 394, 509, 106, 557, 225, 132, 678, 51, 560, 751, 306, 123, 795, 227, 400, 446, 237, 482, 599, 550, 364, 282, 185, 419, 466, 160, 768, 721, 530, 684, 118, 551, 563, 112, 127, 232, 548, 591, 619, 660, 362, 283, 296, 771, 525, 101, 263, 344, 324, 384, 18, 575, 433, 615, 357, 36, 745, 274, 397, 284, 435, 486, 239, 53, 129, 566, 673, 558, 209, 761, 313, 636, 601, 735, 506, 497, 681, 672, 667, 489, 264, 600, 458, 293, 245, 590, 135, 584, 38, 621, 272, 180, 71, 797, 320, 620, 156, 139, 308, 500, 114, 501, 41, 145, 69, 571, 128, 7, 317, 134, 297, 379, 439, 687, 507, 578, 146, 183, 514, 250, 199, 643, 62, 212, 779, 556, 485, 685, 764, 459, 428, 372, 592, 215, 708, 22, 415, 762, 311, 632, 100, 144, 40, 133, 200, 765, 358, 257, 658, 791, 85, 26, 604, 359, 90, 376, 336, 3, 552, 407, 627, 286, 597, 329, 652, 75, 574, 268, 455, 769, 775, 495, 612, 746, 487, 659, 535, 491, 1, 656, 629, 259, 61, 307, 47, 330, 424, 422, 331, 288, 378, 588, 175, 688, 332, 32, 784, 159, 722, 634, 204, 644, 33, 423, 299, 355, 502, 72, 92, 494, 577, 275, 607, 409, 686, 340, 630, 464, 416, 633, 538, 31, 707, 583, 56, 2, 167, 223, 122, 19, 261, 698, 585, 367, 45, 150, 12, 602, 303, 777, 205, 569, 247, 141, 342, 520, 582, 198, 315, 256, 580, 74, 473, 76, 750, 28, 622, 102, 763, 80, 680, 618, 224, 17, 732, 781, 20, 207, 78, 229, 595, 568, 498, 576, 169, 692, 452, 48, 709, 609, 95, 528, 216, 562, 587, 203, 110, 8, 187, 27, 626, 788, 29, 785, 410, 42, 472, 11, 531, 377, 736, 503, 408, 381, 285, 126, 650, 168, 545, 314, 151, 645, 97, 186, 4, 294, 184, 517, 743, 179, 561, 647, 554, 366, 519, 25, 510, 84, 57, 125, 522, 524, 683, 347, 593, 235, 474, 570, 201, 526, 116, 73, 9, 326, 246, 586, 249, 279, 214, 567, 727, 351, 404, 705, 82, 113, 778, 265, 457, 780, 666, 226, 631, 744, 611, 749, 228, 292, 786, 453, 147, 715, 654, 388, 94, 559, 758, 258, 430, 511, 757, 553, 59, 536, 348, 243, 537, 163, 754, 119, 234, 173, 729, 193, 111, 30, 64, 309, 738, 251, 137, 733, 155, 490, 233, 242, 6, 774, 603, 442, 220, 131, 468, 189, 310, 231, 766, 728, 701, 741, 270, 236, 412, 719, 278, 625]\n",
      "DATASET Dataset COCODataset\n",
      "    Number of datapoints: 798\n",
      "    Root Location: datasets/pup/images\n",
      "    Transforms (if any): None\n",
      "    Target Transforms (if any): None 798\n",
      "data yo\n",
      "Dataset COCODataset\n",
      "    Number of datapoints: 798\n",
      "    Root Location: datasets/pup/images\n",
      "    Transforms (if any): None\n",
      "    Target Transforms (if any): None\n",
      "samp 2 <torch.utils.data.sampler.RandomSampler object at 0x2b3179c53860>\n",
      "DATASET Dataset COCODataset\n",
      "    Number of datapoints: 798\n",
      "    Root Location: datasets/pup/images\n",
      "    Transforms (if any): None\n",
      "    Target Transforms (if any): None 798\n",
      "lem lab 180000\n",
      "lem un 180000\n",
      "2021-05-27 06:45:46,889 maskrcnn_benchmark.trainer INFO: Start training\n",
      "DL1:  <torch.utils.data.dataloader.DataLoader object at 0x2b3179c53940> 180000\n",
      "DL2:  <torch.utils.data.dataloader.DataLoader object at 0x2b3179c53978> 180000 798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH:  [356, 76, 513, 586, 340]\n",
      "BATCH:  [557, 752, 365, 576, 407]\n",
      "BATCH:  [450, 41, 70, 672, 781]\n",
      "BATCH:  [684, 625, 184, 602, 704]\n",
      "BATCH:  [197]\n",
      "BATCH:  [294]\n",
      "BATCH:  [626]\n",
      "BATCH:  [621]\n",
      "BATCH:  [262, 551, 787, 444, 727]\n",
      "BATCH:  [560]\n",
      "bm size:  torch.Size([6, 3, 608, 608])\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=3, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "CUDAA\n",
      "summm:  tensor([[5.7650e-06, 1.9161e-08, 2.5636e-08],\n",
      "        [3.4297e-08, 1.4076e-10, 8.4365e-10],\n",
      "        [3.5263e-08, 1.4012e-10, 8.3227e-10],\n",
      "        ...,\n",
      "        [4.2815e-09, 2.2886e-12, 3.2672e-10],\n",
      "        [4.9116e-09, 7.6569e-12, 5.4637e-10],\n",
      "        [3.8517e-07, 1.2545e-08, 1.5962e-07]], device='cuda:0',\n",
      "       grad_fn=<_SigmoidFocalLossBackward>)\n",
      "well heres the loss ay:  tensor(0.6916, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "NUM POS 12\n",
      "TARGETS:  [BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=3, image_width=599, image_height=599, mode=xyxy)]\n",
      "feats:  5\n",
      "feats:  5\n",
      "feats:  256\n",
      "SHAPE THING torch.Size([5, 6, 76, 76])\n",
      "SHAPE THING torch.Size([5, 6, 38, 38])\n",
      "SHAPE THING torch.Size([5, 6, 19, 19])\n",
      "CUDAA\n",
      "summm:  tensor([[3.5619e-07],\n",
      "        [6.3442e-10],\n",
      "        [2.8661e-12],\n",
      "        ...,\n",
      "        [1.8193e-14],\n",
      "        [3.2874e-12],\n",
      "        [1.4153e-08]], device='cuda:0', grad_fn=<_SigmoidFocalLossBackward>)\n",
      "props grcnn\n",
      "[BoxList(num_boxes=23, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=23, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=18, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=7, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=0, image_width=600, image_height=600, mode=xyxy)]\n",
      "0time BoxList(num_boxes=23, image_width=600, image_height=600, mode=xyxy)\n",
      "loss is not heatmap\n",
      "loss_dict {'loss_cls': tensor(0.6703, device='cuda:0', grad_fn=<DivBackward0>), 'loss_centerness': tensor(0.6938, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_kps': tensor(2.5552, device='cuda:0', grad_fn=<DivBackward0>), 'heatmaps_loss': tensor(2.7665, device='cuda:0', grad_fn=<MulBackward0>)}\n",
      "losses  tensor(6.6858, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss_dict_reduced  {'loss_cls': tensor(0.6703, device='cuda:0', grad_fn=<DivBackward0>), 'loss_centerness': tensor(0.6938, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_kps': tensor(2.5552, device='cuda:0', grad_fn=<DivBackward0>), 'heatmaps_loss': tensor(2.7665, device='cuda:0', grad_fn=<MulBackward0>)}\n",
      "losses_reduced  tensor(6.6858, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ITER 2001\n",
      "BATCH:  [558, 137, 49, 449, 511]\n",
      "BATCH:  [193]\n",
      "bm size:  torch.Size([6, 3, 608, 608])\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=2, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "CUDAA\n",
      "summm:  tensor([[3.8080e-05, 3.7866e-07, 1.5871e-07],\n",
      "        [1.3986e-06, 2.4717e-08, 2.9796e-08],\n",
      "        [8.7886e-06, 1.6156e-07, 1.1725e-07],\n",
      "        ...,\n",
      "        [6.2746e-08, 5.7901e-10, 4.5094e-09],\n",
      "        [5.5958e-08, 3.7751e-10, 3.8490e-09],\n",
      "        [2.2651e-06, 8.7475e-08, 4.2598e-07]], device='cuda:0',\n",
      "       grad_fn=<_SigmoidFocalLossBackward>)\n",
      "well heres the loss ay:  tensor(0.6597, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "NUM POS 14\n",
      "TARGETS:  [BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=2, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)]\n",
      "feats:  5\n",
      "feats:  5\n",
      "feats:  256\n",
      "SHAPE THING torch.Size([5, 6, 76, 76])\n",
      "SHAPE THING torch.Size([5, 6, 38, 38])\n",
      "SHAPE THING torch.Size([5, 6, 19, 19])\n",
      "CUDAA\n",
      "summm:  tensor([[8.4596e-06],\n",
      "        [4.6578e-07],\n",
      "        [1.8954e-07],\n",
      "        ...,\n",
      "        [1.1712e-13],\n",
      "        [3.4209e-11],\n",
      "        [1.1463e-07]], device='cuda:0', grad_fn=<_SigmoidFocalLossBackward>)\n",
      "props grcnn\n",
      "[BoxList(num_boxes=22, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=29, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=33, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=23, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=31, image_width=600, image_height=600, mode=xyxy)]\n",
      "0time BoxList(num_boxes=22, image_width=600, image_height=600, mode=xyxy)\n",
      "loss is not heatmap\n",
      "loss_dict {'loss_cls': tensor(0.6574, device='cuda:0', grad_fn=<DivBackward0>), 'loss_centerness': tensor(0.6902, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_kps': tensor(2.4248, device='cuda:0', grad_fn=<DivBackward0>), 'heatmaps_loss': tensor(2.6389, device='cuda:0', grad_fn=<MulBackward0>)}\n",
      "losses  tensor(6.4113, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss_dict_reduced  {'loss_cls': tensor(0.6574, device='cuda:0', grad_fn=<DivBackward0>), 'loss_centerness': tensor(0.6902, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_kps': tensor(2.4248, device='cuda:0', grad_fn=<DivBackward0>), 'heatmaps_loss': tensor(2.6389, device='cuda:0', grad_fn=<MulBackward0>)}\n",
      "losses_reduced  tensor(6.4113, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ITER 2002\n",
      "BATCH:  [644, 224, 164, 531, 69]\n",
      "BATCH:  [360]\n",
      "bm size:  torch.Size([6, 3, 608, 608])\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=2, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "CUDAA\n",
      "summm:  tensor([[6.8706e-05, 1.8315e-06, 6.4570e-07],\n",
      "        [2.1852e-06, 7.8022e-08, 8.1622e-08],\n",
      "        [2.2499e-06, 6.7840e-08, 7.1010e-08],\n",
      "        ...,\n",
      "        [4.3484e-08, 1.9651e-09, 1.1141e-08],\n",
      "        [5.4800e-08, 1.8702e-09, 1.1834e-08],\n",
      "        [2.4614e-06, 2.7909e-07, 8.4623e-07]], device='cuda:0',\n",
      "       grad_fn=<_SigmoidFocalLossBackward>)\n",
      "well heres the loss ay:  tensor(0.6256, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "NUM POS 13\n",
      "TARGETS:  [BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=2, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)]\n",
      "feats:  5\n",
      "feats:  5\n",
      "feats:  256\n",
      "SHAPE THING torch.Size([5, 6, 76, 76])\n",
      "SHAPE THING torch.Size([5, 6, 38, 38])\n",
      "SHAPE THING torch.Size([5, 6, 19, 19])\n",
      "CUDAA\n",
      "summm:  tensor([[1.2581e-05],\n",
      "        [5.6441e-07],\n",
      "        [6.5149e-08],\n",
      "        ...,\n",
      "        [2.5397e-12],\n",
      "        [6.6958e-10],\n",
      "        [7.2823e-07]], device='cuda:0', grad_fn=<_SigmoidFocalLossBackward>)\n",
      "props grcnn\n",
      "[BoxList(num_boxes=69, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=40, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=68, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=73, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=54, image_width=600, image_height=600, mode=xyxy)]\n",
      "0time BoxList(num_boxes=69, image_width=600, image_height=600, mode=xyxy)\n",
      "loss is not heatmap\n",
      "loss_dict {'loss_cls': tensor(0.6213, device='cuda:0', grad_fn=<DivBackward0>), 'loss_centerness': tensor(0.6812, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_kps': tensor(1.9293, device='cuda:0', grad_fn=<DivBackward0>), 'heatmaps_loss': tensor(2.5022, device='cuda:0', grad_fn=<MulBackward0>)}\n",
      "losses  tensor(5.7340, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss_dict_reduced  {'loss_cls': tensor(0.6213, device='cuda:0', grad_fn=<DivBackward0>), 'loss_centerness': tensor(0.6812, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_kps': tensor(1.9293, device='cuda:0', grad_fn=<DivBackward0>), 'heatmaps_loss': tensor(2.5022, device='cuda:0', grad_fn=<MulBackward0>)}\n",
      "losses_reduced  tensor(5.7340, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ITER 2003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH:  [523, 746, 404, 675, 172]\n",
      "BATCH:  [268]\n",
      "bm size:  torch.Size([6, 3, 608, 608])\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=2, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "CUDAA\n",
      "summm:  tensor([[1.5823e-06, 1.0636e-07, 9.7703e-08],\n",
      "        [6.3094e-09, 1.2970e-09, 5.4540e-09],\n",
      "        [1.2834e-08, 2.4736e-09, 9.0174e-09],\n",
      "        ...,\n",
      "        [1.4750e-08, 2.9660e-09, 1.5718e-08],\n",
      "        [1.6008e-08, 2.4136e-09, 1.5076e-08],\n",
      "        [8.5085e-07, 3.9566e-07, 1.0089e-06]], device='cuda:0',\n",
      "       grad_fn=<_SigmoidFocalLossBackward>)\n",
      "well heres the loss ay:  tensor(1.6564, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "NUM POS 12\n",
      "TARGETS:  [BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=2, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)]\n",
      "feats:  5\n",
      "feats:  5\n",
      "feats:  256\n",
      "SHAPE THING torch.Size([5, 6, 76, 76])\n",
      "SHAPE THING torch.Size([5, 6, 38, 38])\n",
      "SHAPE THING torch.Size([5, 6, 19, 19])\n",
      "CUDAA\n",
      "summm:  tensor([[2.4432e-09],\n",
      "        [1.1811e-12],\n",
      "        [2.0843e-15],\n",
      "        ...,\n",
      "        [1.8860e-12],\n",
      "        [4.0219e-10],\n",
      "        [4.2962e-07]], device='cuda:0', grad_fn=<_SigmoidFocalLossBackward>)\n",
      "props grcnn\n",
      "[BoxList(num_boxes=100, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=100, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=100, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=100, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=49, image_width=600, image_height=600, mode=xyxy)]\n",
      "0time BoxList(num_boxes=100, image_width=600, image_height=600, mode=xyxy)\n",
      "loss is not heatmap\n",
      "loss_dict {'loss_cls': tensor(1.0096, device='cuda:0', grad_fn=<DivBackward0>), 'loss_centerness': tensor(0.7511, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_kps': tensor(5.5082, device='cuda:0', grad_fn=<DivBackward0>), 'heatmaps_loss': tensor(6.6258, device='cuda:0', grad_fn=<MulBackward0>)}\n",
      "losses  tensor(13.8948, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss_dict_reduced  {'loss_cls': tensor(1.0096, device='cuda:0', grad_fn=<DivBackward0>), 'loss_centerness': tensor(0.7511, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_kps': tensor(5.5082, device='cuda:0', grad_fn=<DivBackward0>), 'heatmaps_loss': tensor(6.6258, device='cuda:0', grad_fn=<MulBackward0>)}\n",
      "losses_reduced  tensor(13.8948, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ITER 2004\n",
      "BATCH:  [792, 179, 503, 494, 370]\n",
      "BATCH:  [134]\n",
      "bm size:  torch.Size([6, 3, 608, 608])\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "CUDAA\n",
      "summm:  tensor([[1.0945e-06, 1.6501e-07, 1.5958e-07],\n",
      "        [3.3483e-09, 1.8156e-09, 8.2240e-09],\n",
      "        [4.5451e-09, 2.5274e-09, 1.0257e-08],\n",
      "        ...,\n",
      "        [1.4196e-08, 6.5512e-09, 3.3240e-08],\n",
      "        [1.3802e-08, 5.3254e-09, 3.1263e-08],\n",
      "        [6.6428e-07, 7.2489e-07, 1.7767e-06]], device='cuda:0',\n",
      "       grad_fn=<_SigmoidFocalLossBackward>)\n",
      "well heres the loss ay:  tensor(0.6789, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "NUM POS 10\n",
      "TARGETS:  [BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)]\n",
      "feats:  5\n",
      "feats:  5\n",
      "feats:  256\n",
      "SHAPE THING torch.Size([5, 6, 76, 76])\n",
      "SHAPE THING torch.Size([5, 6, 38, 38])\n",
      "SHAPE THING torch.Size([5, 6, 19, 19])\n",
      "CUDAA\n",
      "summm:  tensor([[1.5167e-08],\n",
      "        [1.8353e-11],\n",
      "        [3.3602e-14],\n",
      "        ...,\n",
      "        [3.5160e-12],\n",
      "        [1.2100e-09],\n",
      "        [1.0925e-06]], device='cuda:0', grad_fn=<_SigmoidFocalLossBackward>)\n",
      "props grcnn\n",
      "[BoxList(num_boxes=100, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=100, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=100, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=100, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=100, image_width=600, image_height=600, mode=xyxy)]\n",
      "0time BoxList(num_boxes=100, image_width=600, image_height=600, mode=xyxy)\n",
      "loss is not heatmap\n",
      "loss_dict {'loss_cls': tensor(0.3529, device='cuda:0', grad_fn=<DivBackward0>), 'loss_centerness': tensor(0.7050, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_kps': tensor(2.0553, device='cuda:0', grad_fn=<DivBackward0>), 'heatmaps_loss': tensor(2.7158, device='cuda:0', grad_fn=<MulBackward0>)}\n",
      "losses  tensor(5.8289, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss_dict_reduced  {'loss_cls': tensor(0.3529, device='cuda:0', grad_fn=<DivBackward0>), 'loss_centerness': tensor(0.7050, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_kps': tensor(2.0553, device='cuda:0', grad_fn=<DivBackward0>), 'heatmaps_loss': tensor(2.7158, device='cuda:0', grad_fn=<MulBackward0>)}\n",
      "losses_reduced  tensor(5.8289, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ITER 2005\n",
      "BATCH:  [710, 612, 254, 740, 789]\n",
      "BATCH:  [625]\n",
      "bm size:  torch.Size([6, 3, 608, 608])\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=3, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "CUDAA\n",
      "summm:  tensor([[5.5003e-07, 2.3801e-07, 2.0565e-07],\n",
      "        [1.5296e-09, 3.6041e-09, 1.2167e-08],\n",
      "        [2.7252e-09, 6.5399e-09, 1.8283e-08],\n",
      "        ...,\n",
      "        [5.6929e-09, 8.6479e-09, 5.6592e-08],\n",
      "        [6.1943e-09, 7.2546e-09, 5.4580e-08],\n",
      "        [3.6504e-07, 9.5508e-07, 3.3199e-06]], device='cuda:0',\n",
      "       grad_fn=<_SigmoidFocalLossBackward>)\n",
      "well heres the loss ay:  tensor(0.6095, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "NUM POS 13\n",
      "TARGETS:  [BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=3, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)]\n",
      "feats:  5\n",
      "feats:  5\n",
      "feats:  256\n",
      "SHAPE THING torch.Size([5, 6, 76, 76])\n",
      "SHAPE THING torch.Size([5, 6, 38, 38])\n",
      "SHAPE THING torch.Size([5, 6, 19, 19])\n",
      "CUDAA\n",
      "summm:  tensor([[5.5361e-06],\n",
      "        [5.1301e-07],\n",
      "        [4.1552e-08],\n",
      "        ...,\n",
      "        [8.2945e-12],\n",
      "        [5.0409e-09],\n",
      "        [3.7143e-06]], device='cuda:0', grad_fn=<_SigmoidFocalLossBackward>)\n",
      "props grcnn\n",
      "[BoxList(num_boxes=100, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=100, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=100, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=100, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=100, image_width=600, image_height=600, mode=xyxy)]\n",
      "0time BoxList(num_boxes=100, image_width=600, image_height=600, mode=xyxy)\n",
      "loss is not heatmap\n",
      "loss_dict {'loss_cls': tensor(0.3573, device='cuda:0', grad_fn=<DivBackward0>), 'loss_centerness': tensor(0.7154, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_kps': tensor(1.4851, device='cuda:0', grad_fn=<DivBackward0>), 'heatmaps_loss': tensor(2.4379, device='cuda:0', grad_fn=<MulBackward0>)}\n",
      "losses  tensor(4.9956, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss_dict_reduced  {'loss_cls': tensor(0.3573, device='cuda:0', grad_fn=<DivBackward0>), 'loss_centerness': tensor(0.7154, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_kps': tensor(1.4851, device='cuda:0', grad_fn=<DivBackward0>), 'heatmaps_loss': tensor(2.4379, device='cuda:0', grad_fn=<MulBackward0>)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses_reduced  tensor(4.9956, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ITER 2006\n",
      "BATCH:  [408, 86, 287, 312, 778]\n",
      "BATCH:  [447]\n",
      "bm size:  torch.Size([6, 3, 608, 608])\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=2, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "CUDAA\n",
      "summm:  tensor([[2.9382e-07, 2.8288e-07, 5.0240e-07],\n",
      "        [9.6332e-10, 5.6243e-09, 4.2007e-08],\n",
      "        [3.5627e-09, 2.0103e-08, 1.0983e-07],\n",
      "        ...,\n",
      "        [4.0542e-09, 1.5013e-08, 1.3553e-07],\n",
      "        [4.4375e-09, 1.2555e-08, 1.3100e-07],\n",
      "        [2.8374e-07, 1.3619e-06, 7.2187e-06]], device='cuda:0',\n",
      "       grad_fn=<_SigmoidFocalLossBackward>)\n",
      "well heres the loss ay:  tensor(0.5623, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "NUM POS 14\n",
      "TARGETS:  [BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=2, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)]\n",
      "feats:  5\n",
      "feats:  5\n",
      "feats:  256\n",
      "SHAPE THING torch.Size([5, 6, 76, 76])\n",
      "SHAPE THING torch.Size([5, 6, 38, 38])\n",
      "SHAPE THING torch.Size([5, 6, 19, 19])\n",
      "CUDAA\n",
      "summm:  tensor([[3.1133e-06],\n",
      "        [2.3855e-07],\n",
      "        [1.6075e-08],\n",
      "        ...,\n",
      "        [1.4358e-10],\n",
      "        [2.7149e-08],\n",
      "        [7.7017e-06]], device='cuda:0', grad_fn=<_SigmoidFocalLossBackward>)\n",
      "props grcnn\n",
      "[BoxList(num_boxes=100, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=100, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=100, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=100, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=100, image_width=600, image_height=600, mode=xyxy)]\n",
      "0time BoxList(num_boxes=100, image_width=600, image_height=600, mode=xyxy)\n",
      "loss is not heatmap\n",
      "loss_dict {'loss_cls': tensor(0.3416, device='cuda:0', grad_fn=<DivBackward0>), 'loss_centerness': tensor(0.7033, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_kps': tensor(2.2553, device='cuda:0', grad_fn=<DivBackward0>), 'heatmaps_loss': tensor(2.2493, device='cuda:0', grad_fn=<MulBackward0>)}\n",
      "losses  tensor(5.5494, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss_dict_reduced  {'loss_cls': tensor(0.3416, device='cuda:0', grad_fn=<DivBackward0>), 'loss_centerness': tensor(0.7033, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_kps': tensor(2.2553, device='cuda:0', grad_fn=<DivBackward0>), 'heatmaps_loss': tensor(2.2493, device='cuda:0', grad_fn=<MulBackward0>)}\n",
      "losses_reduced  tensor(5.5494, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ITER 2007\n",
      "BATCH:  [129, 42, 540, 53, 341]\n",
      "BATCH:  [644]\n",
      "bm size:  torch.Size([6, 3, 608, 608])\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "CUDAA\n",
      "summm:  tensor([[7.4878e-07, 1.5302e-06, 2.8376e-06],\n",
      "        [3.9282e-09, 4.6089e-08, 3.3881e-07],\n",
      "        [7.8298e-09, 7.4459e-08, 4.6877e-07],\n",
      "        ...,\n",
      "        [4.6307e-10, 5.3459e-09, 1.1246e-07],\n",
      "        [4.9854e-10, 4.5879e-09, 1.1015e-07],\n",
      "        [5.5205e-08, 8.7764e-07, 8.2085e-06]], device='cuda:0',\n",
      "       grad_fn=<_SigmoidFocalLossBackward>)\n",
      "well heres the loss ay:  tensor(0.8924, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "NUM POS 12\n",
      "TARGETS:  [BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)]\n",
      "feats:  5\n",
      "feats:  5\n",
      "feats:  256\n",
      "SHAPE THING torch.Size([5, 6, 76, 76])\n",
      "SHAPE THING torch.Size([5, 6, 38, 38])\n",
      "SHAPE THING torch.Size([5, 6, 19, 19])\n",
      "CUDAA\n",
      "summm:  tensor([[2.0154e-04],\n",
      "        [4.6516e-05],\n",
      "        [5.8884e-06],\n",
      "        ...,\n",
      "        [4.3308e-10],\n",
      "        [1.8776e-07],\n",
      "        [3.5857e-05]], device='cuda:0', grad_fn=<_SigmoidFocalLossBackward>)\n",
      "props grcnn\n",
      "[BoxList(num_boxes=100, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=100, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=100, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=111, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=113, image_width=600, image_height=600, mode=xyxy)]\n",
      "0time BoxList(num_boxes=100, image_width=600, image_height=600, mode=xyxy)\n",
      "loss is not heatmap\n",
      "loss_dict {'loss_cls': tensor(0.5761, device='cuda:0', grad_fn=<DivBackward0>), 'loss_centerness': tensor(0.7250, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_kps': tensor(2.5855, device='cuda:0', grad_fn=<DivBackward0>), 'heatmaps_loss': tensor(3.5696, device='cuda:0', grad_fn=<MulBackward0>)}\n",
      "losses  tensor(7.4563, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss_dict_reduced  {'loss_cls': tensor(0.5761, device='cuda:0', grad_fn=<DivBackward0>), 'loss_centerness': tensor(0.7250, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_kps': tensor(2.5855, device='cuda:0', grad_fn=<DivBackward0>), 'heatmaps_loss': tensor(3.5696, device='cuda:0', grad_fn=<MulBackward0>)}\n",
      "losses_reduced  tensor(7.4563, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ITER 2008\n",
      "BATCH:  [123, 582, 603, 245, 67]\n",
      "BATCH:  [256]\n",
      "bm size:  torch.Size([6, 3, 608, 608])\n",
      "tim  BoxList(num_boxes=2, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "CUDAA\n",
      "summm:  tensor([[2.1463e-06, 7.9263e-06, 1.7229e-05],\n",
      "        [3.1213e-08, 6.1342e-07, 4.3266e-06],\n",
      "        [9.7718e-08, 1.4653e-06, 7.7788e-06],\n",
      "        ...,\n",
      "        [5.5306e-09, 5.8022e-08, 9.0615e-07],\n",
      "        [3.8122e-09, 3.2475e-08, 6.6609e-07],\n",
      "        [2.0673e-07, 2.3022e-06, 2.9110e-05]], device='cuda:0',\n",
      "       grad_fn=<_SigmoidFocalLossBackward>)\n",
      "well heres the loss ay:  tensor(0.6977, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "NUM POS 11\n",
      "TARGETS:  [BoxList(num_boxes=2, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)]\n",
      "feats:  5\n",
      "feats:  5\n",
      "feats:  256\n",
      "SHAPE THING torch.Size([5, 6, 76, 76])\n",
      "SHAPE THING torch.Size([5, 6, 38, 38])\n",
      "SHAPE THING torch.Size([5, 6, 19, 19])\n",
      "CUDAA\n",
      "summm:  tensor([[9.6963e-04],\n",
      "        [6.6073e-04],\n",
      "        [2.8237e-04],\n",
      "        ...,\n",
      "        [9.0024e-11],\n",
      "        [1.4309e-07],\n",
      "        [4.4301e-05]], device='cuda:0', grad_fn=<_SigmoidFocalLossBackward>)\n",
      "props grcnn\n",
      "[BoxList(num_boxes=100, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=100, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=121, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=178, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=100, image_width=600, image_height=600, mode=xyxy)]\n",
      "0time BoxList(num_boxes=100, image_width=600, image_height=600, mode=xyxy)\n",
      "loss is not heatmap\n",
      "loss_dict {'loss_cls': tensor(0.5352, device='cuda:0', grad_fn=<DivBackward0>), 'loss_centerness': tensor(0.6948, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_kps': tensor(1.5912, device='cuda:0', grad_fn=<DivBackward0>), 'heatmaps_loss': tensor(2.7909, device='cuda:0', grad_fn=<MulBackward0>)}\n",
      "losses  tensor(5.6121, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_dict_reduced  {'loss_cls': tensor(0.5352, device='cuda:0', grad_fn=<DivBackward0>), 'loss_centerness': tensor(0.6948, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_kps': tensor(1.5912, device='cuda:0', grad_fn=<DivBackward0>), 'heatmaps_loss': tensor(2.7909, device='cuda:0', grad_fn=<MulBackward0>)}\n",
      "losses_reduced  tensor(5.6121, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ITER 2009\n",
      "BATCH:  [744, 690, 269, 418, 9]\n",
      "BATCH:  [54]\n",
      "bm size:  torch.Size([6, 3, 608, 608])\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=2, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "CUDAA\n",
      "summm:  tensor([[2.8221e-06, 1.0818e-05, 2.4969e-05],\n",
      "        [4.8653e-08, 1.0940e-06, 8.9247e-06],\n",
      "        [1.7197e-07, 2.3725e-06, 1.5353e-05],\n",
      "        ...,\n",
      "        [8.5427e-10, 1.0879e-08, 3.5609e-07],\n",
      "        [6.5907e-10, 8.3167e-09, 3.1064e-07],\n",
      "        [7.8524e-08, 1.0550e-06, 2.3969e-05]], device='cuda:0',\n",
      "       grad_fn=<_SigmoidFocalLossBackward>)\n",
      "well heres the loss ay:  tensor(0.7274, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "NUM POS 15\n",
      "TARGETS:  [BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=2, image_width=599, image_height=599, mode=xyxy)]\n",
      "feats:  5\n",
      "feats:  5\n",
      "feats:  256\n",
      "SHAPE THING torch.Size([5, 6, 76, 76])\n",
      "SHAPE THING torch.Size([5, 6, 38, 38])\n",
      "SHAPE THING torch.Size([5, 6, 19, 19])\n",
      "CUDAA\n",
      "summm:  tensor([[6.7518e-04],\n",
      "        [3.2601e-04],\n",
      "        [8.5816e-05],\n",
      "        ...,\n",
      "        [3.1654e-14],\n",
      "        [1.6791e-10],\n",
      "        [9.2325e-07]], device='cuda:0', grad_fn=<_SigmoidFocalLossBackward>)\n",
      "props grcnn\n",
      "[BoxList(num_boxes=116, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=100, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=100, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=154, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=175, image_width=600, image_height=600, mode=xyxy)]\n",
      "0time BoxList(num_boxes=116, image_width=600, image_height=600, mode=xyxy)\n",
      "loss is not heatmap\n",
      "loss_dict {'loss_cls': tensor(0.3795, device='cuda:0', grad_fn=<DivBackward0>), 'loss_centerness': tensor(0.6912, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_kps': tensor(2.5586, device='cuda:0', grad_fn=<DivBackward0>), 'heatmaps_loss': tensor(2.9095, device='cuda:0', grad_fn=<MulBackward0>)}\n",
      "losses  tensor(6.5389, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss_dict_reduced  {'loss_cls': tensor(0.3795, device='cuda:0', grad_fn=<DivBackward0>), 'loss_centerness': tensor(0.6912, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_kps': tensor(2.5586, device='cuda:0', grad_fn=<DivBackward0>), 'heatmaps_loss': tensor(2.9095, device='cuda:0', grad_fn=<MulBackward0>)}\n",
      "losses_reduced  tensor(6.5389, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ITER 2010\n",
      "BATCH:  [102, 290, 40, 481, 332]\n",
      "BATCH:  [114]\n",
      "bm size:  torch.Size([6, 3, 608, 608])\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "CUDAA\n",
      "summm:  tensor([[4.3771e-07, 1.1957e-06, 5.5559e-06],\n",
      "        [1.9327e-09, 4.1545e-08, 8.2818e-07],\n",
      "        [6.9337e-09, 1.2709e-07, 1.7968e-06],\n",
      "        ...,\n",
      "        [1.1599e-12, 8.1638e-11, 2.8064e-08],\n",
      "        [9.6365e-11, 5.5755e-10, 1.6272e-07],\n",
      "        [6.9095e-08, 3.6417e-07, 2.0370e-05]], device='cuda:0',\n",
      "       grad_fn=<_SigmoidFocalLossBackward>)\n",
      "well heres the loss ay:  tensor(0.7704, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "NUM POS 14\n",
      "TARGETS:  [BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)]\n",
      "feats:  5\n",
      "feats:  5\n",
      "feats:  256\n",
      "SHAPE THING torch.Size([5, 6, 76, 76])\n",
      "SHAPE THING torch.Size([5, 6, 38, 38])\n",
      "SHAPE THING torch.Size([5, 6, 19, 19])\n",
      "CUDAA\n",
      "summm:  tensor([[3.7677e-05],\n",
      "        [1.4111e-05],\n",
      "        [3.5420e-06],\n",
      "        ...,\n",
      "        [5.2954e-11],\n",
      "        [1.7075e-08],\n",
      "        [4.9002e-06]], device='cuda:0', grad_fn=<_SigmoidFocalLossBackward>)\n",
      "props grcnn\n",
      "[BoxList(num_boxes=140, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=132, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=184, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=194, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=100, image_width=600, image_height=600, mode=xyxy)]\n",
      "0time BoxList(num_boxes=140, image_width=600, image_height=600, mode=xyxy)\n",
      "loss is not heatmap\n",
      "loss_dict {'loss_cls': tensor(0.4781, device='cuda:0', grad_fn=<DivBackward0>), 'loss_centerness': tensor(0.7356, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_kps': tensor(2.7842, device='cuda:0', grad_fn=<DivBackward0>), 'heatmaps_loss': tensor(3.0814, device='cuda:0', grad_fn=<MulBackward0>)}\n",
      "losses  tensor(7.0794, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss_dict_reduced  {'loss_cls': tensor(0.4781, device='cuda:0', grad_fn=<DivBackward0>), 'loss_centerness': tensor(0.7356, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_kps': tensor(2.7842, device='cuda:0', grad_fn=<DivBackward0>), 'heatmaps_loss': tensor(3.0814, device='cuda:0', grad_fn=<MulBackward0>)}\n",
      "losses_reduced  tensor(7.0794, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ITER 2011\n",
      "BATCH:  [275, 113, 641, 600, 122]\n",
      "BATCH:  [356]\n",
      "bm size:  torch.Size([6, 3, 608, 608])\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=2, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=3, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=4, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "CUDAA\n",
      "summm:  tensor([[4.1192e-07, 4.8604e-07, 2.5061e-06],\n",
      "        [8.2728e-10, 4.9521e-09, 2.0357e-07],\n",
      "        [3.1591e-10, 1.6537e-09, 1.1250e-07],\n",
      "        ...,\n",
      "        [3.5525e-09, 2.1834e-08, 3.2451e-07],\n",
      "        [3.7927e-09, 2.2303e-08, 3.3460e-07],\n",
      "        [3.2788e-07, 1.7488e-06, 2.1600e-05]], device='cuda:0',\n",
      "       grad_fn=<_SigmoidFocalLossBackward>)\n",
      "well heres the loss ay:  tensor(0.5861, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "NUM POS 16\n",
      "TARGETS:  [BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=2, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=3, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=4, image_width=599, image_height=599, mode=xyxy)]\n",
      "feats:  5\n",
      "feats:  5\n",
      "feats:  256\n",
      "SHAPE THING torch.Size([5, 6, 76, 76])\n",
      "SHAPE THING torch.Size([5, 6, 38, 38])\n",
      "SHAPE THING torch.Size([5, 6, 19, 19])\n",
      "CUDAA\n",
      "summm:  tensor([[6.0015e-07],\n",
      "        [2.3050e-08],\n",
      "        [9.0965e-10],\n",
      "        ...,\n",
      "        [9.7616e-14],\n",
      "        [3.2563e-11],\n",
      "        [8.0401e-08]], device='cuda:0', grad_fn=<_SigmoidFocalLossBackward>)\n",
      "props grcnn\n",
      "[BoxList(num_boxes=205, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=100, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=186, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=100, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=256, image_width=600, image_height=600, mode=xyxy)]\n",
      "0time BoxList(num_boxes=205, image_width=600, image_height=600, mode=xyxy)\n",
      "loss is not heatmap\n",
      "loss_dict {'loss_cls': tensor(0.4378, device='cuda:0', grad_fn=<DivBackward0>), 'loss_centerness': tensor(0.6932, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_kps': tensor(3.3428, device='cuda:0', grad_fn=<DivBackward0>), 'heatmaps_loss': tensor(2.3444, device='cuda:0', grad_fn=<MulBackward0>)}\n",
      "losses  tensor(6.8182, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss_dict_reduced  {'loss_cls': tensor(0.4378, device='cuda:0', grad_fn=<DivBackward0>), 'loss_centerness': tensor(0.6932, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_kps': tensor(3.3428, device='cuda:0', grad_fn=<DivBackward0>), 'heatmaps_loss': tensor(2.3444, device='cuda:0', grad_fn=<MulBackward0>)}\n",
      "losses_reduced  tensor(6.8182, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ITER 2012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH:  [631, 499, 1, 324, 12]\n",
      "BATCH:  [283]\n",
      "bm size:  torch.Size([6, 3, 608, 608])\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "CUDAA\n",
      "summm:  tensor([[5.9451e-07, 6.0856e-07, 1.4835e-06],\n",
      "        [2.0241e-09, 1.4659e-08, 1.6325e-07],\n",
      "        [6.2171e-09, 3.9544e-08, 3.3651e-07],\n",
      "        ...,\n",
      "        [5.5667e-09, 3.1822e-08, 2.2836e-07],\n",
      "        [6.0127e-09, 3.3179e-08, 2.4410e-07],\n",
      "        [4.6920e-07, 2.2870e-06, 1.1363e-05]], device='cuda:0',\n",
      "       grad_fn=<_SigmoidFocalLossBackward>)\n",
      "well heres the loss ay:  tensor(0.6744, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "NUM POS 10\n",
      "TARGETS:  [BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)]\n",
      "feats:  5\n",
      "feats:  5\n",
      "feats:  256\n",
      "SHAPE THING torch.Size([5, 6, 76, 76])\n",
      "SHAPE THING torch.Size([5, 6, 38, 38])\n",
      "SHAPE THING torch.Size([5, 6, 19, 19])\n",
      "CUDAA\n",
      "summm:  tensor([[2.7613e-09],\n",
      "        [8.0424e-12],\n",
      "        [1.5486e-13],\n",
      "        ...,\n",
      "        [2.2153e-12],\n",
      "        [8.1540e-10],\n",
      "        [6.4921e-07]], device='cuda:0', grad_fn=<_SigmoidFocalLossBackward>)\n",
      "props grcnn\n",
      "[BoxList(num_boxes=204, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=100, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=173, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=100, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=173, image_width=600, image_height=600, mode=xyxy)]\n",
      "0time BoxList(num_boxes=204, image_width=600, image_height=600, mode=xyxy)\n",
      "loss is not heatmap\n",
      "loss_dict {'loss_cls': tensor(0.4054, device='cuda:0', grad_fn=<DivBackward0>), 'loss_centerness': tensor(0.6902, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_kps': tensor(2.4116, device='cuda:0', grad_fn=<DivBackward0>), 'heatmaps_loss': tensor(2.6976, device='cuda:0', grad_fn=<MulBackward0>)}\n",
      "losses  tensor(6.2048, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss_dict_reduced  {'loss_cls': tensor(0.4054, device='cuda:0', grad_fn=<DivBackward0>), 'loss_centerness': tensor(0.6902, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_kps': tensor(2.4116, device='cuda:0', grad_fn=<DivBackward0>), 'heatmaps_loss': tensor(2.6976, device='cuda:0', grad_fn=<MulBackward0>)}\n",
      "losses_reduced  tensor(6.2048, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ITER 2013\n",
      "BATCH:  [663, 380, 180, 650, 294]\n",
      "BATCH:  [111]\n",
      "bm size:  torch.Size([6, 3, 608, 608])\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "CUDAA\n",
      "summm:  tensor([[2.1029e-06, 1.3458e-06, 1.3089e-06],\n",
      "        [1.1714e-08, 4.5925e-08, 1.7382e-07],\n",
      "        [1.7256e-08, 6.2396e-08, 2.1210e-07],\n",
      "        ...,\n",
      "        [6.6494e-09, 2.4367e-08, 6.2263e-08],\n",
      "        [7.4886e-09, 2.6186e-08, 6.8134e-08],\n",
      "        [6.9993e-07, 1.6275e-06, 4.8394e-06]], device='cuda:0',\n",
      "       grad_fn=<_SigmoidFocalLossBackward>)\n",
      "well heres the loss ay:  tensor(0.6813, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "NUM POS 11\n",
      "TARGETS:  [BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)]\n",
      "feats:  5\n",
      "feats:  5\n",
      "feats:  256\n",
      "SHAPE THING torch.Size([5, 6, 76, 76])\n",
      "SHAPE THING torch.Size([5, 6, 38, 38])\n",
      "SHAPE THING torch.Size([5, 6, 19, 19])\n",
      "CUDAA\n",
      "summm:  tensor([[6.9866e-08],\n",
      "        [2.3181e-10],\n",
      "        [4.0173e-13],\n",
      "        ...,\n",
      "        [3.0887e-11],\n",
      "        [1.9680e-09],\n",
      "        [4.6580e-07]], device='cuda:0', grad_fn=<_SigmoidFocalLossBackward>)\n",
      "props grcnn\n",
      "[BoxList(num_boxes=179, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=249, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=151, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=101, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=116, image_width=600, image_height=600, mode=xyxy)]\n",
      "0time BoxList(num_boxes=179, image_width=600, image_height=600, mode=xyxy)\n",
      "loss is not heatmap\n",
      "loss_dict {'loss_cls': tensor(0.3841, device='cuda:0', grad_fn=<DivBackward0>), 'loss_centerness': tensor(0.6913, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_kps': tensor(2.8576, device='cuda:0', grad_fn=<DivBackward0>), 'heatmaps_loss': tensor(2.7250, device='cuda:0', grad_fn=<MulBackward0>)}\n",
      "losses  tensor(6.6579, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "loss_dict_reduced  {'loss_cls': tensor(0.3841, device='cuda:0', grad_fn=<DivBackward0>), 'loss_centerness': tensor(0.6913, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_kps': tensor(2.8576, device='cuda:0', grad_fn=<DivBackward0>), 'heatmaps_loss': tensor(2.7250, device='cuda:0', grad_fn=<MulBackward0>)}\n",
      "losses_reduced  tensor(6.6579, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ITER 2014\n",
      "BATCH:  [544, 47, 421, 776, 461]\n",
      "BATCH:  [177]\n",
      "bm size:  torch.Size([6, 3, 608, 608])\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=4, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=3, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "CUDAA\n",
      "summm:  tensor([[9.5307e-07, 6.5369e-07, 3.6352e-07],\n",
      "        [4.0095e-09, 1.9519e-08, 3.7311e-08],\n",
      "        [7.6005e-09, 3.2984e-08, 5.5151e-08],\n",
      "        ...,\n",
      "        [7.4149e-09, 2.2190e-08, 2.7345e-08],\n",
      "        [7.9632e-09, 2.4213e-08, 2.9256e-08],\n",
      "        [8.8976e-07, 1.3201e-06, 2.5282e-06]], device='cuda:0',\n",
      "       grad_fn=<_SigmoidFocalLossBackward>)\n",
      "well heres the loss ay:  tensor(0.6026, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "NUM POS 17\n",
      "TARGETS:  [BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=4, image_width=599, image_height=599, mode=xyxy), BoxList(num_boxes=3, image_width=599, image_height=599, mode=xyxy)]\n",
      "feats:  5\n",
      "feats:  5\n",
      "feats:  256\n",
      "SHAPE THING torch.Size([5, 6, 76, 76])\n",
      "SHAPE THING torch.Size([5, 6, 38, 38])\n",
      "SHAPE THING torch.Size([5, 6, 19, 19])\n",
      "CUDAA\n",
      "summm:  tensor([[9.5974e-07],\n",
      "        [4.9793e-08],\n",
      "        [1.8624e-09],\n",
      "        ...,\n",
      "        [2.9912e-15],\n",
      "        [2.1365e-13],\n",
      "        [8.6208e-10]], device='cuda:0', grad_fn=<_SigmoidFocalLossBackward>)\n",
      "props grcnn\n",
      "[BoxList(num_boxes=116, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=156, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=126, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=176, image_width=600, image_height=600, mode=xyxy), BoxList(num_boxes=296, image_width=600, image_height=600, mode=xyxy)]\n",
      "0time BoxList(num_boxes=116, image_width=600, image_height=600, mode=xyxy)\n",
      "loss is not heatmap\n",
      "loss_dict {'loss_cls': tensor(0.3989, device='cuda:0', grad_fn=<DivBackward0>), 'loss_centerness': tensor(0.6927, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_kps': tensor(2.0960, device='cuda:0', grad_fn=<DivBackward0>), 'heatmaps_loss': tensor(2.4102, device='cuda:0', grad_fn=<MulBackward0>)}\n",
      "losses  tensor(5.5979, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_dict_reduced  {'loss_cls': tensor(0.3989, device='cuda:0', grad_fn=<DivBackward0>), 'loss_centerness': tensor(0.6927, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_kps': tensor(2.0960, device='cuda:0', grad_fn=<DivBackward0>), 'heatmaps_loss': tensor(2.4102, device='cuda:0', grad_fn=<MulBackward0>)}\n",
      "losses_reduced  tensor(5.5979, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ITER 2015\n",
      "BATCH:  [733, 226, 346, 162, 501]\n",
      "BATCH:  [235]\n",
      "bm size:  torch.Size([6, 3, 608, 608])\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "tim  BoxList(num_boxes=1, image_width=599, image_height=599, mode=xyxy)\n",
      "In hm proc target\n",
      "CUDAA\n",
      "summm:  tensor([[8.2021e-07, 5.3374e-07, 2.3130e-07],\n",
      "        [2.6970e-09, 1.1314e-08, 1.6711e-08],\n",
      "        [3.1174e-09, 1.3038e-08, 1.8252e-08],\n",
      "        ...,\n",
      "        [4.1844e-08, 9.2193e-08, 3.4155e-08],\n",
      "        [3.1741e-08, 6.2354e-08, 2.7479e-08],\n",
      "        [2.1327e-06, 1.7878e-06, 1.9042e-06]], device='cuda:0',\n",
      "       grad_fn=<_SigmoidFocalLossBackward>)\n",
      "well heres the loss ay:  tensor(0.5755, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "NUM POS 12\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bsb2144/miniconda3/envs/directpose/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/bsb2144/miniconda3/envs/directpose/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/bsb2144/miniconda3/envs/directpose/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/bsb2144/miniconda3/envs/directpose/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"train_bee_250.py\", line 182, in <module>\n",
      "    model = train(cfg, local_rank, distributed, loss_type,version)\n",
      "  File \"train_bee_250.py\", line 86, in train\n",
      "    version=version\n",
      "  File \"/share/ctn/users/bsb2144/directpose/maskrcnn_benchmark/engine/trainer.py\", line 132, in do_train\n",
      "    loss_dict, train_mse = model(images, targets, images_la, targets_la,images_un,targets_un, loss_type, alpha, beta, has_unlabeled)\n",
      "  File \"/home/bsb2144/miniconda3/envs/directpose/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 491, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/share/ctn/users/bsb2144/directpose/maskrcnn_benchmark/modeling/detector/generalized_rcnn.py\", line 83, in forward\n",
      "    labeled = True)\n",
      "  File \"/home/bsb2144/miniconda3/envs/directpose/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 491, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/share/ctn/users/bsb2144/directpose/maskrcnn_benchmark/modeling/heatmaps/heatmaps.py\", line 68, in forward\n",
      "    coords_per_type = coords_per_im[coords_per_im[:, 1] == type_i][:, -2:].float() * stride\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python train_bee_250.py \"standard\" \"250\" \"1\" \"0\" \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['combined_loss', 'heatmaps_loss', 'loss_centerness', 'loss_cls', 'loss_kps']\n",
      "dset:  1.617404818534851\n",
      "0 : nan\n",
      "1 : nan\n",
      "2 : nan\n",
      "3 : nan\n",
      "4 : nan\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "log_error_rmse_file = \"error_file_combined_full_batch_t3_1.h5\"              \n",
    "with h5py.File(log_error_rmse_file, \"r\") as f:\n",
    "    ks = list(f.keys())\n",
    "    print(list(f.keys()))\n",
    "    #for i in range(0,len(ks)):\n",
    "    dset = f[ks[1]]\n",
    "    print(\"dset: \", dset[2503])\n",
    "    for idx, dd in enumerate(dset[0:5]):\n",
    "        print(idx,\":\", dd)\n",
    "    #else:\n",
    "     #   for idx, dd in enumerate(dset[0][0]):\n",
    "      #      print(idx,\":\", dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: <KeysViewHDF5 ['frames', 'instances', 'metadata', 'points', 'pred_points', 'suggestions_json', 'tracks_json', 'videos_json']>\n",
      "a_group_key: ['frames', 'instances', 'metadata', 'points', 'pred_points', 'suggestions_json', 'tracks_json', 'videos_json']\n",
      "frames\n",
      "(0, 0, 0, 0, 20)\n",
      "frames\n",
      "(1, 0, 1, 20, 34)\n",
      "frames\n",
      "(2, 0, 2, 34, 45)\n",
      "frames\n",
      "(3, 0, 3, 45, 57)\n",
      "frames\n",
      "(4, 0, 4, 57, 69)\n",
      "frames\n",
      "(5, 0, 5, 69, 80)\n",
      "frames\n",
      "(6, 0, 6, 80, 94)\n",
      "frames\n",
      "(7, 0, 7, 94, 110)\n",
      "frames\n",
      "(8, 0, 8, 110, 126)\n",
      "frames\n",
      "(9, 0, 9, 126, 146)\n",
      "frames\n",
      "(10, 0, 10, 146, 156)\n",
      "frames\n",
      "(11, 0, 11, 156, 168)\n",
      "frames\n",
      "(12, 0, 12, 168, 185)\n",
      "frames\n",
      "(13, 0, 13, 185, 200)\n",
      "frames\n",
      "(14, 0, 14, 200, 221)\n",
      "frames\n",
      "(15, 0, 15, 221, 240)\n",
      "frames\n",
      "(16, 0, 16, 240, 260)\n",
      "frames\n",
      "(17, 0, 17, 260, 277)\n",
      "frames\n",
      "(18, 0, 18, 277, 295)\n",
      "frames\n",
      "(19, 0, 19, 295, 310)\n",
      "frames\n",
      "(20, 0, 20, 310, 326)\n",
      "frames\n",
      "(21, 0, 21, 326, 343)\n",
      "frames\n",
      "(22, 0, 22, 343, 358)\n",
      "frames\n",
      "(23, 0, 23, 358, 373)\n",
      "frames\n",
      "(24, 0, 24, 373, 390)\n",
      "frames\n",
      "(25, 0, 25, 390, 410)\n",
      "frames\n",
      "(26, 0, 26, 410, 427)\n",
      "frames\n",
      "(27, 0, 27, 427, 443)\n",
      "frames\n",
      "(28, 0, 28, 443, 457)\n",
      "frames\n",
      "(29, 0, 29, 457, 467)\n",
      "ints\n",
      "(0, 1, 0, 0, -1, -1, 6.1212597, 0, 5)\n",
      "ints\n",
      "(1, 1, 0, 0, -1, -1, 1.6559818, 5, 10)\n",
      "ints\n",
      "(2, 1, 0, 0, -1, -1, 7.1491723, 10, 15)\n",
      "ints\n",
      "(3, 1, 0, 0, -1, -1, 1.9502741, 15, 20)\n",
      "ints\n",
      "(4, 1, 0, 0, -1, -1, 6.208929, 20, 25)\n",
      "ints\n",
      "(5, 1, 0, 0, -1, -1, 7.1131315, 25, 30)\n",
      "ints\n",
      "(6, 1, 0, 0, -1, -1, 5.893938, 30, 35)\n",
      "ints\n",
      "(7, 1, 0, 0, -1, -1, 7.2472816, 35, 40)\n",
      "ints\n",
      "(8, 1, 0, 0, -1, -1, 7.08401, 40, 45)\n",
      "ints\n",
      "(9, 1, 0, 0, -1, -1, 2.9198895, 45, 50)\n",
      "ints\n",
      "(10, 1, 0, 0, -1, -1, 8.470883, 50, 55)\n",
      "ints\n",
      "(11, 1, 0, 0, -1, -1, 7.69057, 55, 60)\n",
      "ints\n",
      "(12, 1, 0, 0, -1, -1, 6.7568345, 60, 65)\n",
      "ints\n",
      "(13, 1, 0, 0, -1, -1, 7.240335, 65, 70)\n",
      "ints\n",
      "(14, 1, 0, 0, -1, -1, 7.548971, 70, 75)\n",
      "ints\n",
      "(15, 1, 0, 0, -1, -1, 6.8147883, 75, 80)\n",
      "ints\n",
      "(16, 1, 0, 0, -1, -1, 0.69587964, 80, 85)\n",
      "ints\n",
      "(17, 1, 0, 0, -1, -1, 6.3895254, 85, 90)\n",
      "ints\n",
      "(18, 1, 0, 0, -1, -1, 7.259707, 90, 95)\n",
      "ints\n",
      "(19, 1, 0, 0, -1, -1, 1.7608553, 95, 100)\n",
      "ints\n",
      "(20, 1, 1, 0, -1, -1, 4.1390576, 100, 105)\n",
      "ints\n",
      "(21, 1, 1, 0, -1, -1, 7.119459, 105, 110)\n",
      "ints\n",
      "(22, 1, 1, 0, -1, -1, 6.8050003, 110, 115)\n",
      "ints\n",
      "(23, 1, 1, 0, -1, -1, 3.7289116, 115, 120)\n",
      "ints\n",
      "(24, 1, 1, 0, -1, -1, 7.6252937, 120, 125)\n",
      "ints\n",
      "(25, 1, 1, 0, -1, -1, 7.5167513, 125, 130)\n",
      "ints\n",
      "(26, 1, 1, 0, -1, -1, 5.664854, 130, 135)\n",
      "ints\n",
      "(27, 1, 1, 0, -1, -1, 7.476445, 135, 140)\n",
      "ints\n",
      "(28, 1, 1, 0, -1, -1, 7.1437516, 140, 145)\n",
      "ints\n",
      "(29, 1, 1, 0, -1, -1, 7.596635, 145, 150)\n",
      "ints\n",
      "(30, 1, 1, 0, -1, -1, 6.9153366, 150, 155)\n",
      "ints\n",
      "(31, 1, 1, 0, -1, -1, 7.091798, 155, 160)\n",
      "ints\n",
      "(32, 1, 1, 0, -1, -1, 5.783198, 160, 165)\n",
      "ints\n",
      "(33, 1, 1, 0, -1, -1, 4.7712727, 165, 170)\n",
      "ints\n",
      "(34, 1, 2, 0, -1, -1, 6.395546, 170, 175)\n",
      "ints\n",
      "(35, 1, 2, 0, -1, -1, 6.4999657, 175, 180)\n",
      "ints\n",
      "(36, 1, 2, 0, -1, -1, 7.887125, 180, 185)\n",
      "ints\n",
      "(37, 1, 2, 0, -1, -1, 7.882843, 185, 190)\n",
      "ints\n",
      "(38, 1, 2, 0, -1, -1, 4.8318357, 190, 195)\n",
      "ints\n",
      "(39, 1, 2, 0, -1, -1, 7.1409264, 195, 200)\n",
      "ints\n",
      "(40, 1, 2, 0, -1, -1, 7.2958717, 200, 205)\n",
      "ints\n",
      "(41, 1, 2, 0, -1, -1, 7.315293, 205, 210)\n",
      "ints\n",
      "(42, 1, 2, 0, -1, -1, 7.515813, 210, 215)\n",
      "ints\n",
      "(43, 1, 2, 0, -1, -1, 3.9947715, 215, 220)\n",
      "ints\n",
      "(44, 1, 2, 0, -1, -1, 2.1336412, 220, 225)\n",
      "ints\n",
      "(45, 1, 3, 0, -1, -1, 6.2520957, 225, 230)\n",
      "ints\n",
      "(46, 1, 3, 0, -1, -1, 6.8975363, 230, 235)\n",
      "ints\n",
      "(47, 1, 3, 0, -1, -1, 8.046067, 235, 240)\n",
      "ints\n",
      "(48, 1, 3, 0, -1, -1, 6.619405, 240, 245)\n",
      "ints\n",
      "(49, 1, 3, 0, -1, -1, 7.2370467, 245, 250)\n",
      "ints\n",
      "(50, 1, 3, 0, -1, -1, 6.9728494, 250, 255)\n",
      "ints\n",
      "(51, 1, 3, 0, -1, -1, 5.0414944, 255, 260)\n",
      "ints\n",
      "(52, 1, 3, 0, -1, -1, 6.8558574, 260, 265)\n",
      "ints\n",
      "(53, 1, 3, 0, -1, -1, 8.414558, 265, 270)\n",
      "ints\n",
      "(54, 1, 3, 0, -1, -1, 6.8134108, 270, 275)\n",
      "ints\n",
      "(55, 1, 3, 0, -1, -1, 2.7646828, 275, 280)\n",
      "ints\n",
      "(56, 1, 3, 0, -1, -1, 4.342734, 280, 285)\n",
      "ints\n",
      "(57, 1, 4, 0, -1, -1, 6.596565, 285, 290)\n",
      "ints\n",
      "(58, 1, 4, 0, -1, -1, 1.8988243, 290, 295)\n",
      "ints\n",
      "(59, 1, 4, 0, -1, -1, 2.6616173, 295, 300)\n",
      "ints\n",
      "(60, 1, 4, 0, -1, -1, 6.9365234, 300, 305)\n",
      "ints\n",
      "(61, 1, 4, 0, -1, -1, 7.2084627, 305, 310)\n",
      "ints\n",
      "(62, 1, 4, 0, -1, -1, 7.9212575, 310, 315)\n",
      "ints\n",
      "(63, 1, 4, 0, -1, -1, 8.2949915, 315, 320)\n",
      "ints\n",
      "(64, 1, 4, 0, -1, -1, 5.5103817, 320, 325)\n",
      "ints\n",
      "(65, 1, 4, 0, -1, -1, 7.237549, 325, 330)\n",
      "ints\n",
      "(66, 1, 4, 0, -1, -1, 6.9700136, 330, 335)\n",
      "ints\n",
      "(67, 1, 4, 0, -1, -1, 7.4435863, 335, 340)\n",
      "ints\n",
      "(68, 1, 4, 0, -1, -1, 6.702035, 340, 345)\n",
      "ints\n",
      "(69, 1, 5, 0, -1, -1, 1.8909826, 345, 350)\n",
      "ints\n",
      "(70, 1, 5, 0, -1, -1, 6.4468703, 350, 355)\n",
      "ints\n",
      "(71, 1, 5, 0, -1, -1, 7.47262, 355, 360)\n",
      "ints\n",
      "(72, 1, 5, 0, -1, -1, 7.0854406, 360, 365)\n",
      "ints\n",
      "(73, 1, 5, 0, -1, -1, 7.326823, 365, 370)\n",
      "ints\n",
      "(74, 1, 5, 0, -1, -1, 6.580517, 370, 375)\n",
      "ints\n",
      "(75, 1, 5, 0, -1, -1, 6.553419, 375, 380)\n",
      "ints\n",
      "(76, 1, 5, 0, -1, -1, 6.8690796, 380, 385)\n",
      "ints\n",
      "(77, 1, 5, 0, -1, -1, 4.8915424, 385, 390)\n",
      "ints\n",
      "(78, 1, 5, 0, -1, -1, 5.629669, 390, 395)\n",
      "ints\n",
      "(79, 1, 5, 0, -1, -1, 0.4420591, 395, 400)\n",
      "ints\n",
      "(80, 1, 6, 0, -1, -1, 1.8913639, 400, 405)\n",
      "ints\n",
      "(81, 1, 6, 0, -1, -1, 3.9094477, 405, 410)\n",
      "ints\n",
      "(82, 1, 6, 0, -1, -1, 6.915741, 410, 415)\n",
      "ints\n",
      "(83, 1, 6, 0, -1, -1, 7.3719, 415, 420)\n",
      "ints\n",
      "(84, 1, 6, 0, -1, -1, 7.7183957, 420, 425)\n",
      "ints\n",
      "(85, 1, 6, 0, -1, -1, 6.5704308, 425, 430)\n",
      "ints\n",
      "(86, 1, 6, 0, -1, -1, 6.928687, 430, 435)\n",
      "ints\n",
      "(87, 1, 6, 0, -1, -1, 6.9399214, 435, 440)\n",
      "ints\n",
      "(88, 1, 6, 0, -1, -1, 7.326506, 440, 445)\n",
      "ints\n",
      "(89, 1, 6, 0, -1, -1, 5.121156, 445, 450)\n",
      "ints\n",
      "(90, 1, 6, 0, -1, -1, 6.0605164, 450, 455)\n",
      "ints\n",
      "(91, 1, 6, 0, -1, -1, 4.2686663, 455, 460)\n",
      "ints\n",
      "(92, 1, 6, 0, -1, -1, 5.5495706, 460, 465)\n",
      "ints\n",
      "(93, 1, 6, 0, -1, -1, 2.9973326, 465, 470)\n",
      "ints\n",
      "(94, 1, 7, 0, -1, -1, 5.2970915, 470, 475)\n",
      "ints\n",
      "(95, 1, 7, 0, -1, -1, 7.579667, 475, 480)\n",
      "ints\n",
      "(96, 1, 7, 0, -1, -1, 3.2960172, 480, 485)\n",
      "ints\n",
      "(97, 1, 7, 0, -1, -1, 2.8668709, 485, 490)\n",
      "ints\n",
      "(98, 1, 7, 0, -1, -1, 7.0776834, 490, 495)\n",
      "ints\n",
      "(99, 1, 7, 0, -1, -1, 5.990305, 495, 500)\n",
      "ints\n",
      "(100, 1, 7, 0, -1, -1, 6.996189, 500, 505)\n",
      "ints\n",
      "(101, 1, 7, 0, -1, -1, 7.1222944, 505, 510)\n",
      "ints\n",
      "(102, 1, 7, 0, -1, -1, 7.1274815, 510, 515)\n",
      "ints\n",
      "(103, 1, 7, 0, -1, -1, 7.1456037, 515, 520)\n",
      "ints\n",
      "(104, 1, 7, 0, -1, -1, 7.2872524, 520, 525)\n",
      "ints\n",
      "(105, 1, 7, 0, -1, -1, 6.623963, 525, 530)\n",
      "ints\n",
      "(106, 1, 7, 0, -1, -1, 7.447625, 530, 535)\n",
      "ints\n",
      "(107, 1, 7, 0, -1, -1, 6.6395493, 535, 540)\n",
      "ints\n",
      "(108, 1, 7, 0, -1, -1, 5.818192, 540, 545)\n",
      "ints\n",
      "(109, 1, 7, 0, -1, -1, 4.3191257, 545, 550)\n",
      "ints\n",
      "(110, 1, 8, 0, -1, -1, 1.0072024, 550, 555)\n",
      "ints\n",
      "(111, 1, 8, 0, -1, -1, 1.954966, 555, 560)\n",
      "ints\n",
      "(112, 1, 8, 0, -1, -1, 6.855975, 560, 565)\n",
      "ints\n",
      "(113, 1, 8, 0, -1, -1, 6.1314535, 565, 570)\n",
      "ints\n",
      "(114, 1, 8, 0, -1, -1, 6.4156194, 570, 575)\n",
      "ints\n",
      "(115, 1, 8, 0, -1, -1, 5.152318, 575, 580)\n",
      "ints\n",
      "(116, 1, 8, 0, -1, -1, 6.905558, 580, 585)\n",
      "ints\n",
      "(117, 1, 8, 0, -1, -1, 7.858142, 585, 590)\n",
      "ints\n",
      "(118, 1, 8, 0, -1, -1, 6.450181, 590, 595)\n",
      "ints\n",
      "(119, 1, 8, 0, -1, -1, 6.882719, 595, 600)\n",
      "ints\n",
      "(120, 1, 8, 0, -1, -1, 5.029279, 600, 605)\n",
      "ints\n",
      "(121, 1, 8, 0, -1, -1, 1.1661652, 605, 610)\n",
      "ints\n",
      "(122, 1, 8, 0, -1, -1, 6.922064, 610, 615)\n",
      "ints\n",
      "(123, 1, 8, 0, -1, -1, 7.539955, 615, 620)\n",
      "ints\n",
      "(124, 1, 8, 0, -1, -1, 4.821031, 620, 625)\n",
      "ints\n",
      "(125, 1, 8, 0, -1, -1, 4.0598884, 625, 630)\n",
      "ints\n",
      "(126, 1, 9, 0, -1, -1, 6.564122, 630, 635)\n",
      "ints\n",
      "(127, 1, 9, 0, -1, -1, 1.5868365, 635, 640)\n",
      "ints\n",
      "(128, 1, 9, 0, -1, -1, 1.7447453, 640, 645)\n",
      "ints\n",
      "(129, 1, 9, 0, -1, -1, 1.9860868, 645, 650)\n",
      "ints\n",
      "(130, 1, 9, 0, -1, -1, 3.8217397, 650, 655)\n",
      "ints\n",
      "(131, 1, 9, 0, -1, -1, 7.2983365, 655, 660)\n",
      "ints\n",
      "(132, 1, 9, 0, -1, -1, 6.5679054, 660, 665)\n",
      "ints\n",
      "(133, 1, 9, 0, -1, -1, 6.8485556, 665, 670)\n",
      "ints\n",
      "(134, 1, 9, 0, -1, -1, 7.629025, 670, 675)\n",
      "ints\n",
      "(135, 1, 9, 0, -1, -1, 6.902729, 675, 680)\n",
      "ints\n",
      "(136, 1, 9, 0, -1, -1, 5.570175, 680, 685)\n",
      "ints\n",
      "(137, 1, 9, 0, -1, -1, 7.8191233, 685, 690)\n",
      "ints\n",
      "(138, 1, 9, 0, -1, -1, 7.9569807, 690, 695)\n",
      "ints\n",
      "(139, 1, 9, 0, -1, -1, 6.9023747, 695, 700)\n",
      "ints\n",
      "(140, 1, 9, 0, -1, -1, 7.263344, 700, 705)\n",
      "ints\n",
      "(141, 1, 9, 0, -1, -1, 0.95195043, 705, 710)\n",
      "ints\n",
      "(142, 1, 9, 0, -1, -1, 7.091872, 710, 715)\n",
      "ints\n",
      "(143, 1, 9, 0, -1, -1, 6.2108364, 715, 720)\n",
      "ints\n",
      "(144, 1, 9, 0, -1, -1, 4.0162425, 720, 725)\n",
      "ints\n",
      "(145, 1, 9, 0, -1, -1, 0.87667394, 725, 730)\n",
      "ints\n",
      "(146, 1, 10, 0, -1, -1, 6.9451857, 730, 735)\n",
      "ints\n",
      "(147, 1, 10, 0, -1, -1, 5.3277087, 735, 740)\n",
      "ints\n",
      "(148, 1, 10, 0, -1, -1, 7.215118, 740, 745)\n",
      "ints\n",
      "(149, 1, 10, 0, -1, -1, 8.057512, 745, 750)\n",
      "ints\n",
      "(150, 1, 10, 0, -1, -1, 7.4783697, 750, 755)\n",
      "ints\n",
      "(151, 1, 10, 0, -1, -1, 6.0321913, 755, 760)\n",
      "ints\n",
      "(152, 1, 10, 0, -1, -1, 7.1195006, 760, 765)\n",
      "ints\n",
      "(153, 1, 10, 0, -1, -1, 7.4575787, 765, 770)\n",
      "ints\n",
      "(154, 1, 10, 0, -1, -1, 5.3904862, 770, 775)\n",
      "ints\n",
      "(155, 1, 10, 0, -1, -1, 5.916995, 775, 780)\n",
      "ints\n",
      "(156, 1, 11, 0, -1, -1, 6.055945, 780, 785)\n",
      "ints\n",
      "(157, 1, 11, 0, -1, -1, 5.163176, 785, 790)\n",
      "ints\n",
      "(158, 1, 11, 0, -1, -1, 5.7133765, 790, 795)\n",
      "ints\n",
      "(159, 1, 11, 0, -1, -1, 7.3707933, 795, 800)\n",
      "ints\n",
      "(160, 1, 11, 0, -1, -1, 7.784445, 800, 805)\n",
      "ints\n",
      "(161, 1, 11, 0, -1, -1, 6.137567, 805, 810)\n",
      "ints\n",
      "(162, 1, 11, 0, -1, -1, 7.3557663, 810, 815)\n",
      "ints\n",
      "(163, 1, 11, 0, -1, -1, 6.2551227, 815, 820)\n",
      "ints\n",
      "(164, 1, 11, 0, -1, -1, 5.805455, 820, 825)\n",
      "ints\n",
      "(165, 1, 11, 0, -1, -1, 7.4670525, 825, 830)\n",
      "ints\n",
      "(166, 1, 11, 0, -1, -1, 6.8472934, 830, 835)\n",
      "ints\n",
      "(167, 1, 11, 0, -1, -1, 3.3883905, 835, 840)\n",
      "ints\n",
      "(168, 1, 12, 0, -1, -1, 5.180148, 840, 845)\n",
      "ints\n",
      "(169, 1, 12, 0, -1, -1, 1.755909, 845, 850)\n",
      "ints\n",
      "(170, 1, 12, 0, -1, -1, 5.3225455, 850, 855)\n",
      "ints\n",
      "(171, 1, 12, 0, -1, -1, 6.715782, 855, 860)\n",
      "ints\n",
      "(172, 1, 12, 0, -1, -1, 3.1083798, 860, 865)\n",
      "ints\n",
      "(173, 1, 12, 0, -1, -1, 1.6358715, 865, 870)\n",
      "ints\n",
      "(174, 1, 12, 0, -1, -1, 7.471433, 870, 875)\n",
      "ints\n",
      "(175, 1, 12, 0, -1, -1, 7.232421, 875, 880)\n",
      "ints\n",
      "(176, 1, 12, 0, -1, -1, 6.3030577, 880, 885)\n",
      "ints\n",
      "(177, 1, 12, 0, -1, -1, 6.454547, 885, 890)\n",
      "ints\n",
      "(178, 1, 12, 0, -1, -1, 6.1221504, 890, 895)\n",
      "ints\n",
      "(179, 1, 12, 0, -1, -1, 0.40038595, 895, 900)\n",
      "ints\n",
      "(180, 1, 12, 0, -1, -1, 7.074007, 900, 905)\n",
      "ints\n",
      "(181, 1, 12, 0, -1, -1, 5.867407, 905, 910)\n",
      "ints\n",
      "(182, 1, 12, 0, -1, -1, 5.4078913, 910, 915)\n",
      "ints\n",
      "(183, 1, 12, 0, -1, -1, 3.136771, 915, 920)\n",
      "ints\n",
      "(184, 1, 12, 0, -1, -1, 0.44827357, 920, 925)\n",
      "ints\n",
      "(185, 1, 13, 0, -1, -1, 6.356928, 925, 930)\n",
      "ints\n",
      "(186, 1, 13, 0, -1, -1, 2.2028346, 930, 935)\n",
      "ints\n",
      "(187, 1, 13, 0, -1, -1, 6.8958454, 935, 940)\n",
      "ints\n",
      "(188, 1, 13, 0, -1, -1, 6.001664, 940, 945)\n",
      "ints\n",
      "(189, 1, 13, 0, -1, -1, 2.9426293, 945, 950)\n",
      "ints\n",
      "(190, 1, 13, 0, -1, -1, 2.768091, 950, 955)\n",
      "ints\n",
      "(191, 1, 13, 0, -1, -1, 6.983242, 955, 960)\n",
      "ints\n",
      "(192, 1, 13, 0, -1, -1, 5.3267417, 960, 965)\n",
      "ints\n",
      "(193, 1, 13, 0, -1, -1, 7.1770306, 965, 970)\n",
      "ints\n",
      "(194, 1, 13, 0, -1, -1, 6.587126, 970, 975)\n",
      "ints\n",
      "(195, 1, 13, 0, -1, -1, 7.1492853, 975, 980)\n",
      "ints\n",
      "(196, 1, 13, 0, -1, -1, 7.1860375, 980, 985)\n",
      "ints\n",
      "(197, 1, 13, 0, -1, -1, 1.4222672, 985, 990)\n",
      "ints\n",
      "(198, 1, 13, 0, -1, -1, 7.177809, 990, 995)\n",
      "ints\n",
      "(199, 1, 13, 0, -1, -1, 4.514841, 995, 1000)\n",
      "ints\n",
      "(200, 1, 14, 0, -1, -1, 1.0959698, 1000, 1005)\n",
      "ints\n",
      "(201, 1, 14, 0, -1, -1, 8.421114, 1005, 1010)\n",
      "ints\n",
      "(202, 1, 14, 0, -1, -1, 7.9915767, 1010, 1015)\n",
      "ints\n",
      "(203, 1, 14, 0, -1, -1, 5.239147, 1015, 1020)\n",
      "ints\n",
      "(204, 1, 14, 0, -1, -1, 6.363211, 1020, 1025)\n",
      "ints\n",
      "(205, 1, 14, 0, -1, -1, 5.952448, 1025, 1030)\n",
      "ints\n",
      "(206, 1, 14, 0, -1, -1, 3.3018231, 1030, 1035)\n",
      "ints\n",
      "(207, 1, 14, 0, -1, -1, 7.0077057, 1035, 1040)\n",
      "ints\n",
      "(208, 1, 14, 0, -1, -1, 5.8293595, 1040, 1045)\n",
      "ints\n",
      "(209, 1, 14, 0, -1, -1, 6.835247, 1045, 1050)\n",
      "ints\n",
      "(210, 1, 14, 0, -1, -1, 7.336357, 1050, 1055)\n",
      "ints\n",
      "(211, 1, 14, 0, -1, -1, 6.4944735, 1055, 1060)\n",
      "ints\n",
      "(212, 1, 14, 0, -1, -1, 5.6917834, 1060, 1065)\n",
      "ints\n",
      "(213, 1, 14, 0, -1, -1, 7.8542013, 1065, 1070)\n",
      "ints\n",
      "(214, 1, 14, 0, -1, -1, 7.238651, 1070, 1075)\n",
      "ints\n",
      "(215, 1, 14, 0, -1, -1, 4.7379994, 1075, 1080)\n",
      "ints\n",
      "(216, 1, 14, 0, -1, -1, 6.955082, 1080, 1085)\n",
      "ints\n",
      "(217, 1, 14, 0, -1, -1, 5.201124, 1085, 1090)\n",
      "ints\n",
      "(218, 1, 14, 0, -1, -1, 5.852474, 1090, 1095)\n",
      "ints\n",
      "(219, 1, 14, 0, -1, -1, 2.3556633, 1095, 1100)\n",
      "ints\n",
      "(220, 1, 14, 0, -1, -1, 2.232068, 1100, 1105)\n",
      "ints\n",
      "(221, 1, 15, 0, -1, -1, 6.0344415, 1105, 1110)\n",
      "ints\n",
      "(222, 1, 15, 0, -1, -1, 4.8259835, 1110, 1115)\n",
      "ints\n",
      "(223, 1, 15, 0, -1, -1, 1.0577605, 1115, 1120)\n",
      "ints\n",
      "(224, 1, 15, 0, -1, -1, 7.124273, 1120, 1125)\n",
      "ints\n",
      "(225, 1, 15, 0, -1, -1, 3.5359035, 1125, 1130)\n",
      "ints\n",
      "(226, 1, 15, 0, -1, -1, 7.637248, 1130, 1135)\n",
      "ints\n",
      "(227, 1, 15, 0, -1, -1, 7.027709, 1135, 1140)\n",
      "ints\n",
      "(228, 1, 15, 0, -1, -1, 7.374627, 1140, 1145)\n",
      "ints\n",
      "(229, 1, 15, 0, -1, -1, 7.136486, 1145, 1150)\n",
      "ints\n",
      "(230, 1, 15, 0, -1, -1, 6.588766, 1150, 1155)\n",
      "ints\n",
      "(231, 1, 15, 0, -1, -1, 7.697177, 1155, 1160)\n",
      "ints\n",
      "(232, 1, 15, 0, -1, -1, 5.9447174, 1160, 1165)\n",
      "ints\n",
      "(233, 1, 15, 0, -1, -1, 6.5863476, 1165, 1170)\n",
      "ints\n",
      "(234, 1, 15, 0, -1, -1, 7.7284737, 1170, 1175)\n",
      "ints\n",
      "(235, 1, 15, 0, -1, -1, 7.4169817, 1175, 1180)\n",
      "ints\n",
      "(236, 1, 15, 0, -1, -1, 6.8843307, 1180, 1185)\n",
      "ints\n",
      "(237, 1, 15, 0, -1, -1, 5.961664, 1185, 1190)\n",
      "ints\n",
      "(238, 1, 15, 0, -1, -1, 3.3337984, 1190, 1195)\n",
      "ints\n",
      "(239, 1, 15, 0, -1, -1, 1.5529735, 1195, 1200)\n",
      "ints\n",
      "(240, 1, 16, 0, -1, -1, 7.572911, 1200, 1205)\n",
      "ints\n",
      "(241, 1, 16, 0, -1, -1, 7.5160604, 1205, 1210)\n",
      "ints\n",
      "(242, 1, 16, 0, -1, -1, 6.735737, 1210, 1215)\n",
      "ints\n",
      "(243, 1, 16, 0, -1, -1, 6.8002768, 1215, 1220)\n",
      "ints\n",
      "(244, 1, 16, 0, -1, -1, 4.8765306, 1220, 1225)\n",
      "ints\n",
      "(245, 1, 16, 0, -1, -1, 7.0713844, 1225, 1230)\n",
      "ints\n",
      "(246, 1, 16, 0, -1, -1, 7.5211835, 1230, 1235)\n",
      "ints\n",
      "(247, 1, 16, 0, -1, -1, 6.4612637, 1235, 1240)\n",
      "ints\n",
      "(248, 1, 16, 0, -1, -1, 7.6655273, 1240, 1245)\n",
      "ints\n",
      "(249, 1, 16, 0, -1, -1, 6.8477354, 1245, 1250)\n",
      "ints\n",
      "(250, 1, 16, 0, -1, -1, 7.258608, 1250, 1255)\n",
      "ints\n",
      "(251, 1, 16, 0, -1, -1, 6.634522, 1255, 1260)\n",
      "ints\n",
      "(252, 1, 16, 0, -1, -1, 6.048767, 1260, 1265)\n",
      "ints\n",
      "(253, 1, 16, 0, -1, -1, 6.341615, 1265, 1270)\n",
      "ints\n",
      "(254, 1, 16, 0, -1, -1, 3.5157576, 1270, 1275)\n",
      "ints\n",
      "(255, 1, 16, 0, -1, -1, 0.9081672, 1275, 1280)\n",
      "ints\n",
      "(256, 1, 16, 0, -1, -1, 4.087834, 1280, 1285)\n",
      "ints\n",
      "(257, 1, 16, 0, -1, -1, 2.4605603, 1285, 1290)\n",
      "ints\n",
      "(258, 1, 16, 0, -1, -1, 4.983288, 1290, 1295)\n",
      "ints\n",
      "(259, 1, 16, 0, -1, -1, 5.2895308, 1295, 1300)\n",
      "ints\n",
      "(260, 1, 17, 0, -1, -1, 5.483184, 1300, 1305)\n",
      "ints\n",
      "(261, 1, 17, 0, -1, -1, 6.100253, 1305, 1310)\n",
      "ints\n",
      "(262, 1, 17, 0, -1, -1, 7.250168, 1310, 1315)\n",
      "ints\n",
      "(263, 1, 17, 0, -1, -1, 5.9272547, 1315, 1320)\n",
      "ints\n",
      "(264, 1, 17, 0, -1, -1, 6.7290173, 1320, 1325)\n",
      "ints\n",
      "(265, 1, 17, 0, -1, -1, 7.6331697, 1325, 1330)\n",
      "ints\n",
      "(266, 1, 17, 0, -1, -1, 6.253603, 1330, 1335)\n",
      "ints\n",
      "(267, 1, 17, 0, -1, -1, 3.38995, 1335, 1340)\n",
      "ints\n",
      "(268, 1, 17, 0, -1, -1, 5.3453355, 1340, 1345)\n",
      "ints\n",
      "(269, 1, 17, 0, -1, -1, 6.906248, 1345, 1350)\n",
      "ints\n",
      "(270, 1, 17, 0, -1, -1, 7.1127925, 1350, 1355)\n",
      "ints\n",
      "(271, 1, 17, 0, -1, -1, 7.7574844, 1355, 1360)\n",
      "ints\n",
      "(272, 1, 17, 0, -1, -1, 6.475987, 1360, 1365)\n",
      "ints\n",
      "(273, 1, 17, 0, -1, -1, 7.9432993, 1365, 1370)\n",
      "ints\n",
      "(274, 1, 17, 0, -1, -1, 6.2781997, 1370, 1375)\n",
      "ints\n",
      "(275, 1, 17, 0, -1, -1, 4.726475, 1375, 1380)\n",
      "ints\n",
      "(276, 1, 17, 0, -1, -1, 3.2778835, 1380, 1385)\n",
      "ints\n",
      "(277, 1, 18, 0, -1, -1, 1.6729304, 1385, 1390)\n",
      "ints\n",
      "(278, 1, 18, 0, -1, -1, 4.452404, 1390, 1395)\n",
      "ints\n",
      "(279, 1, 18, 0, -1, -1, 5.8908253, 1395, 1400)\n",
      "ints\n",
      "(280, 1, 18, 0, -1, -1, 5.936199, 1400, 1405)\n",
      "ints\n",
      "(281, 1, 18, 0, -1, -1, 6.177172, 1405, 1410)\n",
      "ints\n",
      "(282, 1, 18, 0, -1, -1, 7.9438477, 1410, 1415)\n",
      "ints\n",
      "(283, 1, 18, 0, -1, -1, 6.806715, 1415, 1420)\n",
      "ints\n",
      "(284, 1, 18, 0, -1, -1, 6.5484915, 1420, 1425)\n",
      "ints\n",
      "(285, 1, 18, 0, -1, -1, 6.8746305, 1425, 1430)\n",
      "ints\n",
      "(286, 1, 18, 0, -1, -1, 5.2183194, 1430, 1435)\n",
      "ints\n",
      "(287, 1, 18, 0, -1, -1, 7.257603, 1435, 1440)\n",
      "ints\n",
      "(288, 1, 18, 0, -1, -1, 6.8824434, 1440, 1445)\n",
      "ints\n",
      "(289, 1, 18, 0, -1, -1, 1.8274388, 1445, 1450)\n",
      "ints\n",
      "(290, 1, 18, 0, -1, -1, 5.812377, 1450, 1455)\n",
      "ints\n",
      "(291, 1, 18, 0, -1, -1, 7.5182304, 1455, 1460)\n",
      "ints\n",
      "(292, 1, 18, 0, -1, -1, 5.950449, 1460, 1465)\n",
      "ints\n",
      "(293, 1, 18, 0, -1, -1, 7.4426193, 1465, 1470)\n",
      "ints\n",
      "(294, 1, 18, 0, -1, -1, 4.342498, 1470, 1475)\n",
      "ints\n",
      "(295, 1, 19, 0, -1, -1, 6.6160717, 1475, 1480)\n",
      "ints\n",
      "(296, 1, 19, 0, -1, -1, 1.0631477, 1480, 1485)\n",
      "ints\n",
      "(297, 1, 19, 0, -1, -1, 3.0379145, 1485, 1490)\n",
      "ints\n",
      "(298, 1, 19, 0, -1, -1, 7.700755, 1490, 1495)\n",
      "ints\n",
      "(299, 1, 19, 0, -1, -1, 7.2823453, 1495, 1500)\n",
      "ints\n",
      "(300, 1, 19, 0, -1, -1, 3.8313372, 1500, 1505)\n",
      "ints\n",
      "(301, 1, 19, 0, -1, -1, 6.775839, 1505, 1510)\n",
      "ints\n",
      "(302, 1, 19, 0, -1, -1, 7.2315836, 1510, 1515)\n",
      "ints\n",
      "(303, 1, 19, 0, -1, -1, 7.7161684, 1515, 1520)\n",
      "ints\n",
      "(304, 1, 19, 0, -1, -1, 6.597312, 1520, 1525)\n",
      "ints\n",
      "(305, 1, 19, 0, -1, -1, 6.965676, 1525, 1530)\n",
      "ints\n",
      "(306, 1, 19, 0, -1, -1, 7.0844755, 1530, 1535)\n",
      "ints\n",
      "(307, 1, 19, 0, -1, -1, 2.9502833, 1535, 1540)\n",
      "ints\n",
      "(308, 1, 19, 0, -1, -1, 2.3641815, 1540, 1545)\n",
      "ints\n",
      "(309, 1, 19, 0, -1, -1, 0.27401918, 1545, 1550)\n",
      "ints\n",
      "(310, 1, 20, 0, -1, -1, 7.980553, 1550, 1555)\n",
      "ints\n",
      "(311, 1, 20, 0, -1, -1, 7.955928, 1555, 1560)\n",
      "ints\n",
      "(312, 1, 20, 0, -1, -1, 3.631208, 1560, 1565)\n",
      "ints\n",
      "(313, 1, 20, 0, -1, -1, 7.835668, 1565, 1570)\n",
      "ints\n",
      "(314, 1, 20, 0, -1, -1, 5.1374097, 1570, 1575)\n",
      "ints\n",
      "(315, 1, 20, 0, -1, -1, 4.886781, 1575, 1580)\n",
      "ints\n",
      "(316, 1, 20, 0, -1, -1, 7.3386326, 1580, 1585)\n",
      "ints\n",
      "(317, 1, 20, 0, -1, -1, 8.00932, 1585, 1590)\n",
      "ints\n",
      "(318, 1, 20, 0, -1, -1, 5.7241507, 1590, 1595)\n",
      "ints\n",
      "(319, 1, 20, 0, -1, -1, 8.008528, 1595, 1600)\n",
      "ints\n",
      "(320, 1, 20, 0, -1, -1, 7.5328236, 1600, 1605)\n",
      "ints\n",
      "(321, 1, 20, 0, -1, -1, 8.1043415, 1605, 1610)\n",
      "ints\n",
      "(322, 1, 20, 0, -1, -1, 6.8864284, 1610, 1615)\n",
      "ints\n",
      "(323, 1, 20, 0, -1, -1, 6.737382, 1615, 1620)\n",
      "ints\n",
      "(324, 1, 20, 0, -1, -1, 6.50361, 1620, 1625)\n",
      "ints\n",
      "(325, 1, 20, 0, -1, -1, 7.156735, 1625, 1630)\n",
      "ints\n",
      "(326, 1, 21, 0, -1, -1, 2.7323165, 1630, 1635)\n",
      "ints\n",
      "(327, 1, 21, 0, -1, -1, 6.0545406, 1635, 1640)\n",
      "ints\n",
      "(328, 1, 21, 0, -1, -1, 8.482059, 1640, 1645)\n",
      "ints\n",
      "(329, 1, 21, 0, -1, -1, 5.6519737, 1645, 1650)\n",
      "ints\n",
      "(330, 1, 21, 0, -1, -1, 4.34427, 1650, 1655)\n",
      "ints\n",
      "(331, 1, 21, 0, -1, -1, 6.071471, 1655, 1660)\n",
      "ints\n",
      "(332, 1, 21, 0, -1, -1, 4.4950156, 1660, 1665)\n",
      "ints\n",
      "(333, 1, 21, 0, -1, -1, 6.6117616, 1665, 1670)\n",
      "ints\n",
      "(334, 1, 21, 0, -1, -1, 8.082737, 1670, 1675)\n",
      "ints\n",
      "(335, 1, 21, 0, -1, -1, 6.211252, 1675, 1680)\n",
      "ints\n",
      "(336, 1, 21, 0, -1, -1, 7.334589, 1680, 1685)\n",
      "ints\n",
      "(337, 1, 21, 0, -1, -1, 7.341531, 1685, 1690)\n",
      "ints\n",
      "(338, 1, 21, 0, -1, -1, 7.0280623, 1690, 1695)\n",
      "ints\n",
      "(339, 1, 21, 0, -1, -1, 7.5908737, 1695, 1700)\n",
      "ints\n",
      "(340, 1, 21, 0, -1, -1, 6.98133, 1700, 1705)\n",
      "ints\n",
      "(341, 1, 21, 0, -1, -1, 7.535557, 1705, 1710)\n",
      "ints\n",
      "(342, 1, 21, 0, -1, -1, 5.407419, 1710, 1715)\n",
      "ints\n",
      "(343, 1, 22, 0, -1, -1, 7.140773, 1715, 1720)\n",
      "ints\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(344, 1, 22, 0, -1, -1, 7.817665, 1720, 1725)\n",
      "ints\n",
      "(345, 1, 22, 0, -1, -1, 6.755392, 1725, 1730)\n",
      "ints\n",
      "(346, 1, 22, 0, -1, -1, 6.866767, 1730, 1735)\n",
      "ints\n",
      "(347, 1, 22, 0, -1, -1, 7.6513844, 1735, 1740)\n",
      "ints\n",
      "(348, 1, 22, 0, -1, -1, 7.5168767, 1740, 1745)\n",
      "ints\n",
      "(349, 1, 22, 0, -1, -1, 8.00846, 1745, 1750)\n",
      "ints\n",
      "(350, 1, 22, 0, -1, -1, 7.846744, 1750, 1755)\n",
      "ints\n",
      "(351, 1, 22, 0, -1, -1, 6.223235, 1755, 1760)\n",
      "ints\n",
      "(352, 1, 22, 0, -1, -1, 5.0585814, 1760, 1765)\n",
      "ints\n",
      "(353, 1, 22, 0, -1, -1, 7.2215796, 1765, 1770)\n",
      "ints\n",
      "(354, 1, 22, 0, -1, -1, 1.7390028, 1770, 1775)\n",
      "ints\n",
      "(355, 1, 22, 0, -1, -1, 1.1208501, 1775, 1780)\n",
      "ints\n",
      "(356, 1, 22, 0, -1, -1, 6.3022385, 1780, 1785)\n",
      "ints\n",
      "(357, 1, 22, 0, -1, -1, 5.177369, 1785, 1790)\n",
      "ints\n",
      "(358, 1, 23, 0, -1, -1, 7.631801, 1790, 1795)\n",
      "ints\n",
      "(359, 1, 23, 0, -1, -1, 6.8545775, 1795, 1800)\n",
      "ints\n",
      "(360, 1, 23, 0, -1, -1, 1.6417599, 1800, 1805)\n",
      "ints\n",
      "(361, 1, 23, 0, -1, -1, 7.047724, 1805, 1810)\n",
      "ints\n",
      "(362, 1, 23, 0, -1, -1, 7.2757936, 1810, 1815)\n",
      "ints\n",
      "(363, 1, 23, 0, -1, -1, 6.9573793, 1815, 1820)\n",
      "ints\n",
      "(364, 1, 23, 0, -1, -1, 4.5283318, 1820, 1825)\n",
      "ints\n",
      "(365, 1, 23, 0, -1, -1, 7.441043, 1825, 1830)\n",
      "ints\n",
      "(366, 1, 23, 0, -1, -1, 7.6419425, 1830, 1835)\n",
      "ints\n",
      "(367, 1, 23, 0, -1, -1, 7.4417605, 1835, 1840)\n",
      "ints\n",
      "(368, 1, 23, 0, -1, -1, 7.62955, 1840, 1845)\n",
      "ints\n",
      "(369, 1, 23, 0, -1, -1, 7.909589, 1845, 1850)\n",
      "ints\n",
      "(370, 1, 23, 0, -1, -1, 7.637204, 1850, 1855)\n",
      "ints\n",
      "(371, 1, 23, 0, -1, -1, 6.282478, 1855, 1860)\n",
      "ints\n",
      "(372, 1, 23, 0, -1, -1, 4.4055176, 1860, 1865)\n",
      "ints\n",
      "(373, 1, 24, 0, -1, -1, 2.067489, 1865, 1870)\n",
      "ints\n",
      "(374, 1, 24, 0, -1, -1, 7.085988, 1870, 1875)\n",
      "ints\n",
      "(375, 1, 24, 0, -1, -1, 2.5977108, 1875, 1880)\n",
      "ints\n",
      "(376, 1, 24, 0, -1, -1, 8.281492, 1880, 1885)\n",
      "ints\n",
      "(377, 1, 24, 0, -1, -1, 7.0787215, 1885, 1890)\n",
      "ints\n",
      "(378, 1, 24, 0, -1, -1, 6.13496, 1890, 1895)\n",
      "ints\n",
      "(379, 1, 24, 0, -1, -1, 6.5302525, 1895, 1900)\n",
      "ints\n",
      "(380, 1, 24, 0, -1, -1, 7.643163, 1900, 1905)\n",
      "ints\n",
      "(381, 1, 24, 0, -1, -1, 7.181849, 1905, 1910)\n",
      "ints\n",
      "(382, 1, 24, 0, -1, -1, 5.7014995, 1910, 1915)\n",
      "ints\n",
      "(383, 1, 24, 0, -1, -1, 7.4281487, 1915, 1920)\n",
      "ints\n",
      "(384, 1, 24, 0, -1, -1, 7.5263815, 1920, 1925)\n",
      "ints\n",
      "(385, 1, 24, 0, -1, -1, 7.3180113, 1925, 1930)\n",
      "ints\n",
      "(386, 1, 24, 0, -1, -1, 7.030386, 1930, 1935)\n",
      "ints\n",
      "(387, 1, 24, 0, -1, -1, 6.7058077, 1935, 1940)\n",
      "ints\n",
      "(388, 1, 24, 0, -1, -1, 6.7306557, 1940, 1945)\n",
      "ints\n",
      "(389, 1, 24, 0, -1, -1, 5.9165354, 1945, 1950)\n",
      "ints\n",
      "(390, 1, 25, 0, -1, -1, 1.6876287, 1950, 1955)\n",
      "ints\n",
      "(391, 1, 25, 0, -1, -1, 7.0986958, 1955, 1960)\n",
      "ints\n",
      "(392, 1, 25, 0, -1, -1, 5.572201, 1960, 1965)\n",
      "ints\n",
      "(393, 1, 25, 0, -1, -1, 3.3964927, 1965, 1970)\n",
      "ints\n",
      "(394, 1, 25, 0, -1, -1, 7.064783, 1970, 1975)\n",
      "ints\n",
      "(395, 1, 25, 0, -1, -1, 7.4762797, 1975, 1980)\n",
      "ints\n",
      "(396, 1, 25, 0, -1, -1, 5.5982943, 1980, 1985)\n",
      "ints\n",
      "(397, 1, 25, 0, -1, -1, 8.062967, 1985, 1990)\n",
      "ints\n",
      "(398, 1, 25, 0, -1, -1, 6.813657, 1990, 1995)\n",
      "ints\n",
      "(399, 1, 25, 0, -1, -1, 6.814254, 1995, 2000)\n",
      "ints\n",
      "(400, 1, 25, 0, -1, -1, 7.104128, 2000, 2005)\n",
      "ints\n",
      "(401, 1, 25, 0, -1, -1, 6.883565, 2005, 2010)\n",
      "ints\n",
      "(402, 1, 25, 0, -1, -1, 5.244332, 2010, 2015)\n",
      "ints\n",
      "(403, 1, 25, 0, -1, -1, 6.879403, 2015, 2020)\n",
      "ints\n",
      "(404, 1, 25, 0, -1, -1, 6.180684, 2020, 2025)\n",
      "ints\n",
      "(405, 1, 25, 0, -1, -1, 7.76459, 2025, 2030)\n",
      "ints\n",
      "(406, 1, 25, 0, -1, -1, 6.5151186, 2030, 2035)\n",
      "ints\n",
      "(407, 1, 25, 0, -1, -1, 5.7503414, 2035, 2040)\n",
      "ints\n",
      "(408, 1, 25, 0, -1, -1, 3.8065119, 2040, 2045)\n",
      "ints\n",
      "(409, 1, 25, 0, -1, -1, 0.9677709, 2045, 2050)\n",
      "ints\n",
      "(410, 1, 26, 0, -1, -1, 2.9149768, 2050, 2055)\n",
      "ints\n",
      "(411, 1, 26, 0, -1, -1, 2.8331623, 2055, 2060)\n",
      "ints\n",
      "(412, 1, 26, 0, -1, -1, 6.3915334, 2060, 2065)\n",
      "ints\n",
      "(413, 1, 26, 0, -1, -1, 7.9419146, 2065, 2070)\n",
      "ints\n",
      "(414, 1, 26, 0, -1, -1, 7.6856804, 2070, 2075)\n",
      "ints\n",
      "(415, 1, 26, 0, -1, -1, 7.6223083, 2075, 2080)\n",
      "ints\n",
      "(416, 1, 26, 0, -1, -1, 7.112634, 2080, 2085)\n",
      "ints\n",
      "(417, 1, 26, 0, -1, -1, 6.1594577, 2085, 2090)\n",
      "ints\n",
      "(418, 1, 26, 0, -1, -1, 7.018839, 2090, 2095)\n",
      "ints\n",
      "(419, 1, 26, 0, -1, -1, 6.6226206, 2095, 2100)\n",
      "ints\n",
      "(420, 1, 26, 0, -1, -1, 7.758338, 2100, 2105)\n",
      "ints\n",
      "(421, 1, 26, 0, -1, -1, 7.5096297, 2105, 2110)\n",
      "ints\n",
      "(422, 1, 26, 0, -1, -1, 7.8804355, 2110, 2115)\n",
      "ints\n",
      "(423, 1, 26, 0, -1, -1, 1.5137777, 2115, 2120)\n",
      "ints\n",
      "(424, 1, 26, 0, -1, -1, 4.934617, 2120, 2125)\n",
      "ints\n",
      "(425, 1, 26, 0, -1, -1, 6.6402173, 2125, 2130)\n",
      "ints\n",
      "(426, 1, 26, 0, -1, -1, 5.9752994, 2130, 2135)\n",
      "ints\n",
      "(427, 1, 27, 0, -1, -1, 3.7017279, 2135, 2140)\n",
      "ints\n",
      "(428, 1, 27, 0, -1, -1, 7.144963, 2140, 2145)\n",
      "ints\n",
      "(429, 1, 27, 0, -1, -1, 7.8062067, 2145, 2150)\n",
      "ints\n",
      "(430, 1, 27, 0, -1, -1, 6.855981, 2150, 2155)\n",
      "ints\n",
      "(431, 1, 27, 0, -1, -1, 5.682701, 2155, 2160)\n",
      "ints\n",
      "(432, 1, 27, 0, -1, -1, 7.448551, 2160, 2165)\n",
      "ints\n",
      "(433, 1, 27, 0, -1, -1, 7.956564, 2165, 2170)\n",
      "ints\n",
      "(434, 1, 27, 0, -1, -1, 7.000199, 2170, 2175)\n",
      "ints\n",
      "(435, 1, 27, 0, -1, -1, 7.364579, 2175, 2180)\n",
      "ints\n",
      "(436, 1, 27, 0, -1, -1, 7.516678, 2180, 2185)\n",
      "ints\n",
      "(437, 1, 27, 0, -1, -1, 6.4414606, 2185, 2190)\n",
      "ints\n",
      "(438, 1, 27, 0, -1, -1, 6.558136, 2190, 2195)\n",
      "ints\n",
      "(439, 1, 27, 0, -1, -1, 5.389866, 2195, 2200)\n",
      "ints\n",
      "(440, 1, 27, 0, -1, -1, 5.0142355, 2200, 2205)\n",
      "ints\n",
      "(441, 1, 27, 0, -1, -1, 4.6540017, 2205, 2210)\n",
      "ints\n",
      "(442, 1, 27, 0, -1, -1, 4.5726957, 2210, 2215)\n",
      "ints\n",
      "(443, 1, 28, 0, -1, -1, 1.3678753, 2215, 2220)\n",
      "ints\n",
      "(444, 1, 28, 0, -1, -1, 1.9122635, 2220, 2225)\n",
      "ints\n",
      "(445, 1, 28, 0, -1, -1, 2.7804356, 2225, 2230)\n",
      "ints\n",
      "(446, 1, 28, 0, -1, -1, 7.3832216, 2230, 2235)\n",
      "ints\n",
      "(447, 1, 28, 0, -1, -1, 7.390727, 2235, 2240)\n",
      "ints\n",
      "(448, 1, 28, 0, -1, -1, 7.8470306, 2240, 2245)\n",
      "ints\n",
      "(449, 1, 28, 0, -1, -1, 6.7416058, 2245, 2250)\n",
      "ints\n",
      "(450, 1, 28, 0, -1, -1, 5.3287196, 2250, 2255)\n",
      "ints\n",
      "(451, 1, 28, 0, -1, -1, 7.7117605, 2255, 2260)\n",
      "ints\n",
      "(452, 1, 28, 0, -1, -1, 4.2996683, 2260, 2265)\n",
      "ints\n",
      "(453, 1, 28, 0, -1, -1, 5.399722, 2265, 2270)\n",
      "ints\n",
      "(454, 1, 28, 0, -1, -1, 5.741461, 2270, 2275)\n",
      "ints\n",
      "(455, 1, 28, 0, -1, -1, 5.8191457, 2275, 2280)\n",
      "ints\n",
      "(456, 1, 28, 0, -1, -1, 3.8237765, 2280, 2285)\n",
      "ints\n",
      "(457, 1, 29, 0, -1, -1, 7.4389343, 2285, 2290)\n",
      "ints\n",
      "(458, 1, 29, 0, -1, -1, 7.0271873, 2290, 2295)\n",
      "ints\n",
      "(459, 1, 29, 0, -1, -1, 7.232771, 2295, 2300)\n",
      "ints\n",
      "(460, 1, 29, 0, -1, -1, 7.611714, 2300, 2305)\n",
      "ints\n",
      "(461, 1, 29, 0, -1, -1, 7.778905, 2305, 2310)\n",
      "ints\n",
      "(462, 1, 29, 0, -1, -1, 7.8616796, 2310, 2315)\n",
      "ints\n",
      "(463, 1, 29, 0, -1, -1, 7.5320315, 2315, 2320)\n",
      "ints\n",
      "(464, 1, 29, 0, -1, -1, 7.312891, 2320, 2325)\n",
      "ints\n",
      "(465, 1, 29, 0, -1, -1, 7.362466, 2325, 2330)\n",
      "ints\n",
      "(466, 1, 29, 0, -1, -1, 7.135393, 2330, 2335)\n",
      "pred pont\n",
      "(792.40716553, 10.64623165, True, False, 0.53316313)\n",
      "pred pont\n",
      "(887.86669922, 181.61003113, True, False, 0.90097111)\n",
      "pred pont\n",
      "(851.44158936, 106.54940796, True, False, 0.74797297)\n",
      "pred pont\n",
      "(839.88604736, 227.22018433, True, False, 0.81310922)\n",
      "pred pont\n",
      "(923.94329834, 202.45472717, True, False, 0.54563963)\n",
      "pred pont\n",
      "(973.05096436, 72.96406555, True, False, 0.83320671)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(947.70294189, 10.31491566, True, False, 0.28221521)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1247.8314209, 84.32096863, True, False, 0.80942345)\n",
      "pred pont\n",
      "(1475.33776855, 181.12657166, True, False, 0.98006237)\n",
      "pred pont\n",
      "(1392.70715332, 154.92472839, True, False, 0.7244252)\n",
      "pred pont\n",
      "(1464.54089355, 216.92788696, True, False, 0.75809383)\n",
      "pred pont\n",
      "(1489.4909668, 155.95478821, True, False, 0.75733232)\n",
      "pred pont\n",
      "(647.50390625, 133.14553833, True, False, 0.92530251)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(587.24316406, 17.52002144, True, False, 0.52240801)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1753.2668457, 264.93191528, True, False, 0.95789123)\n",
      "pred pont\n",
      "(1571.41943359, 468.13937378, True, False, 0.93221241)\n",
      "pred pont\n",
      "(1621.29882812, 419.61178589, True, False, 0.84444231)\n",
      "pred pont\n",
      "(1500.04260254, 469.59713745, True, False, 0.98066282)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(697.58868408, 286.84320068, True, False, 1.02689457)\n",
      "pred pont\n",
      "(562.95849609, 505.80056763, True, False, 0.8185007)\n",
      "pred pont\n",
      "(610.39599609, 456.24502563, True, False, 0.82469541)\n",
      "pred pont\n",
      "(490.71438599, 491.79446411, True, False, 0.99048227)\n",
      "pred pont\n",
      "(565.35534668, 564.71936035, True, False, 0.71634263)\n",
      "pred pont\n",
      "(1115.95837402, 287.28747559, True, False, 1.05329728)\n",
      "pred pont\n",
      "(864.66723633, 263.75393677, True, False, 0.83176559)\n",
      "pred pont\n",
      "(935.67449951, 264.91104126, True, False, 0.8799876)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(815.62542725, 311.37741089, True, False, 0.81140244)\n",
      "pred pont\n",
      "(1922.03723145, 312.82373047, True, False, 0.95061123)\n",
      "pred pont\n",
      "(1871.71826172, 70.86807251, True, False, 0.9188996)\n",
      "pred pont\n",
      "(1895.73522949, 156.9573822, True, False, 0.75372213)\n",
      "pred pont\n",
      "(1897.47924805, 34.9794426, True, False, 0.84998572)\n",
      "pred pont\n",
      "(1837.94958496, 49.07504654, True, False, 0.61211973)\n",
      "pred pont\n",
      "(2243.53662109, 383.9704895, True, False, 0.88924479)\n",
      "pred pont\n",
      "(2245.81591797, 121.34230804, True, False, 0.87406987)\n",
      "pred pont\n",
      "(2232.17944336, 214.68135071, True, False, 0.81919193)\n",
      "pred pont\n",
      "(2292.6496582, 108.7675705, True, False, 0.82559836)\n",
      "pred pont\n",
      "(2243.76831055, 97.57376862, True, False, 0.72253692)\n",
      "pred pont\n",
      "(504.77069092, 588.21478271, True, False, 0.78939301)\n",
      "pred pont\n",
      "(599.53375244, 756.37121582, True, False, 0.38167691)\n",
      "pred pont\n",
      "(552.50494385, 683.1751709, True, False, 0.56368738)\n",
      "pred pont\n",
      "(637.39526367, 757.38226318, True, False, 0.21903442)\n",
      "pred pont\n",
      "(599.68182373, 779.50915527, True, False, 0.21401228)\n",
      "pred pont\n",
      "(1201.81262207, 670.68273926, True, False, 0.99614686)\n",
      "pred pont\n",
      "(1174.74768066, 925.6918335, True, False, 1.04903352)\n",
      "pred pont\n",
      "(1177.29162598, 853.06756592, True, False, 0.86814934)\n",
      "pred pont\n",
      "(1127.38842773, 958.75701904, True, False, 0.90252405)\n",
      "pred pont\n",
      "(1235.55969238, 973.8338623, True, False, 0.90978968)\n",
      "pred pont\n",
      "(600.55645752, 780.31695557, True, False, 0.68675739)\n",
      "pred pont\n",
      "(504.69393921, 1007.77813721, True, False, 1.10114312)\n",
      "pred pont\n",
      "(562.66888428, 912.52252197, True, False, 0.70279998)\n",
      "pred pont\n",
      "(456.84628296, 1008.76049805, True, False, 0.97890955)\n",
      "pred pont\n",
      "(538.82214355, 1056.89489746, True, False, 0.86052519)\n",
      "pred pont\n",
      "(1596.09753418, 864.61419678, True, False, 0.97671562)\n",
      "pred pont\n",
      "(1620.10302734, 578.07727051, True, False, 0.88168627)\n",
      "pred pont\n",
      "(1608.92358398, 672.24395752, True, False, 0.75491703)\n",
      "pred pont\n",
      "(1645.98376465, 552.25720215, True, False, 0.65705413)\n",
      "pred pont\n",
      "(1584.76660156, 551.70428467, True, False, 0.78409719)\n",
      "pred pont\n",
      "(733.53265381, 874.72027588, True, False, 0.83274907)\n",
      "pred pont\n",
      "(733.38372803, 611.46276855, True, False, 0.81017631)\n",
      "pred pont\n",
      "(742.89825439, 684.25262451, True, False, 0.81651175)\n",
      "pred pont\n",
      "(790.16693115, 540.98370361, True, False, 0.92162794)\n",
      "pred pont\n",
      "(683.66693115, 552.28442383, True, False, 0.9185946)\n",
      "pred pont\n",
      "(994.63848877, 888.03601074, True, False, 0.89687377)\n",
      "pred pont\n",
      "(1151.98254395, 1092.56665039, True, False, 0.95110172)\n",
      "pred pont\n",
      "(1104.09155273, 1033.76416016, True, False, 0.81646132)\n",
      "pred pont\n",
      "(1140.43981934, 1142.91906738, True, False, 0.79134351)\n",
      "pred pont\n",
      "(1212.02026367, 1093.76745605, True, False, 0.79779685)\n",
      "pred pont\n",
      "(346.49362183, 899.25488281, True, False, 0.85228062)\n",
      "pred pont\n",
      "(432.37518311, 1127.16772461, True, False, 0.99737895)\n",
      "pred pont\n",
      "(408.66891479, 1032.37841797, True, False, 0.74897718)\n",
      "pred pont\n",
      "(397.7286377, 1164.62768555, True, False, 0.81804746)\n",
      "pred pont\n",
      "(456.48248291, 1151.70251465, True, False, 0.52472067)\n",
      "pred pont\n",
      "(685.23248291, 1106.01855469, True, False, 0.40594512)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(711.80822754, 1175.34375, True, False, 0.25895026)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(863.07067871, 1139.79858398, True, False, 0.91941023)\n",
      "pred pont\n",
      "(719.31933594, 913.65380859, True, False, 0.78679603)\n",
      "pred pont\n",
      "(733.87115479, 1007.35986328, True, False, 0.73557687)\n",
      "pred pont\n",
      "(746.01940918, 886.60046387, True, False, 0.71331072)\n",
      "pred pont\n",
      "(662.64099121, 899.8493042, True, False, 0.56382489)\n",
      "pred pont\n",
      "(1812.76257324, 1273.37695312, True, False, 0.50915807)\n",
      "pred pont\n",
      "(1874.04431152, 1116.38098145, True, False, 0.82718188)\n",
      "pred pont\n",
      "(1848.01452637, 1188.31689453, True, False, 0.843099)\n",
      "pred pont\n",
      "(1955.68518066, 1102.03393555, True, False, 1.00550687)\n",
      "pred pont\n",
      "(1871.3137207, 1044.0291748, True, False, 0.88632286)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1453.14013672, 23.22694969, True, False, 0.29711187)\n",
      "pred pont\n",
      "(1405.32519531, 8.03774929, True, False, 0.24661729)\n",
      "pred pont\n",
      "(1499.48681641, 47.43345261, True, False, 0.57236916)\n",
      "pred pont\n",
      "(1510.9342041, 22.70692444, True, False, 0.31409261)\n",
      "pred pont\n",
      "(1404.64343262, 9.48956394, True, False, 0.36583006)\n",
      "pred pont\n",
      "(1464.76245117, 191.5993042, True, False, 0.7598775)\n",
      "pred pont\n",
      "(1463.45910645, 119.73809814, True, False, 0.64686406)\n",
      "pred pont\n",
      "(1440.72875977, 239.57868958, True, False, 0.48191351)\n",
      "pred pont\n",
      "(1521.83190918, 230.00646973, True, False, 0.28875408)\n",
      "pred pont\n",
      "(721.66516113, 16.41791153, True, False, 0.65522987)\n",
      "pred pont\n",
      "(938.22637939, 95.98258972, True, False, 0.97866285)\n",
      "pred pont\n",
      "(865.08276367, 61.27973938, True, False, 0.76911491)\n",
      "pred pont\n",
      "(959.06848145, 167.63430786, True, False, 0.80588347)\n",
      "pred pont\n",
      "(997.53723145, 70.88697052, True, False, 0.88459718)\n",
      "pred pont\n",
      "(1297.33496094, 17.71559525, True, False, 0.60184336)\n",
      "pred pont\n",
      "(1380.12780762, 252.05513, True, False, 1.03261721)\n",
      "pred pont\n",
      "(1344.05322266, 180.50015259, True, False, 0.75410664)\n",
      "pred pont\n",
      "(1346.43811035, 311.89880371, True, False, 0.84273559)\n",
      "pred pont\n",
      "(1439.31408691, 240.94529724, True, False, 0.80393785)\n",
      "pred pont\n",
      "(1094.0324707, 215.58070374, True, False, 0.95777184)\n",
      "pred pont\n",
      "(1225.96826172, 9.94348526, True, False, 0.4273411)\n",
      "pred pont\n",
      "(1198.75732422, 46.94051743, True, False, 0.57142711)\n",
      "pred pont\n",
      "(1261.3046875, 19.9484272, True, False, 0.20063527)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1777.01538086, 239.06634521, True, False, 1.02236104)\n",
      "pred pont\n",
      "(1536.65222168, 347.06835938, True, False, 0.84679246)\n",
      "pred pont\n",
      "(1607.28173828, 323.03326416, True, False, 0.8446604)\n",
      "pred pont\n",
      "(1487.9197998, 301.97036743, True, False, 0.96083957)\n",
      "pred pont\n",
      "(1514.1763916, 396.47766113, True, False, 0.68114293)\n",
      "pred pont\n",
      "(2255.97705078, 383.40008545, True, False, 0.85888916)\n",
      "pred pont\n",
      "(2245.32275391, 130.90811157, True, False, 0.93110424)\n",
      "pred pont\n",
      "(2231.73706055, 216.65344238, True, False, 0.86530048)\n",
      "pred pont\n",
      "(2293.16772461, 109.0787735, True, False, 0.89862996)\n",
      "pred pont\n",
      "(2233.12158203, 97.91549683, True, False, 0.7780565)\n",
      "pred pont\n",
      "(301.33392334, 612.9619751, True, False, 0.85257125)\n",
      "pred pont\n",
      "(431.55130005, 814.43548584, True, False, 0.79039854)\n",
      "pred pont\n",
      "(361.33096313, 720.79296875, True, False, 0.71361661)\n",
      "pred pont\n",
      "(432.77923584, 838.82495117, True, False, 0.40210128)\n",
      "pred pont\n",
      "(504.31008911, 830.2366333, True, False, 0.69186258)\n",
      "pred pont\n",
      "(541.05340576, 613.06390381, True, False, 0.94822699)\n",
      "pred pont\n",
      "(457.13394165, 336.91503906, True, False, 0.82189029)\n",
      "pred pont\n",
      "(469.90881348, 409.09738159, True, False, 0.75735098)\n",
      "pred pont\n",
      "(503.22958374, 276.85134888, True, False, 0.91765106)\n",
      "pred pont\n",
      "(395.71231079, 288.60644531, True, False, 0.95619309)\n",
      "pred pont\n",
      "(371.18939209, 828.47119141, True, False, 0.56698227)\n",
      "pred pont\n",
      "(409.31381226, 1043.88427734, True, False, 0.99349183)\n",
      "pred pont\n",
      "(407.85571289, 961.16937256, True, False, 0.84151924)\n",
      "pred pont\n",
      "(359.65429688, 1080.16455078, True, False, 0.68842077)\n",
      "pred pont\n",
      "(442.93011475, 1093.66601562, True, False, 0.78116679)\n",
      "pred pont\n",
      "(1607.37609863, 862.64813232, True, False, 0.84623849)\n",
      "pred pont\n",
      "(1597.21533203, 565.87225342, True, False, 0.85702115)\n",
      "pred pont\n",
      "(1607.32409668, 649.71722412, True, False, 0.7727834)\n",
      "pred pont\n",
      "(1631.86450195, 540.62176514, True, False, 0.76084083)\n",
      "pred pont\n",
      "(1549.75964355, 552.84259033, True, False, 0.69387221)\n",
      "pred pont\n",
      "(1765.79370117, 995.54364014, True, False, 0.88684404)\n",
      "pred pont\n",
      "(1729.59289551, 746.01159668, True, False, 0.82124442)\n",
      "pred pont\n",
      "(1750.98339844, 840.2331543, True, False, 0.72566414)\n",
      "pred pont\n",
      "(1765.44360352, 719.84057617, True, False, 0.80112267)\n",
      "pred pont\n",
      "(1693.01098633, 753.74353027, True, False, 0.60336173)\n",
      "pred pont\n",
      "(1320.1418457, 1020.42376709, True, False, 0.96412951)\n",
      "pred pont\n",
      "(1357.90783691, 769.45458984, True, False, 0.86020035)\n",
      "pred pont\n",
      "(1344.86804199, 841.4911499, True, False, 0.77827746)\n",
      "pred pont\n",
      "(1415.62780762, 744.54016113, True, False, 0.71198362)\n",
      "pred pont\n",
      "(1321.73083496, 707.03948975, True, False, 0.92855334)\n",
      "pred pont\n",
      "(983.90435791, 1272.05883789, True, False, 0.47967264)\n",
      "pred pont\n",
      "(1007.63806152, 1081.46459961, True, False, 0.83404928)\n",
      "pred pont\n",
      "(985.28662109, 1163.71765137, True, False, 0.80623645)\n",
      "pred pont\n",
      "(973.57312012, 1031.12719727, True, False, 0.76646703)\n",
      "pred pont\n",
      "(1079.41943359, 1068.22155762, True, False, 0.84583271)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(515.79376221, 120.58176422, True, False, 0.87115413)\n",
      "pred pont\n",
      "(468.019104, 36.48135757, True, False, 0.55748433)\n",
      "pred pont\n",
      "(492.69830322, 191.91279602, True, False, 0.86610824)\n",
      "pred pont\n",
      "(575.12078857, 154.32919312, True, False, 0.69756126)\n",
      "pred pont\n",
      "(1174.8449707, 36.31671143, True, False, 0.85325652)\n",
      "pred pont\n",
      "(1307.77941895, 227.78182983, True, False, 0.83440077)\n",
      "pred pont\n",
      "(1236.47875977, 143.75683594, True, False, 0.68588936)\n",
      "pred pont\n",
      "(1308.09997559, 274.22293091, True, False, 0.65375447)\n",
      "pred pont\n",
      "(1356.46496582, 228.45977783, True, False, 0.7460885)\n",
      "pred pont\n",
      "(742.92041016, 155.95297241, True, False, 1.07896078)\n",
      "pred pont\n",
      "(468.2897644, 71.83299255, True, False, 0.81224531)\n",
      "pred pont\n",
      "(540.04846191, 94.7565918, True, False, 0.80286813)\n",
      "pred pont\n",
      "(443.5645752, 24.38129044, True, False, 0.54086149)\n",
      "pred pont\n",
      "(418.90856934, 97.18229675, True, False, 0.77107257)\n",
      "pred pont\n",
      "(1440.26831055, 155.02209473, True, False, 1.04962349)\n",
      "pred pont\n",
      "(1307.84521484, 396.38461304, True, False, 0.97220176)\n",
      "pred pont\n",
      "(1357.18713379, 336.29119873, True, False, 0.8777293)\n",
      "pred pont\n",
      "(1236.55688477, 421.57110596, True, False, 0.97604132)\n",
      "pred pont\n",
      "(1332.86560059, 468.39196777, True, False, 0.77501786)\n",
      "pred pont\n",
      "(1778.47167969, 216.96516418, True, False, 1.05020165)\n",
      "pred pont\n",
      "(1619.23754883, 396.80731201, True, False, 0.95793128)\n",
      "pred pont\n",
      "(1668.42553711, 347.48431396, True, False, 0.83215475)\n",
      "pred pont\n",
      "(1573.81262207, 394.86804199, True, False, 0.73919821)\n",
      "pred pont\n",
      "(1632.8605957, 443.56640625, True, False, 0.6798588)\n",
      "pred pont\n",
      "(2162.02783203, 240.53361511, True, False, 1.01432645)\n",
      "pred pont\n",
      "(2050.6315918, 50.01905823, True, False, 0.62188113)\n",
      "pred pont\n",
      "(2075.56713867, 142.86825562, True, False, 0.66021246)\n",
      "pred pont\n",
      "(2074.85400391, 35.32290268, True, False, 0.27418131)\n",
      "pred pont\n",
      "(2028.32275391, 19.86341286, True, False, 0.23347579)\n",
      "pred pont\n",
      "(864.59460449, 850.675354, True, False, 0.87663841)\n",
      "pred pont\n",
      "(828.0657959, 563.11553955, True, False, 0.89949054)\n",
      "pred pont\n",
      "(829.128479, 636.31640625, True, False, 0.82580543)\n",
      "pred pont\n",
      "(866.19519043, 503.06674194, True, False, 0.88332129)\n",
      "pred pont\n",
      "(779.71520996, 494.06045532, True, False, 0.73321825)\n",
      "pred pont\n",
      "(1597.2310791, 862.91064453, True, False, 0.93228614)\n",
      "pred pont\n",
      "(1596.85913086, 565.71337891, True, False, 0.84612548)\n",
      "pred pont\n",
      "(1597.61914062, 649.84887695, True, False, 0.77308756)\n",
      "pred pont\n",
      "(1631.96337891, 540.18450928, True, False, 0.75553542)\n",
      "pred pont\n",
      "(1559.76965332, 552.10443115, True, False, 0.65330845)\n",
      "pred pont\n",
      "(1320.02453613, 1009.31854248, True, False, 0.95118415)\n",
      "pred pont\n",
      "(1367.30444336, 767.08221436, True, False, 0.86040908)\n",
      "pred pont\n",
      "(1356.01928711, 829.81549072, True, False, 0.79718459)\n",
      "pred pont\n",
      "(1406.26611328, 709.45111084, True, False, 0.85024887)\n",
      "pred pont\n",
      "(1309.51013184, 698.00964355, True, False, 0.96284986)\n",
      "pred pont\n",
      "(730.69366455, 1117.48608398, True, False, 0.92497784)\n",
      "pred pont\n",
      "(697.4418335, 863.95166016, True, False, 0.77713102)\n",
      "pred pont\n",
      "(696.91967773, 936.78601074, True, False, 0.84172028)\n",
      "pred pont\n",
      "(756.03399658, 815.17883301, True, False, 0.92293644)\n",
      "pred pont\n",
      "(660.51879883, 805.83770752, True, False, 0.85954416)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(972.40344238, 59.17896271, True, False, 0.84609991)\n",
      "pred pont\n",
      "(973.37384033, 9.25512409, True, False, 0.64612848)\n",
      "pred pont\n",
      "(924.14984131, 119.43035889, True, False, 0.63986671)\n",
      "pred pont\n",
      "(1033.65087891, 98.31760406, True, False, 0.53694892)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1475.05615234, 24.54253197, True, False, 0.49624723)\n",
      "pred pont\n",
      "(1511.44030762, 6.94428205, True, False, 0.23302633)\n",
      "pred pont\n",
      "(1417.21276855, 48.354496, True, False, 0.62495613)\n",
      "pred pont\n",
      "(1453.17260742, 48.27064514, True, False, 0.35370073)\n",
      "pred pont\n",
      "(1165.35437012, 9.43509388, True, False, 0.39668649)\n",
      "pred pont\n",
      "(1045.03796387, 132.24189758, True, False, 0.87852591)\n",
      "pred pont\n",
      "(1092.58068848, 83.22297668, True, False, 0.84897828)\n",
      "pred pont\n",
      "(983.27856445, 155.64892578, True, False, 0.67616218)\n",
      "pred pont\n",
      "(1091.81689453, 203.79666138, True, False, 0.48666519)\n",
      "pred pont\n",
      "(1897.29187012, 166.09431458, True, False, 1.03273058)\n",
      "pred pont\n",
      "(1692.49060059, 274.44723511, True, False, 0.85622638)\n",
      "pred pont\n",
      "(1764.33740234, 228.28337097, True, False, 0.74351239)\n",
      "pred pont\n",
      "(1678.96594238, 251.08305359, True, False, 0.71588796)\n",
      "pred pont\n",
      "(1703.94592285, 310.7953186, True, False, 0.71778202)\n",
      "pred pont\n",
      "(466.27154541, 360.96994019, True, False, 0.9503513)\n",
      "pred pont\n",
      "(504.36981201, 636.62908936, True, False, 0.96202511)\n",
      "pred pont\n",
      "(503.4934082, 563.61651611, True, False, 0.9428094)\n",
      "pred pont\n",
      "(444.13616943, 683.63024902, True, False, 0.89084136)\n",
      "pred pont\n",
      "(551.37341309, 684.74804688, True, False, 0.80152369)\n",
      "pred pont\n",
      "(658.64782715, 466.96612549, True, False, 0.97841632)\n",
      "pred pont\n",
      "(732.51794434, 707.2713623, True, False, 0.99533498)\n",
      "pred pont\n",
      "(720.39172363, 636.184021, True, False, 0.84978396)\n",
      "pred pont\n",
      "(706.77844238, 769.98193359, True, False, 0.49501327)\n",
      "pred pont\n",
      "(697.73254395, 768.80291748, True, False, 0.49293482)\n",
      "pred pont\n",
      "(1837.99951172, 673.71636963, True, False, 0.92720795)\n",
      "pred pont\n",
      "(1825.0057373, 421.22686768, True, False, 0.84501582)\n",
      "pred pont\n",
      "(1838.08422852, 493.21914673, True, False, 0.76386642)\n",
      "pred pont\n",
      "(1850.03991699, 360.28323364, True, False, 0.85873741)\n",
      "pred pont\n",
      "(1751.76306152, 397.38766479, True, False, 0.78122711)\n",
      "pred pont\n",
      "(1057.16003418, 709.6550293, True, False, 1.08947527)\n",
      "pred pont\n",
      "(803.76287842, 685.21472168, True, False, 0.9347803)\n",
      "pred pont\n",
      "(887.99530029, 696.41802979, True, False, 0.7206412)\n",
      "pred pont\n",
      "(780.34490967, 671.59881592, True, False, 0.6407342)\n",
      "pred pont\n",
      "(780.00439453, 721.25823975, True, False, 0.66682351)\n",
      "pred pont\n",
      "(515.72668457, 815.96588135, True, False, 1.0001688)\n",
      "pred pont\n",
      "(263.13128662, 769.00744629, True, False, 0.73969549)\n",
      "pred pont\n",
      "(336.67959595, 768.21911621, True, False, 0.61431044)\n",
      "pred pont\n",
      "(230.16375732, 745.63909912, True, False, 0.46258694)\n",
      "pred pont\n",
      "(238.50854492, 793.25769043, True, False, 0.44882065)\n",
      "pred pont\n",
      "(1619.67810059, 840.65380859, True, False, 0.90016848)\n",
      "pred pont\n",
      "(1583.80615234, 566.34539795, True, False, 0.80361623)\n",
      "pred pont\n",
      "(1594.98950195, 648.90093994, True, False, 0.7197156)\n",
      "pred pont\n",
      "(1621.58642578, 527.10266113, True, False, 0.84595823)\n",
      "pred pont\n",
      "(1512.77844238, 564.44494629, True, False, 0.64048457)\n",
      "pred pont\n",
      "(251.9622345, 876.81634521, True, False, 0.6950627)\n",
      "pred pont\n",
      "(468.4130249, 961.55310059, True, False, 1.04257572)\n",
      "pred pont\n",
      "(396.51498413, 934.86993408, True, False, 0.86065006)\n",
      "pred pont\n",
      "(470.74298096, 1032.36584473, True, False, 0.84852362)\n",
      "pred pont\n",
      "(541.56640625, 949.6663208, True, False, 1.05497622)\n",
      "pred pont\n",
      "(1380.3717041, 1030.68945312, True, False, 0.92940229)\n",
      "pred pont\n",
      "(1367.92028809, 778.59143066, True, False, 0.83553284)\n",
      "pred pont\n",
      "(1369.08789062, 850.96697998, True, False, 0.75748181)\n",
      "pred pont\n",
      "(1393.87646484, 732.75170898, True, False, 0.62226272)\n",
      "pred pont\n",
      "(1308.64611816, 732.13903809, True, False, 0.90488219)\n",
      "pred pont\n",
      "(1368.91882324, 1114.56542969, True, False, 0.88506705)\n",
      "pred pont\n",
      "(1201.03161621, 1234.69299316, True, False, 0.30130002)\n",
      "pred pont\n",
      "(1272.99523926, 1200.12719727, True, False, 0.71042132)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1295.12304688, 59.52748108, True, False, 0.95210123)\n",
      "pred pont\n",
      "(1355.40673828, 17.83180809, True, False, 0.75869125)\n",
      "pred pont\n",
      "(1238.28491211, 48.06990814, True, False, 0.49068427)\n",
      "pred pont\n",
      "(1295.95214844, 131.30357361, True, False, 0.40969831)\n",
      "pred pont\n",
      "(1033.84875488, 16.50034904, True, False, 0.59236985)\n",
      "pred pont\n",
      "(1103.46691895, 239.34973145, True, False, 0.89687437)\n",
      "pred pont\n",
      "(1091.17456055, 144.44404602, True, False, 0.69447863)\n",
      "pred pont\n",
      "(1068.09667969, 253.06506348, True, False, 0.64865345)\n",
      "pred pont\n",
      "(1129.31384277, 252.48027039, True, False, 0.69576865)\n",
      "pred pont\n",
      "(1620.24084473, 179.61051941, True, False, 0.93385023)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1644.2791748, 34.80020142, True, False, 0.60818517)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1151.90820312, 275.97207642, True, False, 1.02059531)\n",
      "pred pont\n",
      "(1008.68475342, 442.48748779, True, False, 0.36494413)\n",
      "pred pont\n",
      "(1057.83776855, 385.39190674, True, False, 0.58659172)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(803.14312744, 349.56170654, True, False, 0.91179854)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred pont\n",
      "(863.21520996, 97.18156433, True, False, 0.89543957)\n",
      "pred pont\n",
      "(839.17211914, 181.11459351, True, False, 0.78026873)\n",
      "pred pont\n",
      "(912.56433105, 71.62341309, True, False, 0.6962781)\n",
      "pred pont\n",
      "(828.99963379, 36.62669754, True, False, 0.78763068)\n",
      "pred pont\n",
      "(2246.33081055, 406.93273926, True, False, 0.85990256)\n",
      "pred pont\n",
      "(2244.42871094, 156.05532837, True, False, 0.87371856)\n",
      "pred pont\n",
      "(2222.50390625, 240.32476807, True, False, 0.82245636)\n",
      "pred pont\n",
      "(2292.43554688, 143.6587677, True, False, 0.86488658)\n",
      "pred pont\n",
      "(2233.27758789, 121.86734009, True, False, 0.69703102)\n",
      "pred pont\n",
      "(612.67401123, 420.22058105, True, False, 1.02941787)\n",
      "pred pont\n",
      "(540.75854492, 648.43688965, True, False, 1.07785642)\n",
      "pred pont\n",
      "(562.94842529, 576.16027832, True, False, 0.9083038)\n",
      "pred pont\n",
      "(492.03219604, 670.48327637, True, False, 0.85502213)\n",
      "pred pont\n",
      "(576.13830566, 672.83575439, True, False, 0.5091942)\n",
      "pred pont\n",
      "(419.5904541, 516.00726318, True, False, 0.96720105)\n",
      "pred pont\n",
      "(432.22232056, 804.50958252, True, False, 1.04557884)\n",
      "pred pont\n",
      "(431.1918335, 731.44494629, True, False, 0.88030976)\n",
      "pred pont\n",
      "(382.39053345, 852.48602295, True, False, 0.92956555)\n",
      "pred pont\n",
      "(492.30764771, 841.30456543, True, False, 0.94982791)\n",
      "pred pont\n",
      "(948.1751709, 623.44482422, True, False, 0.98262089)\n",
      "pred pont\n",
      "(947.11895752, 384.24337769, True, False, 0.78136939)\n",
      "pred pont\n",
      "(937.17938232, 479.62796021, True, False, 0.70811242)\n",
      "pred pont\n",
      "(973.18670654, 371.67828369, True, False, 0.65878457)\n",
      "pred pont\n",
      "(889.45318604, 360.94543457, True, False, 0.55186659)\n",
      "pred pont\n",
      "(672.22692871, 791.0927124, True, False, 1.02360034)\n",
      "pred pont\n",
      "(877.38079834, 950.12078857, True, False, 0.98200923)\n",
      "pred pont\n",
      "(828.07415771, 900.56481934, True, False, 0.82341057)\n",
      "pred pont\n",
      "(865.23297119, 1031.26318359, True, False, 0.74228436)\n",
      "pred pont\n",
      "(948.72662354, 950.25817871, True, False, 0.86476725)\n",
      "pred pont\n",
      "(1585.69970703, 791.24969482, True, False, 0.93246937)\n",
      "pred pont\n",
      "(1633.16162109, 504.02148438, True, False, 0.84405881)\n",
      "pred pont\n",
      "(1619.38891602, 577.49121094, True, False, 0.72813225)\n",
      "pred pont\n",
      "(1680.57836914, 493.02841187, True, False, 0.82164103)\n",
      "pred pont\n",
      "(1609.01879883, 480.26654053, True, False, 0.66384012)\n",
      "pred pont\n",
      "(1823.12805176, 959.83032227, True, False, 0.87599391)\n",
      "pred pont\n",
      "(1873.26049805, 707.78808594, True, False, 0.83889848)\n",
      "pred pont\n",
      "(1860.97595215, 780.6161499, True, False, 0.82833743)\n",
      "pred pont\n",
      "(1932.55712891, 670.44421387, True, False, 0.93765944)\n",
      "pred pont\n",
      "(1838.16650391, 635.33251953, True, False, 0.81426919)\n",
      "pred pont\n",
      "(1128.84912109, 1235.13427734, True, False, 0.65487874)\n",
      "pred pont\n",
      "(1115.64880371, 996.48486328, True, False, 0.83330125)\n",
      "pred pont\n",
      "(1116.80480957, 1069.61108398, True, False, 0.81308806)\n",
      "pred pont\n",
      "(1174.31872559, 937.63763428, True, False, 0.84736192)\n",
      "pred pont\n",
      "(1067.66430664, 938.15032959, True, False, 0.76848745)\n",
      "pred pont\n",
      "(1619.4395752, 179.60107422, True, False, 0.93907076)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1622.59558105, 24.56460381, True, False, 0.59163904)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1044.2409668, 275.30691528, True, False, 0.96940178)\n",
      "pred pont\n",
      "(875.57067871, 456.14877319, True, False, 0.78168815)\n",
      "pred pont\n",
      "(924.74719238, 372.50912476, True, False, 0.76492071)\n",
      "pred pont\n",
      "(816.22705078, 478.72039795, True, False, 0.74958676)\n",
      "pred pont\n",
      "(899.40612793, 480.11190796, True, False, 0.65513635)\n",
      "pred pont\n",
      "(2256.70507812, 406.83087158, True, False, 0.82781029)\n",
      "pred pont\n",
      "(2244.10131836, 145.41264343, True, False, 0.92951649)\n",
      "pred pont\n",
      "(2222.34814453, 238.63449097, True, False, 0.82199717)\n",
      "pred pont\n",
      "(2292.05224609, 132.29255676, True, False, 0.87515885)\n",
      "pred pont\n",
      "(2231.91381836, 120.05828094, True, False, 0.72747397)\n",
      "pred pont\n",
      "(912.82501221, 480.92199707, True, False, 0.69947869)\n",
      "pred pont\n",
      "(947.77410889, 707.8081665, True, False, 0.92155367)\n",
      "pred pont\n",
      "(936.77612305, 625.63031006, True, False, 0.77241862)\n",
      "pred pont\n",
      "(901.07550049, 744.78063965, True, False, 0.79647291)\n",
      "pred pont\n",
      "(1009.01428223, 733.29187012, True, False, 0.83407068)\n",
      "pred pont\n",
      "(661.3447876, 575.3203125, True, False, 0.89946675)\n",
      "pred pont\n",
      "(623.22131348, 324.74499512, True, False, 0.85334355)\n",
      "pred pont\n",
      "(634.43170166, 396.93148804, True, False, 0.79493064)\n",
      "pred pont\n",
      "(670.4161377, 275.98638916, True, False, 0.85701114)\n",
      "pred pont\n",
      "(542.24847412, 299.65234375, True, False, 0.84415054)\n",
      "pred pont\n",
      "(1595.75500488, 780.4644165, True, False, 0.87788415)\n",
      "pred pont\n",
      "(1621.77294922, 502.50991821, True, False, 0.80927122)\n",
      "pred pont\n",
      "(1619.01647949, 576.93243408, True, False, 0.72052598)\n",
      "pred pont\n",
      "(1667.79956055, 491.53811646, True, False, 0.6923849)\n",
      "pred pont\n",
      "(1596.62561035, 469.42990112, True, False, 0.67741233)\n",
      "pred pont\n",
      "(1644.43334961, 997.46289062, True, False, 0.75050884)\n",
      "pred pont\n",
      "(1715.98217773, 755.51367188, True, False, 0.83556563)\n",
      "pred pont\n",
      "(1703.52404785, 829.42480469, True, False, 0.76715833)\n",
      "pred pont\n",
      "(1753.67407227, 696.96685791, True, False, 0.97531581)\n",
      "pred pont\n",
      "(1670.6072998, 731.15679932, True, False, 0.52256924)\n",
      "pred pont\n",
      "(1056.03076172, 1164.7734375, True, False, 0.68486595)\n",
      "pred pont\n",
      "(1212.18811035, 984.27770996, True, False, 0.85088938)\n",
      "pred pont\n",
      "(1165.62780762, 1043.66784668, True, False, 0.72450864)\n",
      "pred pont\n",
      "(1283.94946289, 971.76348877, True, False, 0.9350844)\n",
      "pred pont\n",
      "(1188.31933594, 900.76757812, True, False, 0.78956032)\n",
      "pred pont\n",
      "(418.06280518, 1235.99682617, True, False, 0.20992142)\n",
      "pred pont\n",
      "(455.77957153, 1117.89428711, True, False, 0.73488432)\n",
      "pred pont\n",
      "(430.83764648, 1188.41784668, True, False, 0.46974424)\n",
      "pred pont\n",
      "(505.31921387, 1090.93579102, True, False, 0.82959414)\n",
      "pred pont\n",
      "(442.73480225, 1079.69348145, True, False, 0.58390272)\n",
      "pred pont\n",
      "(1501.14794922, 1261.2043457, True, False, 0.54038793)\n",
      "pred pont\n",
      "(1511.84033203, 1079.15454102, True, False, 0.81037867)\n",
      "pred pont\n",
      "(1500.31396484, 1153.15966797, True, False, 0.83587158)\n",
      "pred pont\n",
      "(1464.95446777, 1020.23846436, True, False, 0.78642386)\n",
      "pred pont\n",
      "(1560.90759277, 1044.3001709, True, False, 0.76454145)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1247.98291016, 1128.35510254, True, False, 0.203282)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1294.77197266, 1105.34802246, True, False, 0.29376864)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1619.3067627, 179.03213501, True, False, 0.93030465)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1631.93115234, 24.92630959, True, False, 0.5771569)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(961.97607422, 263.41952515, True, False, 0.97331041)\n",
      "pred pont\n",
      "(829.07806396, 467.30419922, True, False, 0.57356882)\n",
      "pred pont\n",
      "(864.26104736, 384.36001587, True, False, 0.67176044)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(830.62615967, 481.17007446, True, False, 0.53310508)\n",
      "pred pont\n",
      "(2257.20117188, 370.90859985, True, False, 0.82589233)\n",
      "pred pont\n",
      "(2245.875, 120.17869568, True, False, 0.86914402)\n",
      "pred pont\n",
      "(2231.89526367, 205.12397766, True, False, 0.83542812)\n",
      "pred pont\n",
      "(2293.15625, 108.38108063, True, False, 0.80379128)\n",
      "pred pont\n",
      "(2244.44042969, 96.17197418, True, False, 0.70873129)\n",
      "pred pont\n",
      "(635.10668945, 551.46905518, True, False, 0.86419708)\n",
      "pred pont\n",
      "(601.65490723, 323.97741699, True, False, 0.8599363)\n",
      "pred pont\n",
      "(611.74591064, 396.93621826, True, False, 0.80339825)\n",
      "pred pont\n",
      "(649.27264404, 287.81280518, True, False, 0.61236393)\n",
      "pred pont\n",
      "(564.75, 253.97583008, True, False, 0.91475713)\n",
      "pred pont\n",
      "(1284.42980957, 660.91040039, True, False, 0.91273117)\n",
      "pred pont\n",
      "(1249.88708496, 408.51391602, True, False, 0.81497616)\n",
      "pred pont\n",
      "(1271.78100586, 481.37295532, True, False, 0.79268086)\n",
      "pred pont\n",
      "(1283.96166992, 335.91183472, True, False, 0.91974348)\n",
      "pred pont\n",
      "(1186.51184082, 384.56185913, True, False, 0.88219726)\n",
      "pred pont\n",
      "(695.71472168, 731.31964111, True, False, 0.80799907)\n",
      "pred pont\n",
      "(743.78039551, 515.99951172, True, False, 0.80821192)\n",
      "pred pont\n",
      "(719.03997803, 599.94909668, True, False, 0.77556133)\n",
      "pred pont\n",
      "(782.25946045, 492.70401001, True, False, 0.86742318)\n",
      "pred pont\n",
      "(709.33300781, 492.22717285, True, False, 0.74245572)\n",
      "pred pont\n",
      "(1823.7109375, 769.61639404, True, False, 0.78895855)\n",
      "pred pont\n",
      "(1970.6295166, 564.03149414, True, False, 0.8602314)\n",
      "pred pont\n",
      "(1933.64697266, 623.76879883, True, False, 0.80984432)\n",
      "pred pont\n",
      "(2051.77954102, 562.54803467, True, False, 0.89430994)\n",
      "pred pont\n",
      "(1981.17321777, 481.15945435, True, False, 0.87022871)\n",
      "pred pont\n",
      "(1585.49633789, 781.26416016, True, False, 0.96998101)\n",
      "pred pont\n",
      "(1621.25549316, 492.96566772, True, False, 0.83478779)\n",
      "pred pont\n",
      "(1619.13391113, 576.05743408, True, False, 0.72674906)\n",
      "pred pont\n",
      "(1657.42651367, 480.32507324, True, False, 0.71568674)\n",
      "pred pont\n",
      "(1584.62585449, 469.79660034, True, False, 0.6881339)\n",
      "pred pont\n",
      "(1094.35351562, 889.76263428, True, False, 0.91729927)\n",
      "pred pont\n",
      "(1056.06188965, 647.17999268, True, False, 0.79538894)\n",
      "pred pont\n",
      "(1068.14074707, 710.35668945, True, False, 0.80759829)\n",
      "pred pont\n",
      "(1093.10778809, 577.37054443, True, False, 0.90677291)\n",
      "pred pont\n",
      "(997.59332275, 612.65338135, True, False, 0.72007042)\n",
      "pred pont\n",
      "(899.05529785, 912.44610596, True, False, 0.88890672)\n",
      "pred pont\n",
      "(817.32629395, 686.01531982, True, False, 0.65325528)\n",
      "pred pont\n",
      "(841.19610596, 781.12414551, True, False, 0.74119914)\n",
      "pred pont\n",
      "(839.73736572, 673.05712891, True, False, 0.4397063)\n",
      "pred pont\n",
      "(780.93060303, 660.10455322, True, False, 0.62671149)\n",
      "pred pont\n",
      "(1704.27770996, 1212.42504883, True, False, 0.68120575)\n",
      "pred pont\n",
      "(1715.60144043, 983.17901611, True, False, 0.77607608)\n",
      "pred pont\n",
      "(1705.79040527, 1056.26989746, True, False, 0.78077948)\n",
      "pred pont\n",
      "(1762.8046875, 926.02624512, True, False, 0.60465103)\n",
      "pred pont\n",
      "(1668.06103516, 923.24395752, True, False, 0.48812914)\n",
      "pred pont\n",
      "(611.41577148, 1238.82824707, True, False, 0.31704396)\n",
      "pred pont\n",
      "(599.26055908, 1115.98388672, True, False, 0.73418629)\n",
      "pred pont\n",
      "(601.10583496, 1188.28881836, True, False, 0.57536995)\n",
      "pred pont\n",
      "(553.36035156, 1081.34533691, True, False, 0.56440455)\n",
      "pred pont\n",
      "(622.55352783, 1069.65185547, True, False, 0.60099691)\n",
      "pred pont\n",
      "(1452.67956543, 1247.57202148, True, False, 0.64950091)\n",
      "pred pont\n",
      "(1487.7644043, 997.29455566, True, False, 0.8284865)\n",
      "pred pont\n",
      "(1465.85534668, 1078.84448242, True, False, 0.83301306)\n",
      "pred pont\n",
      "(1536.18579102, 961.38275146, True, False, 0.40312004)\n",
      "pred pont\n",
      "(1466.39318848, 936.4552002, True, False, 0.49336964)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(912.34020996, 696.80273438, True, False, 0.69610065)\n",
      "pred pont\n",
      "(899.25268555, 611.64727783, True, False, 0.41752651)\n",
      "pred pont\n",
      "(889.45227051, 718.02001953, True, False, 0.35658702)\n",
      "pred pont\n",
      "(948.14831543, 718.84130859, True, False, 0.43151474)\n",
      "pred pont\n",
      "(1283.40441895, 10.41872692, True, False, 0.48559535)\n",
      "pred pont\n",
      "(1117.12512207, 179.15036011, True, False, 0.86852872)\n",
      "pred pont\n",
      "(1162.9095459, 108.29863739, True, False, 0.80807739)\n",
      "pred pont\n",
      "(1045.58483887, 194.91952515, True, False, 0.44377542)\n",
      "pred pont\n",
      "(1127.59997559, 217.72399902, True, False, 0.25428277)\n",
      "pred pont\n",
      "(468.47888184, 107.15856934, True, False, 0.92575103)\n",
      "pred pont\n",
      "(360.72525024, 325.18014526, True, False, 0.96054369)\n",
      "pred pont\n",
      "(407.47369385, 264.87246704, True, False, 0.86492586)\n",
      "pred pont\n",
      "(288.92190552, 336.05084229, True, False, 0.84471518)\n",
      "pred pont\n",
      "(361.35870361, 395.20172119, True, False, 0.63377047)\n",
      "pred pont\n",
      "(1597.49572754, 180.42752075, True, False, 0.91788715)\n",
      "pred pont\n",
      "(1669.36560059, 9.42984295, True, False, 0.27860218)\n",
      "pred pont\n",
      "(1633.41430664, 46.56826401, True, False, 0.62177777)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1058.12390137, 181.18634033, True, False, 0.38331413)\n",
      "pred pont\n",
      "(2016.35192871, 228.96444702, True, False, 0.81272918)\n",
      "pred pont\n",
      "(2053.45800781, 38.08229446, True, False, 0.32721579)\n",
      "pred pont\n",
      "(2040.0078125, 108.66179657, True, False, 0.67620069)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(744.36230469, 314.00500488, True, False, 0.80151796)\n",
      "pred pont\n",
      "(685.75518799, 83.34269714, True, False, 0.82605785)\n",
      "pred pont\n",
      "(697.33380127, 167.47468567, True, False, 0.75102055)\n",
      "pred pont\n",
      "(721.58148193, 36.37186813, True, False, 0.73112792)\n",
      "pred pont\n",
      "(648.48059082, 60.51617432, True, False, 0.69792038)\n",
      "pred pont\n",
      "(877.50238037, 310.70083618, True, False, 0.84044307)\n",
      "pred pont\n",
      "(973.06542969, 529.10064697, True, False, 0.81002223)\n",
      "pred pont\n",
      "(924.77593994, 444.26516724, True, False, 0.66339481)\n",
      "pred pont\n",
      "(949.99847412, 565.10400391, True, False, 0.53599566)\n",
      "pred pont\n",
      "(1020.50189209, 528.86602783, True, False, 0.67437255)\n",
      "pred pont\n",
      "(2256.8347168, 396.41516113, True, False, 0.86017686)\n",
      "pred pont\n",
      "(2233.97558594, 153.91880798, True, False, 0.88721305)\n",
      "pred pont\n",
      "(2220.69384766, 238.52497864, True, False, 0.82821018)\n",
      "pred pont\n",
      "(2281.44897461, 132.5199585, True, False, 0.83333898)\n",
      "pred pont\n",
      "(2222.38378906, 120.65693665, True, False, 0.74493253)\n",
      "pred pont\n",
      "(587.51489258, 528.29486084, True, False, 1.01855218)\n",
      "pred pont\n",
      "(553.62255859, 288.58874512, True, False, 0.84449226)\n",
      "pred pont\n",
      "(563.40557861, 371.95767212, True, False, 0.84920156)\n",
      "pred pont\n",
      "(577.49304199, 240.69239807, True, False, 0.66967326)\n",
      "pred pont\n",
      "(527.72045898, 252.6318512, True, False, 0.58384562)\n",
      "pred pont\n",
      "(864.3727417, 527.3649292, True, False, 0.7417509)\n",
      "pred pont\n",
      "(972.51812744, 768.93487549, True, False, 0.98742932)\n",
      "pred pont\n",
      "(937.56262207, 707.64025879, True, False, 0.8526352)\n",
      "pred pont\n",
      "(925.0604248, 828.95135498, True, False, 0.73786438)\n",
      "pred pont\n",
      "(1022.32421875, 815.65527344, True, False, 1.00776505)\n",
      "pred pont\n",
      "(1139.09594727, 649.32006836, True, False, 0.88107282)\n",
      "pred pont\n",
      "(1080.16027832, 397.42245483, True, False, 0.81571198)\n",
      "pred pont\n",
      "(1094.26098633, 469.31072998, True, False, 0.77788502)\n",
      "pred pont\n",
      "(1116.54199219, 336.63122559, True, False, 0.78737313)\n",
      "pred pont\n",
      "(1019.80609131, 373.61901855, True, False, 0.80892181)\n",
      "pred pont\n",
      "(1595.09240723, 781.26837158, True, False, 0.84582597)\n",
      "pred pont\n",
      "(1609.39526367, 492.7008667, True, False, 0.8756395)\n",
      "pred pont\n",
      "(1609.58361816, 577.18054199, True, False, 0.74945593)\n",
      "pred pont\n",
      "(1644.50244141, 469.29592896, True, False, 0.68533075)\n",
      "pred pont\n",
      "(1571.22314453, 468.49908447, True, False, 0.76727748)\n",
      "pred pont\n",
      "(1489.47680664, 865.35845947, True, False, 0.825257)\n",
      "pred pont\n",
      "(1381.00292969, 613.34014893, True, False, 0.83125013)\n",
      "pred pont\n",
      "(1404.60961914, 696.16107178, True, False, 0.77852172)\n",
      "pred pont\n",
      "(1405.55041504, 575.69195557, True, False, 0.47143045)\n",
      "pred pont\n",
      "(1321.8371582, 575.19421387, True, False, 0.84308857)\n",
      "pred pont\n",
      "(1836.85217285, 934.79840088, True, False, 0.92915827)\n",
      "pred pont\n",
      "(1860.15490723, 660.33898926, True, False, 0.86888689)\n",
      "pred pont\n",
      "(1859.74719238, 733.98120117, True, False, 0.81683326)\n",
      "pred pont\n",
      "(1909.16796875, 611.57830811, True, False, 0.91039044)\n",
      "pred pont\n",
      "(1812.65637207, 612.02539062, True, False, 0.7149514)\n",
      "pred pont\n",
      "(574.18664551, 949.14752197, True, False, 0.94822615)\n",
      "pred pont\n",
      "(661.0447998, 1177.67700195, True, False, 0.93029511)\n",
      "pred pont\n",
      "(623.75714111, 1115.93457031, True, False, 0.80174744)\n",
      "pred pont\n",
      "(647.41882324, 1214.13037109, True, False, 0.56305885)\n",
      "pred pont\n",
      "(708.60400391, 1189.7220459, True, False, 0.64266801)\n",
      "pred pont\n",
      "(1981.3314209, 1259.62219238, True, False, 0.64937419)\n",
      "pred pont\n",
      "(1884.64379883, 1093.69873047, True, False, 0.79199368)\n",
      "pred pont\n",
      "(1909.91040039, 1164.7310791, True, False, 0.87076604)\n",
      "pred pont\n",
      "(1897.56518555, 1043.80407715, True, False, 0.46268377)\n",
      "pred pont\n",
      "(1897.35888672, 1044.69592285, True, False, 0.54940701)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1357.37390137, 71.19338989, True, False, 0.9360742)\n",
      "pred pont\n",
      "(1415.7166748, 16.70101357, True, False, 0.71402729)\n",
      "pred pont\n",
      "(1356.24829102, 108.81724548, True, False, 0.5285306)\n",
      "pred pont\n",
      "(1320.3157959, 96.02886963, True, False, 0.70077276)\n",
      "pred pont\n",
      "(648.70178223, 17.22332001, True, False, 0.58131164)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(816.58856201, 82.45832062, True, False, 0.54108363)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(998.0958252, 108.59940338, True, False, 0.87188542)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(998.64691162, 16.18332291, True, False, 0.39797962)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(661.80548096, 120.07381439, True, False, 0.83882505)\n",
      "pred pont\n",
      "(695.16082764, 419.65817261, True, False, 0.94402319)\n",
      "pred pont\n",
      "(684.89569092, 324.41049194, True, False, 0.81110263)\n",
      "pred pont\n",
      "(658.65423584, 466.94696045, True, False, 0.66194177)\n",
      "pred pont\n",
      "(743.64599609, 444.90304565, True, False, 0.695544)\n",
      "pred pont\n",
      "(1153.15881348, 240.87902832, True, False, 0.97694045)\n",
      "pred pont\n",
      "(1235.09143066, 15.55556393, True, False, 0.83161312)\n",
      "pred pont\n",
      "(1199.29541016, 85.40963745, True, False, 0.78218323)\n",
      "pred pont\n",
      "(1272.2611084, 17.28031349, True, False, 0.63358766)\n",
      "pred pont\n",
      "(1234.8215332, 8.92847157, True, False, 0.43930149)\n",
      "pred pont\n",
      "(550.92907715, 312.228302, True, False, 0.91647762)\n",
      "pred pont\n",
      "(561.86187744, 598.56182861, True, False, 0.92535526)\n",
      "pred pont\n",
      "(540.83740234, 503.89614868, True, False, 0.73544812)\n",
      "pred pont\n",
      "(503.53970337, 624.77825928, True, False, 0.70896757)\n",
      "pred pont\n",
      "(601.30786133, 634.74810791, True, False, 1.12878788)\n",
      "pred pont\n",
      "(853.39379883, 382.38729858, True, False, 0.98069972)\n",
      "pred pont\n",
      "(779.74188232, 625.60437012, True, False, 0.90720457)\n",
      "pred pont\n",
      "(803.15618896, 553.10418701, True, False, 0.86185712)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(804.57800293, 647.10949707, True, False, 0.60729617)\n",
      "pred pont\n",
      "(2258.27124023, 419.22344971, True, False, 0.80106223)\n",
      "pred pont\n",
      "(2232.68212891, 193.21443176, True, False, 0.87443525)\n",
      "pred pont\n",
      "(2218.81762695, 277.43670654, True, False, 0.82997972)\n",
      "pred pont\n",
      "(2291.79248047, 191.54129028, True, False, 0.84789902)\n",
      "pred pont\n",
      "(2244.80273438, 167.27096558, True, False, 0.74288547)\n",
      "pred pont\n",
      "(1910.50256348, 514.93237305, True, False, 0.92521471)\n",
      "pred pont\n",
      "(1957.21716309, 781.80700684, True, False, 0.96069157)\n",
      "pred pont\n",
      "(1957.67687988, 719.70892334, True, False, 0.82456231)\n",
      "pred pont\n",
      "(1907.04833984, 839.39715576, True, False, 0.95486718)\n",
      "pred pont\n",
      "(2015.57470703, 841.7590332, True, False, 0.95030594)\n",
      "pred pont\n",
      "(300.88684082, 721.01141357, True, False, 0.87352127)\n",
      "pred pont\n",
      "(275.96356201, 936.94567871, True, False, 0.83288163)\n",
      "pred pont\n",
      "(301.30090332, 851.86480713, True, False, 0.72682869)\n",
      "pred pont\n",
      "(228.62876892, 947.78955078, True, False, 0.73148829)\n",
      "pred pont\n",
      "(276.53042603, 971.55767822, True, False, 0.63120627)\n",
      "pred pont\n",
      "(1608.41430664, 768.77929688, True, False, 0.94222653)\n",
      "pred pont\n",
      "(1643.52587891, 480.94021606, True, False, 0.86264175)\n",
      "pred pont\n",
      "(1622.22924805, 563.66052246, True, False, 0.731682)\n",
      "pred pont\n",
      "(1680.96606445, 457.89279175, True, False, 0.72965938)\n",
      "pred pont\n",
      "(1609.17199707, 455.77279663, True, False, 0.69664127)\n",
      "pred pont\n",
      "(635.67102051, 947.34460449, True, False, 0.9321807)\n",
      "pred pont\n",
      "(648.83642578, 683.84490967, True, False, 0.7996822)\n",
      "pred pont\n",
      "(636.6519165, 755.36169434, True, False, 0.84338987)\n",
      "pred pont\n",
      "(697.36376953, 623.99884033, True, False, 1.0215466)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1955.10107422, 1070.24060059, True, False, 0.75040799)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1993.83874512, 1176.85668945, True, False, 0.52844143)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1523.15356445, 1127.85461426, True, False, 0.87051362)\n",
      "pred pont\n",
      "(1764.11669922, 1080.80859375, True, False, 0.93153208)\n",
      "pred pont\n",
      "(1691.99816895, 1093.91223145, True, False, 0.76872933)\n",
      "pred pont\n",
      "(1777.53601074, 1116.51123047, True, False, 0.75000554)\n",
      "pred pont\n",
      "(1764.79260254, 1020.04577637, True, False, 0.75751233)\n",
      "pred pont\n",
      "(923.24346924, 1166.38391113, True, False, 0.75951511)\n",
      "pred pont\n",
      "(1034.22070312, 983.66973877, True, False, 0.85583752)\n",
      "pred pont\n",
      "(1007.67114258, 1045.43981934, True, False, 0.81175548)\n",
      "pred pont\n",
      "(1106.5546875, 958.64556885, True, False, 0.95807105)\n",
      "pred pont\n",
      "(1009.15655518, 901.36437988, True, False, 0.88769877)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1416.13659668, 72.16848755, True, False, 0.96508837)\n",
      "pred pont\n",
      "(1357.33728027, 17.37618256, True, False, 0.73435915)\n",
      "pred pont\n",
      "(1393.4934082, 121.41503906, True, False, 0.64179295)\n",
      "pred pont\n",
      "(1477.9387207, 60.95585632, True, False, 0.72299421)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(851.78723145, 155.46920776, True, False, 0.77732295)\n",
      "pred pont\n",
      "(840.02276611, 60.62716293, True, False, 0.54431665)\n",
      "pred pont\n",
      "(865.12908936, 227.08396912, True, False, 0.58343869)\n",
      "pred pont\n",
      "(865.3996582, 215.89730835, True, False, 0.47021693)\n",
      "pred pont\n",
      "(695.45422363, 36.78704071, True, False, 0.8770234)\n",
      "pred pont\n",
      "(625.66033936, 277.18438721, True, False, 0.81947935)\n",
      "pred pont\n",
      "(647.51977539, 205.05148315, True, False, 0.81838763)\n",
      "pred pont\n",
      "(612.1831665, 313.35018921, True, False, 0.42629018)\n",
      "pred pont\n",
      "(649.22265625, 326.0093689, True, False, 0.69522142)\n",
      "pred pont\n",
      "(1127.39648438, 73.48338318, True, False, 0.72383898)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1094.22497559, 16.80922508, True, False, 0.43202385)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(504.53726196, 133.71737671, True, False, 0.79880148)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(515.50854492, 22.23918152, True, False, 0.45977965)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1381.63818359, 203.02548218, True, False, 0.99839646)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1283.77624512, 299.50915527, True, False, 0.70554149)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1031.66149902, 227.55418396, True, False, 0.53430575)\n",
      "pred pont\n",
      "(996.83996582, 23.88106728, True, False, 0.61959261)\n",
      "pred pont\n",
      "(1021.03820801, 155.14849854, True, False, 0.55171919)\n",
      "pred pont\n",
      "(1031.73132324, 18.15127754, True, False, 0.55698597)\n",
      "pred pont\n",
      "(997.10656738, 10.40504169, True, False, 0.43646455)\n",
      "pred pont\n",
      "(299.26983643, 382.60662842, True, False, 0.93050879)\n",
      "pred pont\n",
      "(252.49360657, 167.71131897, True, False, 0.80237514)\n",
      "pred pont\n",
      "(264.84439087, 251.58808899, True, False, 0.80301178)\n",
      "pred pont\n",
      "(287.16244507, 133.89543152, True, False, 0.83655512)\n",
      "pred pont\n",
      "(205.24711609, 144.03041077, True, False, 0.8742739)\n",
      "pred pont\n",
      "(2246.66162109, 383.4029541, True, False, 0.91313249)\n",
      "pred pont\n",
      "(2245.82543945, 143.48074341, True, False, 0.77411032)\n",
      "pred pont\n",
      "(2222.10791016, 229.06439209, True, False, 0.82451069)\n",
      "pred pont\n",
      "(2292.99951172, 132.42504883, True, False, 0.70187825)\n",
      "pred pont\n",
      "(2245.90991211, 110.39031982, True, False, 0.6277613)\n",
      "pred pont\n",
      "(563.3973999, 455.396698, True, False, 0.91362804)\n",
      "pred pont\n",
      "(624.55859375, 720.10321045, True, False, 0.89978683)\n",
      "pred pont\n",
      "(623.80853271, 636.59100342, True, False, 0.81496727)\n",
      "pred pont\n",
      "(586.5916748, 756.47845459, True, False, 0.70303869)\n",
      "pred pont\n",
      "(671.17138672, 766.65704346, True, False, 0.6775744)\n",
      "pred pont\n",
      "(756.73376465, 504.57946777, True, False, 0.87435788)\n",
      "pred pont\n",
      "(816.92810059, 781.76055908, True, False, 0.95660347)\n",
      "pred pont\n",
      "(803.72424316, 708.94787598, True, False, 0.86453205)\n",
      "pred pont\n",
      "(767.67016602, 829.24462891, True, False, 0.81380403)\n",
      "pred pont\n",
      "(876.25439453, 829.519104, True, False, 0.93392676)\n",
      "pred pont\n",
      "(419.92785645, 575.58117676, True, False, 0.83855349)\n",
      "pred pont\n",
      "(408.62728882, 334.5112915, True, False, 0.82347697)\n",
      "pred pont\n",
      "(408.88769531, 407.58914185, True, False, 0.78653091)\n",
      "pred pont\n",
      "(457.10757446, 312.55429077, True, False, 0.59623814)\n",
      "pred pont\n",
      "(372.62939453, 323.24389648, True, False, 0.64483225)\n",
      "pred pont\n",
      "(1272.16394043, 576.53094482, True, False, 1.06408179)\n",
      "pred pont\n",
      "(1165.05517578, 348.43475342, True, False, 0.79973072)\n",
      "pred pont\n",
      "(1187.98669434, 444.53674316, True, False, 0.81386834)\n",
      "pred pont\n",
      "(1163.41064453, 323.78436279, True, False, 0.49795708)\n",
      "pred pont\n",
      "(1082.4440918, 347.9034729, True, False, 0.60363501)\n",
      "pred pont\n",
      "(1898.02966309, 576.97668457, True, False, 0.8892101)\n",
      "pred pont\n",
      "(1884.43945312, 311.12963867, True, False, 0.85351866)\n",
      "pred pont\n",
      "(1897.43969727, 384.74725342, True, False, 0.80630428)\n",
      "pred pont\n",
      "(1919.84216309, 241.09677124, True, False, 0.91040963)\n",
      "pred pont\n",
      "(1812.13793945, 275.35693359, True, False, 0.93722504)\n",
      "pred pont\n",
      "(1127.97241211, 708.39031982, True, False, 0.90073955)\n",
      "pred pont\n",
      "(1382.11950684, 662.3135376, True, False, 0.94195288)\n",
      "pred pont\n",
      "(1297.13195801, 673.35736084, True, False, 0.80775338)\n",
      "pred pont\n",
      "(1439.55944824, 707.19848633, True, False, 0.90418124)\n",
      "pred pont\n",
      "(1439.52172852, 636.13250732, True, False, 0.84487236)\n",
      "pred pont\n",
      "(1620.72570801, 768.55004883, True, False, 0.93825662)\n",
      "pred pont\n",
      "(1655.97338867, 481.33627319, True, False, 0.89119285)\n",
      "pred pont\n",
      "(1633.3190918, 553.84631348, True, False, 0.78446239)\n",
      "pred pont\n",
      "(1728.54748535, 468.38244629, True, False, 0.89268708)\n",
      "pred pont\n",
      "(1633.15002441, 445.64633179, True, False, 0.56696469)\n",
      "pred pont\n",
      "(409.07901001, 793.59228516, True, False, 0.91776854)\n",
      "pred pont\n",
      "(650.10205078, 888.10986328, True, False, 0.99326271)\n",
      "pred pont\n",
      "(587.39776611, 863.3168335, True, False, 0.75147945)\n",
      "pred pont\n",
      "(640.12036133, 936.13861084, True, False, 0.47285125)\n",
      "pred pont\n",
      "(706.88391113, 864.29016113, True, False, 0.94981277)\n",
      "pred pont\n",
      "(1490.50610352, 826.87774658, True, False, 0.58851963)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1513.88745117, 695.78137207, True, False, 0.40279847)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(299.54882812, 865.38525391, True, False, 0.83761233)\n",
      "pred pont\n",
      "(457.54751587, 1019.84436035, True, False, 1.0137974)\n",
      "pred pont\n",
      "(407.39663696, 960.31628418, True, False, 0.85745704)\n",
      "pred pont\n",
      "(469.42233276, 1091.74499512, True, False, 0.9206233)\n",
      "pred pont\n",
      "(517.61663818, 998.19262695, True, False, 0.49062771)\n",
      "pred pont\n",
      "(1574.14501953, 1091.91821289, True, False, 0.93705064)\n",
      "pred pont\n",
      "(1536.82324219, 841.85058594, True, False, 0.7965197)\n",
      "pred pont\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1549.06262207, 936.56018066, True, False, 0.67968184)\n",
      "pred pont\n",
      "(1572.51916504, 816.52319336, True, False, 0.63057309)\n",
      "pred pont\n",
      "(1501.84741211, 839.37890625, True, False, 0.58693093)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(995.37390137, 276.01953125, True, False, 0.9081583)\n",
      "pred pont\n",
      "(901.34002686, 251.70933533, True, False, 0.63884741)\n",
      "pred pont\n",
      "(1008.87390137, 301.19332886, True, False, 0.64706665)\n",
      "pred pont\n",
      "(1008.47814941, 251.95570374, True, False, 0.5910961)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(816.39172363, 144.87371826, True, False, 0.49046099)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1094.3001709, 347.2883606, True, False, 0.27638775)\n",
      "pred pont\n",
      "(782.06884766, 133.33869934, True, False, 0.48332536)\n",
      "pred pont\n",
      "(1727.68640137, 132.96148682, True, False, 0.88783854)\n",
      "pred pont\n",
      "(1692.95166016, 384.18811035, True, False, 0.89254373)\n",
      "pred pont\n",
      "(1705.87658691, 300.04354858, True, False, 0.78851217)\n",
      "pred pont\n",
      "(1645.55407715, 385.71374512, True, False, 0.76783848)\n",
      "pred pont\n",
      "(1716.08996582, 396.4163208, True, False, 0.70169663)\n",
      "pred pont\n",
      "(2258.11474609, 299.98596191, True, False, 0.90836418)\n",
      "pred pont\n",
      "(2246.33276367, 71.78575897, True, False, 0.60960835)\n",
      "pred pont\n",
      "(2232.71508789, 166.86845398, True, False, 0.76201642)\n",
      "pred pont\n",
      "(2292.16943359, 61.34727478, True, False, 0.43690279)\n",
      "pred pont\n",
      "(2245.19238281, 49.04185486, True, False, 0.27388629)\n",
      "pred pont\n",
      "(1235.45507812, 480.2253418, True, False, 0.9902997)\n",
      "pred pont\n",
      "(1153.71948242, 216.03918457, True, False, 0.79665619)\n",
      "pred pont\n",
      "(1176.0123291, 289.22454834, True, False, 0.79358572)\n",
      "pred pont\n",
      "(1200.02539062, 156.46498108, True, False, 0.766886)\n",
      "pred pont\n",
      "(1093.32971191, 158.23875427, True, False, 0.85905671)\n",
      "pred pont\n",
      "(479.74563599, 514.79644775, True, False, 0.93137223)\n",
      "pred pont\n",
      "(433.6272583, 264.7250061, True, False, 0.94767696)\n",
      "pred pont\n",
      "(445.43988037, 347.11459351, True, False, 0.81045282)\n",
      "pred pont\n",
      "(469.10113525, 215.94451904, True, False, 0.79720485)\n",
      "pred pont\n",
      "(371.02630615, 238.99787903, True, False, 0.85537368)\n",
      "pred pont\n",
      "(695.5166626, 732.57043457, True, False, 0.78650939)\n",
      "pred pont\n",
      "(684.11218262, 491.93988037, True, False, 0.85386628)\n",
      "pred pont\n",
      "(682.79003906, 564.87030029, True, False, 0.80996847)\n",
      "pred pont\n",
      "(742.84130859, 455.67260742, True, False, 0.92404348)\n",
      "pred pont\n",
      "(637.48828125, 444.41540527, True, False, 0.79336601)\n",
      "pred pont\n",
      "(780.74707031, 757.64849854, True, False, 0.85254514)\n",
      "pred pont\n",
      "(790.71014404, 1032.43688965, True, False, 0.87925929)\n",
      "pred pont\n",
      "(816.88238525, 971.3772583, True, False, 0.61232877)\n",
      "pred pont\n",
      "(731.79833984, 1067.85400391, True, False, 0.68406612)\n",
      "pred pont\n",
      "(839.33093262, 1079.74853516, True, False, 0.58028281)\n",
      "pred pont\n",
      "(1608.2557373, 768.22741699, True, False, 0.96342379)\n",
      "pred pont\n",
      "(1657.71789551, 480.44992065, True, False, 0.85709387)\n",
      "pred pont\n",
      "(1645.29699707, 554.0145874, True, False, 0.75669134)\n",
      "pred pont\n",
      "(1682.18261719, 457.24707031, True, False, 0.72472262)\n",
      "pred pont\n",
      "(1632.39099121, 457.43106079, True, False, 0.64038718)\n",
      "pred pont\n",
      "(1680.21777344, 998.12689209, True, False, 0.87848997)\n",
      "pred pont\n",
      "(1847.74414062, 828.87335205, True, False, 0.91386312)\n",
      "pred pont\n",
      "(1799.3659668, 888.44537354, True, False, 0.74769968)\n",
      "pred pont\n",
      "(1873.72607422, 840.20513916, True, False, 0.81175297)\n",
      "pred pont\n",
      "(1824.78869629, 792.27703857, True, False, 0.88784516)\n",
      "pred pont\n",
      "(1442.32788086, 1261.84301758, True, False, 0.6101616)\n",
      "pred pont\n",
      "(1489.07019043, 1055.9786377, True, False, 0.79701656)\n",
      "pred pont\n",
      "(1475.90124512, 1129.08959961, True, False, 0.81629777)\n",
      "pred pont\n",
      "(1450.10876465, 1007.4755249, True, False, 0.63342375)\n",
      "pred pont\n",
      "(1537.56713867, 1020.95263672, True, False, 0.77782822)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1032.13439941, 1069.7142334, True, False, 1.06590009)\n",
      "pred pont\n",
      "(973.5881958, 1019.76092529, True, False, 0.7660982)\n",
      "pred pont\n",
      "(1033.36108398, 1151.85400391, True, False, 0.88979512)\n",
      "pred pont\n",
      "(1079.82006836, 1070.88513184, True, False, 0.503479)\n",
      "pred pont\n",
      "(1174.88195801, 8.98227596, True, False, 0.46045369)\n",
      "pred pont\n",
      "(1211.45996094, 192.44429016, True, False, 0.84572548)\n",
      "pred pont\n",
      "(1189.09033203, 108.18154907, True, False, 0.76777405)\n",
      "pred pont\n",
      "(1151.83813477, 227.98970032, True, False, 0.63740957)\n",
      "pred pont\n",
      "(1258.39575195, 204.35342407, True, False, 0.58685982)\n",
      "pred pont\n",
      "(469.31164551, 169.35906982, True, False, 0.87604696)\n",
      "pred pont\n",
      "(264.81240845, 239.38769531, True, False, 0.72905785)\n",
      "pred pont\n",
      "(373.27520752, 216.49263, True, False, 0.67610443)\n",
      "pred pont\n",
      "(239.44902039, 214.77890015, True, False, 0.52274323)\n",
      "pred pont\n",
      "(229.18927002, 253.58705139, True, False, 0.46582955)\n",
      "pred pont\n",
      "(251.43554688, 289.96829224, True, False, 0.5661872)\n",
      "pred pont\n",
      "(252.91018677, 526.69958496, True, False, 0.80517721)\n",
      "pred pont\n",
      "(264.76370239, 431.50747681, True, False, 0.69892311)\n",
      "pred pont\n",
      "(216.07629395, 551.05395508, True, False, 0.52435607)\n",
      "pred pont\n",
      "(287.07769775, 575.44537354, True, False, 0.66908681)\n",
      "pred pont\n",
      "(1235.31628418, 312.58041382, True, False, 0.93636191)\n",
      "pred pont\n",
      "(1465.31774902, 239.54768372, True, False, 0.95275825)\n",
      "pred pont\n",
      "(1380.32885742, 250.30644226, True, False, 0.81466722)\n",
      "pred pont\n",
      "(1488.74829102, 300.12939453, True, False, 0.96151739)\n",
      "pred pont\n",
      "(1501.42272949, 228.82855225, True, False, 0.44515246)\n",
      "pred pont\n",
      "(2244.91748047, 371.21191406, True, False, 0.94021255)\n",
      "pred pont\n",
      "(2244.87353516, 131.79910278, True, False, 0.92391843)\n",
      "pred pont\n",
      "(2222.51757812, 226.79695129, True, False, 0.88681108)\n",
      "pred pont\n",
      "(2292.06982422, 119.38703918, True, False, 0.91174871)\n",
      "pred pont\n",
      "(2233.61572266, 98.18000793, True, False, 0.81159472)\n",
      "pred pont\n",
      "(493.58380127, 479.6895752, True, False, 0.91634822)\n",
      "pred pont\n",
      "(454.94378662, 264.27557373, True, False, 0.82401347)\n",
      "pred pont\n",
      "(467.95343018, 382.80032349, True, False, 0.75257194)\n",
      "pred pont\n",
      "(470.39242554, 240.9176178, True, False, 0.64911121)\n",
      "pred pont\n",
      "(432.91903687, 243.15167236, True, False, 0.63786966)\n",
      "pred pont\n",
      "(888.23974609, 491.80990601, True, False, 0.97981244)\n",
      "pred pont\n",
      "(768.8248291, 299.58312988, True, False, 0.86739773)\n",
      "pred pont\n",
      "(805.34875488, 372.53988647, True, False, 0.72014797)\n",
      "pred pont\n",
      "(816.42144775, 252.38816833, True, False, 0.75058109)\n",
      "pred pont\n",
      "(710.18151855, 310.88861084, True, False, 0.66346133)\n",
      "pred pont\n",
      "(745.46899414, 515.40118408, True, False, 0.92424053)\n",
      "pred pont\n",
      "(562.16766357, 395.37527466, True, False, 0.82825595)\n",
      "pred pont\n",
      "(635.88781738, 455.13842773, True, False, 0.77438235)\n",
      "pred pont\n",
      "(553.07714844, 359.8744812, True, False, 0.62573159)\n",
      "pred pont\n",
      "(530.11157227, 397.36727905, True, False, 0.67592496)\n",
      "pred pont\n",
      "(1380.82861328, 552.45361328, True, False, 0.85059822)\n",
      "pred pont\n",
      "(1428.70483398, 335.42745972, True, False, 0.98310286)\n",
      "pred pont\n",
      "(1405.75378418, 431.10180664, True, False, 0.80714375)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1415.0880127, 311.74768066, True, False, 0.83162731)\n",
      "pred pont\n",
      "(1620.36743164, 732.03283691, True, False, 0.8437798)\n",
      "pred pont\n",
      "(1680.97119141, 456.79031372, True, False, 0.89747411)\n",
      "pred pont\n",
      "(1666.76208496, 529.30767822, True, False, 0.73967248)\n",
      "pred pont\n",
      "(1752.49499512, 444.09677124, True, False, 1.00125933)\n",
      "pred pont\n",
      "(1655.12670898, 420.67687988, True, False, 0.7128672)\n",
      "pred pont\n",
      "(1560.37805176, 900.32366943, True, False, 0.83919793)\n",
      "pred pont\n",
      "(1465.16577148, 672.0881958, True, False, 0.78603572)\n",
      "pred pont\n",
      "(1499.70837402, 732.87811279, True, False, 0.81829369)\n",
      "pred pont\n",
      "(1488.08630371, 612.24401855, True, False, 0.76771468)\n",
      "pred pont\n",
      "(1381.22241211, 661.62561035, True, False, 0.72351849)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1345.70898438, 60.62708282, True, False, 0.80781549)\n",
      "pred pont\n",
      "(1394.31616211, 16.20122528, True, False, 0.63147056)\n",
      "pred pont\n",
      "(1307.85876465, 61.04272079, True, False, 0.48767084)\n",
      "pred pont\n",
      "(1333.35681152, 108.25289154, True, False, 0.51513344)\n",
      "pred pont\n",
      "(852.51025391, 9.01283169, True, False, 0.37859833)\n",
      "pred pont\n",
      "(827.83325195, 192.44140625, True, False, 0.75011367)\n",
      "pred pont\n",
      "(827.61547852, 108.48246765, True, False, 0.65769058)\n",
      "pred pont\n",
      "(793.16168213, 215.82484436, True, False, 0.4747529)\n",
      "pred pont\n",
      "(852.73693848, 204.19284058, True, False, 0.58024436)\n",
      "pred pont\n",
      "(637.09924316, 108.16706848, True, False, 0.96341908)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(517.4173584, 16.40613174, True, False, 0.59387511)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1235.6307373, 239.29560852, True, False, 0.92561251)\n",
      "pred pont\n",
      "(995.25823975, 228.63018799, True, False, 0.77928293)\n",
      "pred pont\n",
      "(1080.62219238, 180.57455444, True, False, 0.61310327)\n",
      "pred pont\n",
      "(949.71130371, 227.48629761, True, False, 0.60487229)\n",
      "pred pont\n",
      "(984.47808838, 263.36349487, True, False, 0.54101467)\n",
      "pred pont\n",
      "(1429.86486816, 240.76225281, True, False, 0.85722214)\n",
      "pred pont\n",
      "(1550.97644043, 456.44161987, True, False, 0.98318046)\n",
      "pred pont\n",
      "(1513.51306152, 383.30465698, True, False, 0.65934545)\n",
      "pred pont\n",
      "(1535.84655762, 482.5218811, True, False, 0.65116012)\n",
      "pred pont\n",
      "(1597.8190918, 467.35687256, True, False, 0.84195054)\n",
      "pred pont\n",
      "(252.89178467, 264.55813599, True, False, 0.83231115)\n",
      "pred pont\n",
      "(263.03964233, 83.05386353, True, False, 0.44589323)\n",
      "pred pont\n",
      "(252.35418701, 155.4296875, True, False, 0.59551823)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1006.82818604, 395.54309082, True, False, 0.84807086)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(937.53521729, 312.02246094, True, False, 0.45582989)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(2245.70678711, 396.53363037, True, False, 0.88160628)\n",
      "pred pont\n",
      "(2245.04101562, 142.68362427, True, False, 0.8896395)\n",
      "pred pont\n",
      "(2231.14257812, 228.8694458, True, False, 0.84113222)\n",
      "pred pont\n",
      "(2292.81616211, 121.28961945, True, False, 0.92443627)\n",
      "pred pont\n",
      "(2233.47412109, 109.15377808, True, False, 0.78755271)\n",
      "pred pont\n",
      "(480.34008789, 491.80978394, True, False, 0.94028139)\n",
      "pred pont\n",
      "(420.09188843, 241.45484924, True, False, 0.91284448)\n",
      "pred pont\n",
      "(432.63366699, 313.56134033, True, False, 0.80202258)\n",
      "pred pont\n",
      "(479.43734741, 202.03735352, True, False, 0.60669583)\n",
      "pred pont\n",
      "(370.33163452, 181.29226685, True, False, 0.78785586)\n",
      "pred pont\n",
      "(792.64465332, 526.52478027, True, False, 0.88054401)\n",
      "pred pont\n",
      "(851.89422607, 313.80804443, True, False, 0.80992657)\n",
      "pred pont\n",
      "(805.67022705, 418.62472534, True, False, 0.76288384)\n",
      "pred pont\n",
      "(877.26275635, 323.17559814, True, False, 0.7631315)\n",
      "pred pont\n",
      "(840.05749512, 288.36123657, True, False, 0.71681237)\n",
      "pred pont\n",
      "(1609.3223877, 733.16021729, True, False, 0.799474)\n",
      "pred pont\n",
      "(1680.89123535, 456.70925903, True, False, 0.81257689)\n",
      "pred pont\n",
      "(1657.50708008, 529.68048096, True, False, 0.6809715)\n",
      "pred pont\n",
      "(1728.39379883, 444.3526001, True, False, 0.74113381)\n",
      "pred pont\n",
      "(1643.88439941, 444.44915771, True, False, 0.75583547)\n",
      "pred pont\n",
      "(900.6317749, 781.05004883, True, False, 0.52590674)\n",
      "pred pont\n",
      "(841.55236816, 554.95037842, True, False, 0.86222321)\n",
      "pred pont\n",
      "(851.46820068, 648.66064453, True, False, 0.74353665)\n",
      "pred pont\n",
      "(888.79711914, 516.80041504, True, False, 0.78418165)\n",
      "pred pont\n",
      "(815.43804932, 541.61425781, True, False, 0.59567678)\n",
      "pred pont\n",
      "(864.92773438, 792.36755371, True, False, 0.52358085)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(904.19281006, 241.45562744, True, False, 0.21023598)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1764.88671875, 865.84265137, True, False, 0.85119879)\n",
      "pred pont\n",
      "(1728.20605469, 648.29827881, True, False, 0.78355652)\n",
      "pred pont\n",
      "(1742.0234375, 755.66912842, True, False, 0.72263753)\n",
      "pred pont\n",
      "(1741.37939453, 623.67578125, True, False, 0.77831239)\n",
      "pred pont\n",
      "(1692.64440918, 647.19775391, True, False, 0.81679344)\n",
      "pred pont\n",
      "(372.21124268, 935.13323975, True, False, 0.90109181)\n",
      "pred pont\n",
      "(445.08334351, 1153.52978516, True, False, 0.79738569)\n",
      "pred pont\n",
      "(442.62582397, 1079.22753906, True, False, 0.70249754)\n",
      "pred pont\n",
      "(408.78637695, 1187.81799316, True, False, 0.60153395)\n",
      "pred pont\n",
      "(469.0262146, 1176.72839355, True, False, 0.60681379)\n",
      "pred pont\n",
      "(912.60723877, 1250.48730469, True, False, 0.4675464)\n",
      "pred pont\n",
      "(816.45129395, 1116.48303223, True, False, 0.81889772)\n",
      "pred pont\n",
      "(840.14483643, 1188.93615723, True, False, 0.85777891)\n",
      "pred pont\n",
      "(792.37750244, 1059.05407715, True, False, 0.50985312)\n",
      "pred pont\n",
      "(840.31964111, 1056.55297852, True, False, 0.51224935)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1224.70251465, 37.31910324, True, False, 0.80899632)\n",
      "pred pont\n",
      "(1260.53942871, 8.14712238, True, False, 0.47759148)\n",
      "pred pont\n",
      "(1210.55981445, 83.10681152, True, False, 0.48545691)\n",
      "pred pont\n",
      "(1271.72497559, 107.3297348, True, False, 0.52860725)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(538.04943848, 1150.69067383, True, False, 0.33400214)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(527.95056152, 1128.28234863, True, False, 0.2986092)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(829.47509766, 10.5740366, True, False, 0.41510403)\n",
      "pred pont\n",
      "(745.33685303, 203.76213074, True, False, 0.88063151)\n",
      "pred pont\n",
      "(767.79333496, 109.16911316, True, False, 0.71528625)\n",
      "pred pont\n",
      "(719.41876221, 215.66517639, True, False, 0.64365482)\n",
      "pred pont\n",
      "(780.50872803, 227.46138, True, False, 0.6946454)\n",
      "pred pont\n",
      "(587.15576172, 24.47399139, True, False, 0.76579666)\n",
      "pred pont\n",
      "(407.01644897, 15.61588383, True, False, 0.47978389)\n",
      "pred pont\n",
      "(480.07952881, 14.75963211, True, False, 0.5369795)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(382.22344971, 23.28803635, True, False, 0.26504058)\n",
      "pred pont\n",
      "(1438.82446289, 193.260849, True, False, 0.87629193)\n",
      "pred pont\n",
      "(1514.12548828, 408.51681519, True, False, 0.97641343)\n",
      "pred pont\n",
      "(1487.24536133, 324.93371582, True, False, 0.66003799)\n",
      "pred pont\n",
      "(1498.9230957, 432.38778687, True, False, 0.7506333)\n",
      "pred pont\n",
      "(1548.63745117, 418.94161987, True, False, 0.84184778)\n",
      "pred pont\n",
      "(1201.05895996, 227.21247864, True, False, 1.00037491)\n",
      "pred pont\n",
      "(949.24914551, 166.3614502, True, False, 0.82017046)\n",
      "pred pont\n",
      "(1044.26867676, 145.07798767, True, False, 0.65649635)\n",
      "pred pont\n",
      "(923.21539307, 145.13104248, True, False, 0.57273513)\n",
      "pred pont\n",
      "(936.74737549, 192.49682617, True, False, 0.64330709)\n",
      "pred pont\n",
      "(229.37710571, 241.35090637, True, False, 0.68812895)\n",
      "pred pont\n",
      "(253.12826538, 60.31473541, True, False, 0.47252491)\n",
      "pred pont\n",
      "(241.25308228, 143.89247131, True, False, 0.57876909)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(865.02130127, 311.26165771, True, False, 0.62271255)\n",
      "pred pont\n",
      "(888.08679199, 108.29554749, True, False, 0.38842958)\n",
      "pred pont\n",
      "(864.7175293, 214.43849182, True, False, 0.58115035)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(865.78271484, 84.75673676, True, False, 0.2559081)\n",
      "pred pont\n",
      "(2257.96630859, 360.38638306, True, False, 0.86224109)\n",
      "pred pont\n",
      "(2245.83374023, 118.69829559, True, False, 0.84914917)\n",
      "pred pont\n",
      "(2231.9453125, 204.59606934, True, False, 0.86073577)\n",
      "pred pont\n",
      "(2293.24609375, 97.49927521, True, False, 0.78551632)\n",
      "pred pont\n",
      "(2244.95727539, 86.17209625, True, False, 0.67929971)\n",
      "pred pont\n",
      "(695.45703125, 456.14199829, True, False, 0.66099066)\n",
      "pred pont\n",
      "(840.9765625, 336.38824463, True, False, 0.78743058)\n",
      "pred pont\n",
      "(767.91992188, 384.50439453, True, False, 0.56612003)\n",
      "pred pont\n",
      "(910.39666748, 383.87512207, True, False, 0.69547993)\n",
      "pred pont\n",
      "(839.38580322, 301.00738525, True, False, 0.70996004)\n",
      "pred pont\n",
      "(480.85540771, 480.32339478, True, False, 0.98527545)\n",
      "pred pont\n",
      "(433.24124146, 229.77427673, True, False, 0.91285074)\n",
      "pred pont\n",
      "(444.29736328, 311.68408203, True, False, 0.81565249)\n",
      "pred pont\n",
      "(502.7038269, 191.97009277, True, False, 0.61972386)\n",
      "pred pont\n",
      "(395.67303467, 167.58062744, True, False, 0.69760591)\n",
      "pred pont\n",
      "(851.83703613, 685.46447754, True, False, 0.89358252)\n",
      "pred pont\n",
      "(695.55841064, 515.78485107, True, False, 0.82329559)\n",
      "pred pont\n",
      "(744.52624512, 576.43481445, True, False, 0.76314831)\n",
      "pred pont\n",
      "(698.37677002, 478.75732422, True, False, 0.68606025)\n",
      "pred pont\n",
      "(625.29614258, 517.85314941, True, False, 0.76568216)\n",
      "pred pont\n",
      "(1607.93334961, 743.19763184, True, False, 0.90187442)\n",
      "pred pont\n",
      "(1680.2097168, 456.3137207, True, False, 0.85097009)\n",
      "pred pont\n",
      "(1657.41149902, 539.47973633, True, False, 0.70880806)\n",
      "pred pont\n",
      "(1717.15441895, 455.5140686, True, False, 0.68401569)\n",
      "pred pont\n",
      "(1644.1472168, 432.61776733, True, False, 0.75889933)\n",
      "pred pont\n",
      "(1754.26489258, 828.4319458, True, False, 0.8424015)\n",
      "pred pont\n",
      "(1872.15917969, 624.83056641, True, False, 0.91033816)\n",
      "pred pont\n",
      "(1837.08947754, 695.00952148, True, False, 0.78076226)\n",
      "pred pont\n",
      "(1933.33508301, 600.54986572, True, False, 0.97247583)\n",
      "pred pont\n",
      "(1848.00109863, 587.68688965, True, False, 0.59387326)\n",
      "pred pont\n",
      "(397.6081543, 1069.70947266, True, False, 0.78631097)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(433.11819458, 1165.921875, True, False, 0.42580727)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(851.8225708, 1116.42529297, True, False, 0.79608959)\n",
      "pred pont\n",
      "(769.15527344, 889.56347656, True, False, 0.83942837)\n",
      "pred pont\n",
      "(791.51080322, 972.24884033, True, False, 0.78012103)\n",
      "pred pont\n",
      "(816.71008301, 829.69372559, True, False, 0.86561257)\n",
      "pred pont\n",
      "(719.70965576, 850.86749268, True, False, 0.88045669)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1260.69848633, 71.32306671, True, False, 0.84246475)\n",
      "pred pont\n",
      "(1273.93310547, 9.88556671, True, False, 0.62689698)\n",
      "pred pont\n",
      "(1211.84301758, 95.36582184, True, False, 0.72723532)\n",
      "pred pont\n",
      "(1306.4876709, 120.73754883, True, False, 0.75724316)\n",
      "pred pont\n",
      "(551.7769165, 61.33349609, True, False, 0.73162794)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(624.28326416, 120.93771362, True, False, 0.63562483)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1944.72424316, 94.7068634, True, False, 0.98493958)\n",
      "pred pont\n",
      "(1848.50085449, 312.13717651, True, False, 0.99313992)\n",
      "pred pont\n",
      "(1871.61425781, 239.76956177, True, False, 0.90664029)\n",
      "pred pont\n",
      "(1787.59533691, 347.15750122, True, False, 1.05796874)\n",
      "pred pont\n",
      "(1906.37634277, 361.65335083, True, False, 0.7718755)\n",
      "pred pont\n",
      "(1571.62841797, 226.81230164, True, False, 0.99889082)\n",
      "pred pont\n",
      "(1417.81860352, 442.75076294, True, False, 0.93686718)\n",
      "pred pont\n",
      "(1464.53967285, 373.06271362, True, False, 0.86247909)\n",
      "pred pont\n",
      "(1379.40869141, 444.00646973, True, False, 0.84216857)\n",
      "pred pont\n",
      "(1441.42089844, 514.41534424, True, False, 0.87555975)\n",
      "pred pont\n",
      "(2257.81518555, 312.69412231, True, False, 0.91655767)\n",
      "pred pont\n",
      "(2246.625, 85.04800415, True, False, 0.58870739)\n",
      "pred pont\n",
      "(2233.19750977, 180.04455566, True, False, 0.79813188)\n",
      "pred pont\n",
      "(2291.47973633, 73.49788666, True, False, 0.41122392)\n",
      "pred pont\n",
      "(2255.03393555, 71.82118225, True, False, 0.26988021)\n",
      "pred pont\n",
      "(217.01374817, 323.69473267, True, False, 0.6205098)\n",
      "pred pont\n",
      "(262.94345093, 96.10431671, True, False, 0.78646398)\n",
      "pred pont\n",
      "(241.28106689, 191.99691772, True, False, 0.80648267)\n",
      "pred pont\n",
      "(287.9644165, 72.16049194, True, False, 0.77900928)\n",
      "pred pont\n",
      "(229.52384949, 71.86705017, True, False, 0.57867587)\n",
      "pred pont\n",
      "(421.8611145, 383.96749878, True, False, 0.79065835)\n",
      "pred pont\n",
      "(493.52197266, 190.89279175, True, False, 0.79473776)\n",
      "pred pont\n",
      "(455.82876587, 288.39492798, True, False, 0.85385048)\n",
      "pred pont\n",
      "(529.18139648, 179.98092651, True, False, 0.74910408)\n",
      "pred pont\n",
      "(480.88793945, 156.00798035, True, False, 0.70219463)\n",
      "pred pont\n",
      "(528.71533203, 421.07852173, True, False, 0.68798715)\n",
      "pred pont\n",
      "(674.56677246, 216.08735657, True, False, 0.6166597)\n",
      "pred pont\n",
      "(624.28930664, 312.54141235, True, False, 0.66614527)\n",
      "pred pont\n",
      "(719.74194336, 203.39178467, True, False, 0.5060668)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(814.78155518, 419.92446899, True, False, 0.87651008)\n",
      "pred pont\n",
      "(804.90533447, 180.9241333, True, False, 0.79120189)\n",
      "pred pont\n",
      "(814.71697998, 263.43658447, True, False, 0.72470313)\n",
      "pred pont\n",
      "(841.45733643, 131.50662231, True, False, 0.82244837)\n",
      "pred pont\n",
      "(756.37091064, 156.44958496, True, False, 0.59565747)\n",
      "pred pont\n",
      "(635.15313721, 456.28689575, True, False, 0.93030405)\n",
      "pred pont\n",
      "(373.62911987, 553.51745605, True, False, 0.75650167)\n",
      "pred pont\n",
      "(444.62872314, 552.7734375, True, False, 0.80084962)\n",
      "pred pont\n",
      "(322.78219604, 538.77703857, True, False, 0.73624045)\n",
      "pred pont\n",
      "(334.76449585, 599.85467529, True, False, 0.34311643)\n",
      "pred pont\n",
      "(313.39981079, 490.83169556, True, False, 0.85247761)\n",
      "pred pont\n",
      "(337.46432495, 264.37686157, True, False, 0.84553462)\n",
      "pred pont\n",
      "(324.81192017, 360.86416626, True, False, 0.82645762)\n",
      "pred pont\n",
      "(361.73513794, 252.9414978, True, False, 0.76115727)\n",
      "pred pont\n",
      "(323.28430176, 241.22148132, True, False, 0.7414633)\n",
      "pred pont\n",
      "(695.55993652, 647.87579346, True, False, 0.96438622)\n",
      "pred pont\n",
      "(757.51275635, 913.67364502, True, False, 0.9340151)\n",
      "pred pont\n",
      "(733.99609375, 851.48730469, True, False, 0.80553913)\n",
      "pred pont\n",
      "(731.19592285, 960.48730469, True, False, 0.77173364)\n",
      "pred pont\n",
      "(817.06976318, 959.45013428, True, False, 0.93071544)\n",
      "pred pont\n",
      "(1333.65844727, 659.43756104, True, False, 0.82697052)\n",
      "pred pont\n",
      "(1319.24267578, 408.24642944, True, False, 0.84840757)\n",
      "pred pont\n",
      "(1310.1315918, 491.04431152, True, False, 0.81782115)\n",
      "pred pont\n",
      "(1357.27746582, 396.29421997, True, False, 0.84009767)\n",
      "pred pont\n",
      "(1261.19042969, 361.71408081, True, False, 0.78459597)\n",
      "pred pont\n",
      "(1115.0513916, 709.6015625, True, False, 0.92328614)\n",
      "pred pont\n",
      "(1298.16687012, 875.27087402, True, False, 0.90658718)\n",
      "pred pont\n",
      "(1236.67810059, 805.86010742, True, False, 0.77290887)\n",
      "pred pont\n",
      "(1274.75195312, 924.91009521, True, False, 0.53860611)\n",
      "pred pont\n",
      "(1355.47558594, 839.32843018, True, False, 0.65091443)\n",
      "pred pont\n",
      "(1598.48962402, 731.93127441, True, False, 0.95653921)\n",
      "pred pont\n",
      "(1706.2911377, 455.99331665, True, False, 0.91727465)\n",
      "pred pont\n",
      "(1691.4876709, 527.27203369, True, False, 0.79251391)\n",
      "pred pont\n",
      "(1764.78234863, 419.39212036, True, False, 1.02080953)\n",
      "pred pont\n",
      "(1666.91674805, 395.57296753, True, False, 0.8698079)\n",
      "pred pont\n",
      "(419.11724854, 899.30255127, True, False, 0.80949891)\n",
      "pred pont\n",
      "(299.74353027, 684.46923828, True, False, 0.87185508)\n",
      "pred pont\n",
      "(346.35180664, 755.23712158, True, False, 0.7532168)\n",
      "pred pont\n",
      "(334.48455811, 614.44140625, True, False, 0.69923514)\n",
      "pred pont\n",
      "(251.24357605, 683.19604492, True, False, 0.69893664)\n",
      "pred pont\n",
      "(684.84173584, 972.08514404, True, False, 0.66445881)\n",
      "pred pont\n",
      "(697.31512451, 1177.55322266, True, False, 0.69072986)\n",
      "pred pont\n",
      "(732.90588379, 1104.00964355, True, False, 0.69732124)\n",
      "pred pont\n",
      "(661.21923828, 1188.27246094, True, False, 0.40904179)\n",
      "pred pont\n",
      "(696.55950928, 1202.02331543, True, False, 0.4333145)\n",
      "pred pont\n",
      "(539.73980713, 1067.4786377, True, False, 0.98388356)\n",
      "pred pont\n",
      "(503.21826172, 838.84515381, True, False, 0.84472567)\n",
      "pred pont\n",
      "(514.87695312, 924.87561035, True, False, 0.76296115)\n",
      "pred pont\n",
      "(517.29193115, 758.51422119, True, False, 0.81563467)\n",
      "pred pont\n",
      "(465.97137451, 815.99517822, True, False, 0.55583167)\n",
      "pred pont\n",
      "(1404.77832031, 1091.86560059, True, False, 0.9316318)\n",
      "pred pont\n",
      "(1393.87390137, 840.84851074, True, False, 0.80593568)\n",
      "pred pont\n",
      "(1403.87341309, 913.11486816, True, False, 0.79303122)\n",
      "pred pont\n",
      "(1429.52124023, 781.72045898, True, False, 0.74193877)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(793.14715576, 1260.45629883, True, False, 0.6426934)\n",
      "pred pont\n",
      "(923.63684082, 1079.1907959, True, False, 0.83579332)\n",
      "pred pont\n",
      "(886.74804688, 1140.07458496, True, False, 0.79586291)\n",
      "pred pont\n",
      "(971.50250244, 1055.64416504, True, False, 0.39036903)\n",
      "pred pont\n",
      "(936.68383789, 1020.98901367, True, False, 0.58651996)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(469.10025024, 24.76058769, True, False, 0.73614341)\n",
      "pred pont\n",
      "(527.93536377, 8.80976486, True, False, 0.47007528)\n",
      "pred pont\n",
      "(455.27529907, 60.13240051, True, False, 0.46302313)\n",
      "pred pont\n",
      "(467.86309814, 50.22066498, True, False, 0.41954395)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1751.77807617, 60.20223236, True, False, 0.50858396)\n",
      "pred pont\n",
      "(1726.75732422, 9.67419529, True, False, 0.39478904)\n",
      "pred pont\n",
      "(1706.46240234, 120.03626251, True, False, 0.44460741)\n",
      "pred pont\n",
      "(1798.58959961, 82.24894714, True, False, 0.26201072)\n",
      "pred pont\n",
      "(1666.97607422, 144.19186401, True, False, 0.5475682)\n",
      "pred pont\n",
      "(1490.02404785, 238.25445557, True, False, 0.91279954)\n",
      "pred pont\n",
      "(1573.30249023, 181.17062378, True, False, 0.73961055)\n",
      "pred pont\n",
      "(1454.58520508, 216.56230164, True, False, 0.75052518)\n",
      "pred pont\n",
      "(1490.87817383, 286.94219971, True, False, 0.68518424)\n",
      "pred pont\n",
      "(1835.31518555, 205.0141449, True, False, 0.97799671)\n",
      "pred pont\n",
      "(1668.30957031, 17.62634468, True, False, 0.71513551)\n",
      "pred pont\n",
      "(1729.05541992, 95.40390778, True, False, 0.67054284)\n",
      "pred pont\n",
      "(1680.18859863, 10.19198227, True, False, 0.30247518)\n",
      "pred pont\n",
      "(1621.5769043, 16.32000923, True, False, 0.45134452)\n",
      "pred pont\n",
      "(217.50828552, 287.31161499, True, False, 0.39767489)\n",
      "pred pont\n",
      "(242.02835083, 228.9876709, True, False, 0.27818421)\n",
      "pred pont\n",
      "(252.04225159, 143.73895264, True, False, 0.44837698)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(482.15924072, 286.96212769, True, False, 0.94537634)\n",
      "pred pont\n",
      "(337.15515137, 469.44274902, True, False, 0.92173737)\n",
      "pred pont\n",
      "(373.71096802, 408.41674805, True, False, 0.80033761)\n",
      "pred pont\n",
      "(300.32516479, 480.04269409, True, False, 0.51429933)\n",
      "pred pont\n",
      "(371.32269287, 526.56604004, True, False, 0.79437673)\n",
      "pred pont\n",
      "(1693.31750488, 288.11663818, True, False, 0.80518502)\n",
      "pred pont\n",
      "(1860.64123535, 456.29190063, True, False, 0.62546808)\n",
      "pred pont\n",
      "(1800.22729492, 432.45669556, True, False, 0.56081337)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1918.71582031, 420.46847534, True, False, 0.66323185)\n",
      "pred pont\n",
      "(599.14978027, 359.34603882, True, False, 0.92595118)\n",
      "pred pont\n",
      "(623.89349365, 611.87103271, True, False, 0.96496511)\n",
      "pred pont\n",
      "(623.76593018, 529.00219727, True, False, 0.87817615)\n",
      "pred pont\n",
      "(586.64227295, 648.76635742, True, False, 0.86993861)\n",
      "pred pont\n",
      "(682.96343994, 637.36297607, True, False, 0.62152332)\n",
      "pred pont\n",
      "(1416.04858398, 360.05273438, True, False, 0.92405647)\n",
      "pred pont\n",
      "(1392.55249023, 96.63976288, True, False, 0.79220325)\n",
      "pred pont\n",
      "(1393.84411621, 169.74772644, True, False, 0.73868352)\n",
      "pred pont\n",
      "(1440.42016602, 37.22277069, True, False, 0.81962329)\n",
      "pred pont\n",
      "(1333.02563477, 60.15391159, True, False, 0.84034932)\n",
      "pred pont\n",
      "(1187.36535645, 409.69088745, True, False, 0.81034058)\n",
      "pred pont\n",
      "(1151.41113281, 179.14152527, True, False, 0.87237698)\n",
      "pred pont\n",
      "(1163.44824219, 252.58270264, True, False, 0.78737998)\n",
      "pred pont\n",
      "(1198.75463867, 132.19094849, True, False, 0.76577884)\n",
      "pred pont\n",
      "(1080.72741699, 132.59136963, True, False, 0.90115142)\n",
      "pred pont\n",
      "(2123.42749023, 406.85308838, True, False, 0.88605148)\n",
      "pred pont\n",
      "(2076.69018555, 649.17132568, True, False, 0.9667502)\n",
      "pred pont\n",
      "(2099.42944336, 576.5579834, True, False, 0.86927128)\n",
      "pred pont\n",
      "(2016.7598877, 682.19116211, True, False, 0.76667857)\n",
      "pred pont\n",
      "(2089.75610352, 684.9352417, True, False, 0.51208091)\n",
      "pred pont\n",
      "(2269.66870117, 431.628479, True, False, 0.74912548)\n",
      "pred pont\n",
      "(2243.69921875, 204.94378662, True, False, 0.84989458)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred pont\n",
      "(2220.34765625, 299.36233521, True, False, 0.81459498)\n",
      "pred pont\n",
      "(2292.29980469, 203.42572021, True, False, 0.74318886)\n",
      "pred pont\n",
      "(2256.70263672, 180.3966217, True, False, 0.72690606)\n",
      "pred pont\n",
      "(815.76727295, 493.37832642, True, False, 0.90281659)\n",
      "pred pont\n",
      "(803.07250977, 733.63317871, True, False, 1.01418293)\n",
      "pred pont\n",
      "(801.96234131, 659.43896484, True, False, 0.76797968)\n",
      "pred pont\n",
      "(767.59643555, 745.38067627, True, False, 0.72848231)\n",
      "pred pont\n",
      "(839.05084229, 743.76342773, True, False, 0.81981748)\n",
      "pred pont\n",
      "(277.17608643, 540.7175293, True, False, 0.9188301)\n",
      "pred pont\n",
      "(313.39727783, 791.89385986, True, False, 0.74394947)\n",
      "pred pont\n",
      "(312.63098145, 683.37445068, True, False, 0.74428761)\n",
      "pred pont\n",
      "(276.48739624, 816.74163818, True, False, 0.55253232)\n",
      "pred pont\n",
      "(349.66116333, 815.73571777, True, False, 0.58451664)\n",
      "pred pont\n",
      "(1584.84606934, 780.2232666, True, False, 1.04995906)\n",
      "pred pont\n",
      "(1704.5814209, 515.07037354, True, False, 0.84489799)\n",
      "pred pont\n",
      "(1681.1517334, 578.21429443, True, False, 0.7231037)\n",
      "pred pont\n",
      "(1751.3269043, 494.01422119, True, False, 0.74874979)\n",
      "pred pont\n",
      "(1667.59863281, 480.69052124, True, False, 0.65736324)\n",
      "pred pont\n",
      "(289.3609314, 841.33551025, True, False, 0.63865894)\n",
      "pred pont\n",
      "(443.22659302, 973.57037354, True, False, 0.98696893)\n",
      "pred pont\n",
      "(383.46362305, 912.04486084, True, False, 0.80131173)\n",
      "pred pont\n",
      "(431.77868652, 1044.19848633, True, False, 0.99873322)\n",
      "pred pont\n",
      "(480.98803711, 995.18304443, True, False, 0.54650849)\n",
      "pred pont\n",
      "(1945.84094238, 839.51049805, True, False, 0.93058008)\n",
      "pred pont\n",
      "(1874.0369873, 552.08410645, True, False, 0.89447159)\n",
      "pred pont\n",
      "(1886.76086426, 623.48883057, True, False, 0.75335056)\n",
      "pred pont\n",
      "(1919.49523926, 504.03735352, True, False, 0.9031499)\n",
      "pred pont\n",
      "(1825.49584961, 517.46295166, True, False, 0.69793737)\n",
      "pred pont\n",
      "(1163.70544434, 863.80773926, True, False, 0.91247541)\n",
      "pred pont\n",
      "(1058.13256836, 625.67242432, True, False, 0.76723933)\n",
      "pred pont\n",
      "(1091.77087402, 707.74572754, True, False, 0.78928769)\n",
      "pred pont\n",
      "(1092.15161133, 564.41772461, True, False, 0.8445127)\n",
      "pred pont\n",
      "(985.7265625, 589.57739258, True, False, 0.73497236)\n",
      "pred pont\n",
      "(839.96124268, 1270.24353027, True, False, 0.58351904)\n",
      "pred pont\n",
      "(900.90423584, 1090.65930176, True, False, 0.79344302)\n",
      "pred pont\n",
      "(876.4755249, 1153.50842285, True, False, 0.76361221)\n",
      "pred pont\n",
      "(875.74926758, 1033.32458496, True, False, 0.80654949)\n",
      "pred pont\n",
      "(949.42816162, 1045.59643555, True, False, 0.88939172)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(409.23312378, 721.00292969, True, False, 0.77511603)\n",
      "pred pont\n",
      "(492.11645508, 685.11108398, True, False, 0.53044796)\n",
      "pred pont\n",
      "(373.02337646, 709.85913086, True, False, 0.55453807)\n",
      "pred pont\n",
      "(430.11029053, 791.2041626, True, False, 0.58043939)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(598.45825195, 745.66113281, True, False, 0.63227302)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(599.67486572, 769.49249268, True, False, 0.30149606)\n",
      "pred pont\n",
      "(623.42352295, 745.06329346, True, False, 0.42000568)\n",
      "pred pont\n",
      "(589.48797607, 215.06440735, True, False, 0.89315718)\n",
      "pred pont\n",
      "(587.38702393, 467.58099365, True, False, 0.87337071)\n",
      "pred pont\n",
      "(600.54779053, 396.16607666, True, False, 0.91224903)\n",
      "pred pont\n",
      "(517.32012939, 493.26132202, True, False, 0.88836879)\n",
      "pred pont\n",
      "(623.26312256, 517.47180176, True, False, 0.7262311)\n",
      "pred pont\n",
      "(1706.26843262, 276.57727051, True, False, 0.92096692)\n",
      "pred pont\n",
      "(1513.25549316, 421.12478638, True, False, 0.87670666)\n",
      "pred pont\n",
      "(1582.85241699, 382.72946167, True, False, 0.78910458)\n",
      "pred pont\n",
      "(1476.5526123, 406.83334351, True, False, 0.76765531)\n",
      "pred pont\n",
      "(1536.06188965, 467.79193115, True, False, 0.65755463)\n",
      "pred pont\n",
      "(457.96942139, 288.27520752, True, False, 1.01213145)\n",
      "pred pont\n",
      "(335.15332031, 502.39868164, True, False, 0.85218471)\n",
      "pred pont\n",
      "(359.55441284, 421.42602539, True, False, 0.81017947)\n",
      "pred pont\n",
      "(299.14080811, 503.69799805, True, False, 0.50750548)\n",
      "pred pont\n",
      "(360.68963623, 528.88671875, True, False, 0.74901497)\n",
      "pred pont\n",
      "(1165.60229492, 310.42294312, True, False, 0.83340824)\n",
      "pred pont\n",
      "(1105.64660645, 72.17205048, True, False, 0.82325727)\n",
      "pred pont\n",
      "(1139.48400879, 156.09985352, True, False, 0.75411302)\n",
      "pred pont\n",
      "(1139.72143555, 57.77377701, True, False, 0.73314512)\n",
      "pred pont\n",
      "(1056.83349609, 72.76836395, True, False, 0.76538211)\n",
      "pred pont\n",
      "(1859.80212402, 334.4631958, True, False, 0.84505385)\n",
      "pred pont\n",
      "(1908.50390625, 565.11187744, True, False, 0.85795659)\n",
      "pred pont\n",
      "(1885.93554688, 480.55319214, True, False, 0.66781384)\n",
      "pred pont\n",
      "(1872.52307129, 600.7387085, True, False, 0.65762961)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(2089.54370117, 335.84844971, True, False, 0.91870916)\n",
      "pred pont\n",
      "(1967.68640137, 107.00992584, True, False, 0.97962099)\n",
      "pred pont\n",
      "(2006.50634766, 168.59925842, True, False, 0.7485401)\n",
      "pred pont\n",
      "(1980.50878906, 60.66983032, True, False, 0.57794827)\n",
      "pred pont\n",
      "(1909.9041748, 84.4384613, True, False, 0.73795891)\n",
      "pred pont\n",
      "(2245.66308594, 444.96142578, True, False, 0.73716021)\n",
      "pred pont\n",
      "(2233.50366211, 205.42037964, True, False, 0.88447112)\n",
      "pred pont\n",
      "(2219.72485352, 301.18087769, True, False, 0.94139796)\n",
      "pred pont\n",
      "(2293.16992188, 192.62149048, True, False, 0.98005342)\n",
      "pred pont\n",
      "(2245.12817383, 179.63995361, True, False, 0.81506836)\n",
      "pred pont\n",
      "(504.0894165, 599.26568604, True, False, 0.89038485)\n",
      "pred pont\n",
      "(251.60267639, 695.21936035, True, False, 0.82346088)\n",
      "pred pont\n",
      "(335.40777588, 660.16101074, True, False, 0.74190938)\n",
      "pred pont\n",
      "(215.75254822, 672.28875732, True, False, 0.69550854)\n",
      "pred pont\n",
      "(229.7474823, 731.89892578, True, False, 0.73684108)\n",
      "pred pont\n",
      "(661.17462158, 637.49829102, True, False, 0.93746865)\n",
      "pred pont\n",
      "(492.75161743, 804.62445068, True, False, 0.97408617)\n",
      "pred pont\n",
      "(541.09259033, 755.05993652, True, False, 0.8363474)\n",
      "pred pont\n",
      "(444.84112549, 803.77032471, True, False, 0.72675085)\n",
      "pred pont\n",
      "(515.94378662, 853.41430664, True, False, 0.73349619)\n",
      "pred pont\n",
      "(2016.58618164, 636.72772217, True, False, 0.79468006)\n",
      "pred pont\n",
      "(2256.22338867, 611.50384521, True, False, 0.94760275)\n",
      "pred pont\n",
      "(2172.52587891, 623.62493896, True, False, 0.78997993)\n",
      "pred pont\n",
      "(2280.81079102, 637.19775391, True, False, 0.78859949)\n",
      "pred pont\n",
      "(2280.06201172, 575.96490479, True, False, 0.843503)\n",
      "pred pont\n",
      "(1619.19372559, 757.80615234, True, False, 0.92048842)\n",
      "pred pont\n",
      "(1668.1361084, 480.1824646, True, False, 0.88612717)\n",
      "pred pont\n",
      "(1657.33215332, 554.17785645, True, False, 0.7965889)\n",
      "pred pont\n",
      "(1693.91809082, 445.16970825, True, False, 0.56409168)\n",
      "pred pont\n",
      "(1620.18774414, 445.56155396, True, False, 0.72894722)\n",
      "pred pont\n",
      "(311.51657104, 778.16259766, True, False, 0.75759083)\n",
      "pred pont\n",
      "(252.68907166, 972.91680908, True, False, 0.92272335)\n",
      "pred pont\n",
      "(265.23602295, 875.862854, True, False, 0.75185168)\n",
      "pred pont\n",
      "(216.63450623, 996.41473389, True, False, 0.73067981)\n",
      "pred pont\n",
      "(275.47546387, 1008.47711182, True, False, 0.71161973)\n",
      "pred pont\n",
      "(1789.50793457, 793.30285645, True, False, 0.87504631)\n",
      "pred pont\n",
      "(1969.28723145, 625.45166016, True, False, 0.87829334)\n",
      "pred pont\n",
      "(1920.36901855, 684.66876221, True, False, 0.67240727)\n",
      "pred pont\n",
      "(2006.70141602, 646.63793945, True, False, 0.63842636)\n",
      "pred pont\n",
      "(1979.03259277, 587.96875, True, False, 0.86188227)\n",
      "pred pont\n",
      "(588.59857178, 864.28582764, True, False, 0.81218141)\n",
      "pred pont\n",
      "(576.94677734, 1092.84448242, True, False, 0.80895174)\n",
      "pred pont\n",
      "(611.66540527, 1032.17504883, True, False, 0.79249716)\n",
      "pred pont\n",
      "(515.56359863, 1094.58215332, True, False, 0.77562028)\n",
      "pred pont\n",
      "(589.77868652, 1140.73486328, True, False, 0.67886841)\n",
      "pred pont\n",
      "(408.73190308, 1103.69238281, True, False, 0.95106632)\n",
      "pred pont\n",
      "(384.97494507, 793.82305908, True, False, 0.52887279)\n",
      "pred pont\n",
      "(420.0927124, 924.3873291, True, False, 0.61914945)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(362.1020813, 783.84466553, True, False, 0.34209383)\n",
      "pred pont\n",
      "(827.55163574, 1117.50976562, True, False, 0.63190931)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(851.5333252, 1189.57641602, True, False, 0.22810367)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1056.89221191, 1237.44226074, True, False, 0.3373155)\n",
      "pred pont\n",
      "(1152.58850098, 1128.8326416, True, False, 0.72957426)\n",
      "pred pont\n",
      "(1104.52392578, 1188.99816895, True, False, 0.60581636)\n",
      "pred pont\n",
      "(1153.32763672, 1080.63110352, True, False, 0.29221049)\n",
      "pred pont\n",
      "(1163.01574707, 1057.34387207, True, False, 0.58026087)\n",
      "pred pont\n",
      "(492.53945923, 1249.58068848, True, False, 0.26790845)\n",
      "pred pont\n",
      "(420.35412598, 1141.27478027, True, False, 0.45301816)\n",
      "pred pont\n",
      "(467.74557495, 1200.39465332, True, False, 0.42036736)\n",
      "pred pont\n",
      "(360.52044678, 1119.40979004, True, False, 0.27954707)\n",
      "pred pont\n",
      "(373.16329956, 1119.52966309, True, False, 0.30113259)\n",
      "pred pont\n",
      "(1250.14746094, 1259.86401367, True, False, 0.59757918)\n",
      "pred pont\n",
      "(1321.86535645, 1057.52941895, True, False, 0.75324959)\n",
      "pred pont\n",
      "(1297.91479492, 1129.37390137, True, False, 0.7288658)\n",
      "pred pont\n",
      "(1344.4362793, 1008.74023438, True, False, 0.51964283)\n",
      "pred pont\n",
      "(1368.81091309, 1021.68896484, True, False, 0.70008171)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(527.13000488, 131.02407837, True, False, 1.04844904)\n",
      "pred pont\n",
      "(517.84155273, 47.02230453, True, False, 0.79011261)\n",
      "pred pont\n",
      "(481.23275757, 180.30181885, True, False, 0.62505841)\n",
      "pred pont\n",
      "(577.08911133, 167.03767395, True, False, 0.64720702)\n",
      "pred pont\n",
      "(648.97900391, 10.47438335, True, False, 0.48295426)\n",
      "pred pont\n",
      "(709.82293701, 204.14517212, True, False, 0.86066467)\n",
      "pred pont\n",
      "(695.26977539, 120.16243744, True, False, 0.73666501)\n",
      "pred pont\n",
      "(696.33947754, 240.21588135, True, False, 0.59823048)\n",
      "pred pont\n",
      "(743.43481445, 217.8948822, True, False, 0.45027232)\n",
      "pred pont\n",
      "(1440.04321289, 15.14731216, True, False, 0.53307241)\n",
      "pred pont\n",
      "(1535.63781738, 191.80601501, True, False, 0.8882702)\n",
      "pred pont\n",
      "(1501.1796875, 107.51333618, True, False, 0.77086943)\n",
      "pred pont\n",
      "(1501.14404297, 218.0925293, True, False, 0.37011653)\n",
      "pred pont\n",
      "(1595.85998535, 239.9375, True, False, 0.96394074)\n",
      "pred pont\n",
      "(2194.79003906, 264.4286499, True, False, 0.88902313)\n",
      "pred pont\n",
      "(2039.41503906, 85.49982452, True, False, 0.97447091)\n",
      "pred pont\n",
      "(2077.99462891, 155.24134827, True, False, 0.84029293)\n",
      "pred pont\n",
      "(2052.12524414, 48.54504776, True, False, 0.75902593)\n",
      "pred pont\n",
      "(2004.66088867, 108.24169159, True, False, 0.46900445)\n",
      "pred pont\n",
      "(1766.90576172, 274.66378784, True, False, 0.80089277)\n",
      "pred pont\n",
      "(1920.54125977, 107.47761536, True, False, 0.94499987)\n",
      "pred pont\n",
      "(1848.29248047, 179.26763916, True, False, 0.60752791)\n",
      "pred pont\n",
      "(1956.9050293, 118.78900146, True, False, 0.68560576)\n",
      "pred pont\n",
      "(1920.28747559, 73.81856537, True, False, 0.73956501)\n",
      "pred pont\n",
      "(1283.34411621, 324.93856812, True, False, 0.91356581)\n",
      "pred pont\n",
      "(1391.37609863, 96.95791626, True, False, 0.85377961)\n",
      "pred pont\n",
      "(1333.05090332, 169.03225708, True, False, 0.75517207)\n",
      "pred pont\n",
      "(1427.4934082, 108.80841827, True, False, 0.83063275)\n",
      "pred pont\n",
      "(1380.91235352, 70.9936676, True, False, 0.65800261)\n",
      "pred pont\n",
      "(502.20877075, 337.31063843, True, False, 0.9963547)\n",
      "pred pont\n",
      "(408.01925659, 600.37799072, True, False, 0.87852007)\n",
      "pred pont\n",
      "(432.16851807, 539.51025391, True, False, 0.91972256)\n",
      "pred pont\n",
      "(335.23629761, 624.67590332, True, False, 0.88479078)\n",
      "pred pont\n",
      "(443.4642334, 672.54016113, True, False, 0.73785484)\n",
      "pred pont\n",
      "(1788.42626953, 444.01644897, True, False, 0.84610808)\n",
      "pred pont\n",
      "(1920.95556641, 253.46852112, True, False, 0.81810498)\n",
      "pred pont\n",
      "(1871.67614746, 348.73693848, True, False, 0.72753918)\n",
      "pred pont\n",
      "(1945.69274902, 251.17837524, True, False, 0.6371482)\n",
      "pred pont\n",
      "(1907.61938477, 229.51042175, True, False, 0.76846516)\n",
      "pred pont\n",
      "(589.49920654, 493.31643677, True, False, 0.86713487)\n",
      "pred pont\n",
      "(645.9520874, 719.56451416, True, False, 0.46043646)\n",
      "pred pont\n",
      "(635.17071533, 623.62475586, True, False, 0.54232752)\n",
      "pred pont\n",
      "(620.19702148, 899.38433838, True, False, 0.41667336)\n",
      "pred pont\n",
      "(635.62371826, 743.23962402, True, False, 0.20574261)\n",
      "pred pont\n",
      "(731.24200439, 659.99414062, True, False, 0.47673291)\n",
      "pred pont\n",
      "(648.92944336, 875.93261719, True, False, 0.83656341)\n",
      "pred pont\n",
      "(697.71710205, 803.92547607, True, False, 0.75806725)\n",
      "pred pont\n",
      "(575.83135986, 865.64190674, True, False, 0.71389288)\n",
      "pred pont\n",
      "(637.26489258, 912.10821533, True, False, 0.51178628)\n",
      "pred pont\n",
      "(1620.76782227, 745.14233398, True, False, 1.00581491)\n",
      "pred pont\n",
      "(1680.99755859, 467.83560181, True, False, 0.83222306)\n",
      "pred pont\n",
      "(1667.50219727, 551.66571045, True, False, 0.72228408)\n",
      "pred pont\n",
      "(1716.24584961, 457.24899292, True, False, 0.76330096)\n",
      "pred pont\n",
      "(1655.39489746, 444.65512085, True, False, 0.69377911)\n",
      "pred pont\n",
      "(1308.12780762, 778.63555908, True, False, 0.97677529)\n",
      "pred pont\n",
      "(1223.6373291, 1019.34637451, True, False, 0.8947832)\n",
      "pred pont\n",
      "(1247.71875, 946.9239502, True, False, 0.76299644)\n",
      "pred pont\n",
      "(1174.99536133, 1044.34606934, True, False, 0.71214277)\n",
      "pred pont\n",
      "(1247.87023926, 1054.9744873, True, False, 0.65530884)\n",
      "pred pont\n",
      "(961.65551758, 887.02441406, True, False, 0.9703753)\n",
      "pred pont\n",
      "(1022.3605957, 647.37060547, True, False, 0.91172099)\n",
      "pred pont\n",
      "(1007.57232666, 719.66558838, True, False, 0.81876689)\n",
      "pred pont\n",
      "(1104.00244141, 614.14703369, True, False, 1.01646912)\n",
      "pred pont\n",
      "(1006.9899292, 566.27258301, True, False, 0.85953593)\n",
      "pred pont\n",
      "(553.58813477, 935.14996338, True, False, 0.75325024)\n",
      "pred pont\n",
      "(611.68078613, 1175.04858398, True, False, 0.91782302)\n",
      "pred pont\n",
      "(587.30285645, 1091.72412109, True, False, 0.76033419)\n",
      "pred pont\n",
      "(587.75457764, 1201.30554199, True, False, 0.58862525)\n",
      "pred pont\n",
      "(658.92681885, 1187.21948242, True, False, 0.67329121)\n",
      "pred pont\n",
      "(1427.53552246, 1055.35510254, True, False, 0.88693702)\n",
      "pred pont\n",
      "(1464.8302002, 793.69567871, True, False, 0.88696259)\n",
      "pred pont\n",
      "(1453.94909668, 865.22564697, True, False, 0.89187574)\n",
      "pred pont\n",
      "(1536.39685059, 768.11975098, True, False, 0.97783279)\n",
      "pred pont\n",
      "(1418.05541992, 733.12414551, True, False, 1.01228213)\n",
      "pred pont\n",
      "(287.60894775, 1225.48620605, True, False, 0.59755719)\n",
      "pred pont\n",
      "(323.05718994, 1008.43353271, True, False, 0.93859065)\n",
      "pred pont\n",
      "(299.39407349, 1104.04064941, True, False, 0.7998454)\n",
      "pred pont\n",
      "(372.14855957, 985.64813232, True, False, 0.57081819)\n",
      "pred pont\n",
      "(277.52163696, 972.96557617, True, False, 0.60528314)\n",
      "pred pont\n",
      "(1247.97937012, 1260.71582031, True, False, 0.72196269)\n",
      "pred pont\n",
      "(1308.95495605, 1078.48254395, True, False, 0.62997901)\n",
      "pred pont\n",
      "(1282.91784668, 1140.7746582, True, False, 0.70105922)\n",
      "pred pont\n",
      "(1297.9453125, 1043.84484863, True, False, 0.46950141)\n",
      "pred pont\n",
      "(1345.29943848, 1055.33703613, True, False, 0.50273657)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(842.18115234, 35.97990799, True, False, 0.71827233)\n",
      "pred pont\n",
      "(827.46203613, 6.80738831, True, False, 0.27415285)\n",
      "pred pont\n",
      "(806.0111084, 96.64143372, True, False, 0.50493991)\n",
      "pred pont\n",
      "(802.21557617, 72.4830246, True, False, 0.52942705)\n",
      "pred pont\n",
      "(1727.93457031, 106.64438629, True, False, 0.8697893)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1644.45996094, 16.46940231, True, False, 0.42014194)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(442.96630859, 132.82548523, True, False, 0.82863849)\n",
      "pred pont\n",
      "(478.5135498, 395.48892212, True, False, 0.74400431)\n",
      "pred pont\n",
      "(467.79385376, 252.26965332, True, False, 0.63538015)\n",
      "pred pont\n",
      "(480.55432129, 444.1083374, True, False, 0.43728942)\n",
      "pred pont\n",
      "(481.88754272, 408.6288147, True, False, 0.35939512)\n",
      "pred pont\n",
      "(1271.84643555, 214.59979248, True, False, 0.71750224)\n",
      "pred pont\n",
      "(1057.28479004, 216.54818726, True, False, 0.89055508)\n",
      "pred pont\n",
      "(1152.33190918, 227.28233337, True, False, 0.77778566)\n",
      "pred pont\n",
      "(1032.0670166, 168.53991699, True, False, 0.64100748)\n",
      "pred pont\n",
      "(1043.66308594, 240.79911804, True, False, 0.60912633)\n",
      "pred pont\n",
      "(1836.7467041, 228.11274719, True, False, 0.88499111)\n",
      "pred pont\n",
      "(1872.54589844, 23.92132187, True, False, 0.77764481)\n",
      "pred pont\n",
      "(1861.49047852, 105.96823883, True, False, 0.75620192)\n",
      "pred pont\n",
      "(1908.8137207, 16.62160683, True, False, 0.61131316)\n",
      "pred pont\n",
      "(1849.22436523, 11.11994076, True, False, 0.47065741)\n",
      "pred pont\n",
      "(1690.98803711, 264.93597412, True, False, 0.87301582)\n",
      "pred pont\n",
      "(1596.43554688, 515.24957275, True, False, 0.78157634)\n",
      "pred pont\n",
      "(1621.18652344, 442.94039917, True, False, 0.73765415)\n",
      "pred pont\n",
      "(1546.9720459, 517.19537354, True, False, 0.69225866)\n",
      "pred pont\n",
      "(1632.68188477, 529.12261963, True, False, 0.61800063)\n",
      "pred pont\n",
      "(586.51800537, 300.77685547, True, False, 0.94260472)\n",
      "pred pont\n",
      "(670.99139404, 566.16992188, True, False, 1.00739884)\n",
      "pred pont\n",
      "(647.47839355, 492.72836304, True, False, 0.82774431)\n",
      "pred pont\n",
      "(646.55627441, 611.67492676, True, False, 0.78118116)\n",
      "pred pont\n",
      "(733.1807251, 599.15252686, True, False, 0.88725567)\n",
      "pred pont\n",
      "(1368.83105469, 312.12634277, True, False, 0.98126489)\n",
      "pred pont\n",
      "(1308.81506348, 60.56940842, True, False, 0.8487004)\n",
      "pred pont\n",
      "(1334.30517578, 132.01408386, True, False, 0.77355254)\n",
      "pred pont\n",
      "(1311.2902832, 15.99966335, True, False, 0.65487194)\n",
      "pred pont\n",
      "(1237.50866699, 59.06697845, True, False, 0.70969427)\n",
      "pred pont\n",
      "(2063.30981445, 384.64489746, True, False, 0.9368732)\n",
      "pred pont\n",
      "(2030.33203125, 133.74517822, True, False, 0.83240533)\n",
      "pred pont\n",
      "(2041.95849609, 205.27879333, True, False, 0.70524657)\n",
      "pred pont\n",
      "(2065.09399414, 110.06999207, True, False, 0.596672)\n",
      "pred pont\n",
      "(1982.75854492, 120.68971252, True, False, 0.72183102)\n",
      "pred pont\n",
      "(974.42077637, 396.16696167, True, False, 0.88781941)\n",
      "pred pont\n",
      "(959.47076416, 154.70527649, True, False, 0.84370017)\n",
      "pred pont\n",
      "(960.13165283, 239.64416504, True, False, 0.78014779)\n",
      "pred pont\n",
      "(1007.57598877, 132.44755554, True, False, 0.65131378)\n",
      "pred pont\n",
      "(911.45574951, 97.08087921, True, False, 0.84039557)\n",
      "pred pont\n",
      "(407.93933105, 467.87271118, True, False, 0.44269666)\n",
      "pred pont\n",
      "(338.00930786, 253.03321838, True, False, 0.93157929)\n",
      "pred pont\n",
      "(359.77597046, 348.8348999, True, False, 0.67237383)\n",
      "pred pont\n",
      "(371.95166016, 229.04258728, True, False, 0.53279191)\n",
      "pred pont\n",
      "(288.22216797, 205.91880798, True, False, 0.73640293)\n",
      "pred pont\n",
      "(623.73480225, 612.440979, True, False, 0.90117902)\n",
      "pred pont\n",
      "(480.10855103, 828.82666016, True, False, 0.94632638)\n",
      "pred pont\n",
      "(541.08483887, 779.6963501, True, False, 0.87160063)\n",
      "pred pont\n",
      "(422.15057373, 816.19702148, True, False, 0.8970862)\n",
      "pred pont\n",
      "(480.58956909, 889.35388184, True, False, 0.67779207)\n",
      "pred pont\n",
      "(312.03491211, 682.63110352, True, False, 0.83830363)\n",
      "pred pont\n",
      "(394.89355469, 925.26733398, True, False, 0.93509179)\n",
      "pred pont\n",
      "(371.51803589, 840.91033936, True, False, 0.74650717)\n",
      "pred pont\n",
      "(371.32095337, 973.12530518, True, False, 0.65370619)\n",
      "pred pont\n",
      "(444.73031616, 971.13659668, True, False, 0.96117496)\n",
      "pred pont\n",
      "(252.25259399, 754.65484619, True, False, 0.31745797)\n",
      "pred pont\n",
      "(288.23046875, 912.92456055, True, False, 0.42082429)\n",
      "pred pont\n",
      "(265.09875488, 841.03759766, True, False, 0.37095377)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(299.48815918, 934.19793701, True, False, 0.21945813)\n",
      "pred pont\n",
      "(1619.05932617, 766.88244629, True, False, 0.85579056)\n",
      "pred pont\n",
      "(1715.88647461, 504.51846313, True, False, 0.82681423)\n",
      "pred pont\n",
      "(1682.05651855, 612.18414307, True, False, 0.6615898)\n",
      "pred pont\n",
      "(1741.34204102, 491.11639404, True, False, 0.4707005)\n",
      "pred pont\n",
      "(1702.48596191, 480.29580688, True, False, 0.61506206)\n",
      "pred pont\n",
      "(1020.59320068, 887.20678711, True, False, 0.93780661)\n",
      "pred pont\n",
      "(1044.21240234, 635.0458374, True, False, 0.88532656)\n",
      "pred pont\n",
      "(1033.3104248, 707.84698486, True, False, 0.85039473)\n",
      "pred pont\n",
      "(1082.18200684, 586.40228271, True, False, 0.82236046)\n",
      "pred pont\n",
      "(985.78955078, 575.66461182, True, False, 0.90115142)\n",
      "pred pont\n",
      "(1789.22949219, 935.77764893, True, False, 0.93094581)\n",
      "pred pont\n",
      "(1871.61206055, 672.18878174, True, False, 0.86432064)\n",
      "pred pont\n",
      "(1848.7244873, 744.82507324, True, False, 0.79963756)\n",
      "pred pont\n",
      "(1910.00537109, 613.42016602, True, False, 0.77335709)\n",
      "pred pont\n",
      "(1848.61499023, 637.52783203, True, False, 0.3748183)\n",
      "pred pont\n",
      "(1381.72460938, 1139.57299805, True, False, 0.81339043)\n",
      "pred pont\n",
      "(1465.92932129, 910.6852417, True, False, 0.89848155)\n",
      "pred pont\n",
      "(1439.06225586, 971.23364258, True, False, 0.84146929)\n",
      "pred pont\n",
      "(1537.24572754, 889.34545898, True, False, 0.90022916)\n",
      "pred pont\n",
      "(1453.49475098, 839.29351807, True, False, 0.75626343)\n",
      "pred pont\n",
      "(1165.79443359, 1249.00024414, True, False, 0.32103142)\n",
      "pred pont\n",
      "(1235.7800293, 1105.52246094, True, False, 0.65439773)\n",
      "pred pont\n",
      "(1200.63598633, 1165.92504883, True, False, 0.69525784)\n",
      "pred pont\n",
      "(1213.22705078, 1056.74438477, True, False, 0.51301146)\n",
      "pred pont\n",
      "(1273.24560547, 1078.61950684, True, False, 0.6337254)\n",
      "pred pont\n",
      "(264.00146484, 108.93845367, True, False, 0.87379777)\n",
      "pred pont\n",
      "(300.53533936, 370.49880981, True, False, 1.02015269)\n",
      "pred pont\n",
      "(300.31106567, 275.50292969, True, False, 0.69948363)\n",
      "pred pont\n",
      "(276.47695923, 421.81182861, True, False, 0.55799794)\n",
      "pred pont\n",
      "(346.12319946, 408.63577271, True, False, 0.51537347)\n",
      "pred pont\n",
      "(614.28759766, 132.53862, True, False, 0.59710908)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(613.38201904, 11.22995377, True, False, 0.35872948)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(959.84002686, 154.52015686, True, False, 0.82307786)\n",
      "pred pont\n",
      "(961.17541504, 9.02707481, True, False, 0.23848359)\n",
      "pred pont\n",
      "(949.79425049, 47.0853653, True, False, 0.59171975)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(478.20236206, 72.34269714, True, False, 0.34877557)\n",
      "pred pont\n",
      "(1631.47546387, 193.42539978, True, False, 0.99631697)\n",
      "pred pont\n",
      "(1558.8482666, 432.14044189, True, False, 0.98783553)\n",
      "pred pont\n",
      "(1573.12133789, 359.18078613, True, False, 0.76944119)\n",
      "pred pont\n",
      "(1523.67443848, 432.11981201, True, False, 0.59710509)\n",
      "pred pont\n",
      "(1585.24707031, 454.71160889, True, False, 0.84950751)\n",
      "pred pont\n",
      "(876.85644531, 241.30839539, True, False, 0.94452971)\n",
      "pred pont\n",
      "(721.23358154, 60.99826813, True, False, 0.82734889)\n",
      "pred pont\n",
      "(768.76464844, 143.68704224, True, False, 0.748541)\n",
      "pred pont\n",
      "(734.05358887, 18.06748199, True, False, 0.59720963)\n",
      "pred pont\n",
      "(686.12652588, 61.09189224, True, False, 0.76552808)\n",
      "pred pont\n",
      "(491.37734985, 311.25582886, True, False, 0.84278578)\n",
      "pred pont\n",
      "(491.04025269, 96.45794678, True, False, 0.65105313)\n",
      "pred pont\n",
      "(490.77023315, 215.34980774, True, False, 0.63158774)\n",
      "pred pont\n",
      "(515.5279541, 83.95252991, True, False, 0.39905986)\n",
      "pred pont\n",
      "(290.58944702, 421.80194092, True, False, 0.49057877)\n",
      "pred pont\n",
      "(1391.3079834, 454.39230347, True, False, 0.83225453)\n",
      "pred pont\n",
      "(1320.97961426, 214.970047, True, False, 0.78302211)\n",
      "pred pont\n",
      "(1332.66174316, 289.01754761, True, False, 0.78451639)\n",
      "pred pont\n",
      "(1379.80480957, 157.24295044, True, False, 0.81210726)\n",
      "pred pont\n",
      "(1261.96276855, 168.83595276, True, False, 0.82740247)\n",
      "pred pont\n",
      "(1619.90686035, 730.80541992, True, False, 0.94137794)\n",
      "pred pont\n",
      "(1705.37121582, 457.41156006, True, False, 0.86875945)\n",
      "pred pont\n",
      "(1691.69262695, 540.17169189, True, False, 0.7476151)\n",
      "pred pont\n",
      "(1751.29333496, 432.44177246, True, False, 0.71742129)\n",
      "pred pont\n",
      "(1668.08190918, 420.01620483, True, False, 0.8029567)\n",
      "pred pont\n",
      "(792.61108398, 745.44494629, True, False, 0.9209162)\n",
      "pred pont\n",
      "(817.25054932, 480.45800781, True, False, 0.85586184)\n",
      "pred pont\n",
      "(804.71881104, 552.24700928, True, False, 0.8671279)\n",
      "pred pont\n",
      "(877.13104248, 433.96508789, True, False, 0.85727358)\n",
      "pred pont\n",
      "(769.76177979, 420.80963135, True, False, 0.85642993)\n",
      "pred pont\n",
      "(1055.64868164, 805.53216553, True, False, 0.82911789)\n",
      "pred pont\n",
      "(995.71185303, 600.20422363, True, False, 0.86648196)\n",
      "pred pont\n",
      "(1018.94219971, 671.50866699, True, False, 0.78705454)\n",
      "pred pont\n",
      "(985.65197754, 528.2543335, True, False, 0.77442497)\n",
      "pred pont\n",
      "(923.2633667, 564.76531982, True, False, 0.66017604)\n",
      "pred pont\n",
      "(492.33053589, 887.99737549, True, False, 0.86959106)\n",
      "pred pont\n",
      "(599.81585693, 1082.63525391, True, False, 0.91190904)\n",
      "pred pont\n",
      "(588.26733398, 1019.82391357, True, False, 0.88462257)\n",
      "pred pont\n",
      "(573.91595459, 1141.7722168, True, False, 0.60172486)\n",
      "pred pont\n",
      "(637.14123535, 1128.85949707, True, False, 0.54790139)\n",
      "pred pont\n",
      "(1080.1685791, 936.56958008, True, False, 0.94038618)\n",
      "pred pont\n",
      "(1201.15979004, 731.05596924, True, False, 0.92879337)\n",
      "pred pont\n",
      "(1176.32043457, 803.21026611, True, False, 0.82210815)\n",
      "pred pont\n",
      "(1249.41796875, 673.9319458, True, False, 0.7451418)\n",
      "pred pont\n",
      "(1212.16650391, 682.90411377, True, False, 0.41950998)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(382.76348877, 286.83343506, True, False, 0.7976771)\n",
      "pred pont\n",
      "(397.54614258, 203.61672974, True, False, 0.58524531)\n",
      "pred pont\n",
      "(407.58032227, 322.06643677, True, False, 0.35960415)\n",
      "pred pont\n",
      "(384.21664429, 313.4536438, True, False, 0.45664215)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1321.14331055, 25.27996826, True, False, 0.56404048)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1308.18200684, 82.98258209, True, False, 0.73906809)\n",
      "pred pont\n",
      "(1381.08959961, 59.26241302, True, False, 0.63559043)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(611.04193115, 119.49448395, True, False, 0.26924971)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(625.99969482, 131.86888123, True, False, 0.20586519)\n",
      "pred pont\n",
      "(1777.44885254, 131.36361694, True, False, 1.13469636)\n",
      "pred pont\n",
      "(1597.98498535, 300.38442993, True, False, 0.95734316)\n",
      "pred pont\n",
      "(1668.25366211, 262.6991272, True, False, 0.85199958)\n",
      "pred pont\n",
      "(1548.48632812, 286.76416016, True, False, 0.95886737)\n",
      "pred pont\n",
      "(1585.49365234, 350.07330322, True, False, 0.83359492)\n",
      "pred pont\n",
      "(1465.32971191, 156.90475464, True, False, 0.78806394)\n",
      "pred pont\n",
      "(1502.58068848, 420.37234497, True, False, 0.92991263)\n",
      "pred pont\n",
      "(1499.32788086, 347.8034668, True, False, 0.91372383)\n",
      "pred pont\n",
      "(1454.29833984, 469.73104858, True, False, 0.85742152)\n",
      "pred pont\n",
      "(1560.81567383, 456.66619873, True, False, 0.89563882)\n",
      "pred pont\n",
      "(731.01043701, 227.66256714, True, False, 0.83974856)\n",
      "pred pont\n",
      "(671.8704834, 10.04486275, True, False, 0.55413574)\n",
      "pred pont\n",
      "(684.53839111, 60.72501373, True, False, 0.70726299)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1380.89404297, 299.6383667, True, False, 0.92341626)\n",
      "pred pont\n",
      "(1261.32116699, 504.21139526, True, False, 0.96076578)\n",
      "pred pont\n",
      "(1306.61804199, 442.59240723, True, False, 0.87297672)\n",
      "pred pont\n",
      "(1212.52087402, 493.95419312, True, False, 0.70363015)\n",
      "pred pont\n",
      "(1285.73864746, 575.0223999, True, False, 0.77546287)\n",
      "pred pont\n",
      "(373.63720703, 337.18551636, True, False, 0.88684756)\n",
      "pred pont\n",
      "(492.19030762, 120.28094482, True, False, 0.75577992)\n",
      "pred pont\n",
      "(432.94921875, 203.98606873, True, False, 0.69633782)\n",
      "pred pont\n",
      "(528.77825928, 132.61532593, True, False, 0.62240767)\n",
      "pred pont\n",
      "(480.7671814, 108.62760925, True, False, 0.58081758)\n",
      "pred pont\n",
      "(635.16503906, 349.39901733, True, False, 0.97440588)\n",
      "pred pont\n",
      "(733.50976562, 599.42852783, True, False, 0.84225398)\n",
      "pred pont\n",
      "(719.37316895, 527.48553467, True, False, 0.82409298)\n",
      "pred pont\n",
      "(734.49066162, 622.93994141, True, False, 0.42746383)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1910.57287598, 372.75985718, True, False, 0.9116407)\n",
      "pred pont\n",
      "(1862.48376465, 120.77259827, True, False, 0.87981075)\n",
      "pred pont\n",
      "(1885.04296875, 204.55262756, True, False, 0.74812466)\n",
      "pred pont\n",
      "(1897.3059082, 83.6218338, True, False, 0.79807991)\n",
      "pred pont\n",
      "(1837.14331055, 107.88504791, True, False, 0.64689887)\n",
      "pred pont\n",
      "(420.73300171, 493.41879272, True, False, 0.97886509)\n",
      "pred pont\n",
      "(505.73516846, 755.9744873, True, False, 0.9248454)\n",
      "pred pont\n",
      "(491.50982666, 682.7612915, True, False, 0.87454933)\n",
      "pred pont\n",
      "(467.18023682, 816.26885986, True, False, 0.90539414)\n",
      "pred pont\n",
      "(575.70336914, 792.04431152, True, False, 0.96177948)\n",
      "pred pont\n",
      "(552.00549316, 493.72109985, True, False, 0.70563048)\n",
      "pred pont\n",
      "(514.66900635, 265.48199463, True, False, 0.72585839)\n",
      "pred pont\n",
      "(518.04577637, 371.55203247, True, False, 0.66800684)\n",
      "pred pont\n",
      "(539.80010986, 240.05064392, True, False, 0.75761163)\n",
      "pred pont\n",
      "(490.04333496, 252.47480774, True, False, 0.69713449)\n",
      "pred pont\n",
      "(1009.03198242, 650.12060547, True, False, 0.85966283)\n",
      "pred pont\n",
      "(1031.89099121, 406.99887085, True, False, 0.85375577)\n",
      "pred pont\n",
      "(1031.04321289, 479.12564087, True, False, 0.84316915)\n",
      "pred pont\n",
      "(1091.40527344, 361.07522583, True, False, 0.86505455)\n",
      "pred pont\n",
      "(971.02545166, 348.78778076, True, False, 1.01059806)\n",
      "pred pont\n",
      "(1332.74658203, 661.08880615, True, False, 0.94692957)\n",
      "pred pont\n",
      "(1441.63110352, 887.51702881, True, False, 0.98612177)\n",
      "pred pont\n",
      "(1405.54321289, 817.02478027, True, False, 0.72348851)\n",
      "pred pont\n",
      "(1427.52001953, 925.50543213, True, False, 0.68707711)\n",
      "pred pont\n",
      "(1499.43103027, 890.14074707, True, False, 0.81995982)\n",
      "pred pont\n",
      "(1632.37194824, 731.3572998, True, False, 0.99871796)\n",
      "pred pont\n",
      "(1751.58374023, 457.72470093, True, False, 0.92818087)\n",
      "pred pont\n",
      "(1727.45739746, 529.08721924, True, False, 0.82007539)\n",
      "pred pont\n",
      "(1803.20153809, 430.9420166, True, False, 0.95873564)\n",
      "pred pont\n",
      "(1717.6932373, 408.69772339, True, False, 0.72600687)\n",
      "pred pont\n",
      "(1094.41906738, 815.27960205, True, False, 0.85343081)\n",
      "pred pont\n",
      "(1223.64428711, 610.77880859, True, False, 0.82857245)\n",
      "pred pont\n",
      "(1177.26501465, 671.10968018, True, False, 0.7471717)\n",
      "pred pont\n",
      "(1283.29528809, 610.78985596, True, False, 0.80478597)\n",
      "pred pont\n",
      "(1200.64819336, 588.96496582, True, False, 0.46143243)\n",
      "pred pont\n",
      "(1547.82800293, 851.52380371, True, False, 1.00829005)\n",
      "pred pont\n",
      "(1694.58996582, 1066.64025879, True, False, 0.90166581)\n",
      "pred pont\n",
      "(1656.59191895, 996.48852539, True, False, 0.76472628)\n",
      "pred pont\n",
      "(1691.90063477, 1114.91003418, True, False, 0.55297625)\n",
      "pred pont\n",
      "(1740.05944824, 1068.0916748, True, False, 0.65292799)\n",
      "pred pont\n",
      "(828.36877441, 910.66809082, True, False, 0.96271849)\n",
      "pred pont\n",
      "(779.30493164, 636.77990723, True, False, 0.87444037)\n",
      "pred pont\n",
      "(792.87957764, 709.58526611, True, False, 0.78958422)\n",
      "pred pont\n",
      "(804.76953125, 588.44702148, True, False, 0.85189646)\n",
      "pred pont\n",
      "(697.70043945, 625.57336426, True, False, 0.69539911)\n",
      "pred pont\n",
      "(442.7729187, 1008.67614746, True, False, 1.00882566)\n",
      "pred pont\n",
      "(313.98260498, 769.77648926, True, False, 0.81430852)\n",
      "pred pont\n",
      "(349.69046021, 840.21539307, True, False, 0.81783211)\n",
      "pred pont\n",
      "(359.84213257, 719.67089844, True, False, 0.77046043)\n",
      "pred pont\n",
      "(263.59841919, 744.48114014, True, False, 0.80587089)\n",
      "pred pont\n",
      "(937.11962891, 10.72224617, True, False, 0.3781184)\n",
      "pred pont\n",
      "(853.72814941, 203.69471741, True, False, 0.51260078)\n",
      "pred pont\n",
      "(912.50177002, 84.45613098, True, False, 0.28873077)\n",
      "pred pont\n",
      "(829.70361328, 191.96130371, True, False, 0.3022221)\n",
      "pred pont\n",
      "(875.72436523, 217.58338928, True, False, 0.28320146)\n",
      "pred pont\n",
      "(1020.08044434, 7.38918972, True, False, 0.23604047)\n",
      "pred pont\n",
      "(1067.05810547, 133.00914001, True, False, 0.95634717)\n",
      "pred pont\n",
      "(1056.62475586, 58.91675949, True, False, 0.75783849)\n",
      "pred pont\n",
      "(1042.72949219, 201.65802002, True, False, 0.43733042)\n",
      "pred pont\n",
      "(1103.04162598, 168.96218872, True, False, 0.45403439)\n",
      "pred pont\n",
      "(1788.54370117, 37.18887711, True, False, 1.07982647)\n",
      "pred pont\n",
      "(1658.46240234, 264.3180542, True, False, 0.9034844)\n",
      "pred pont\n",
      "(1704.95117188, 204.42918396, True, False, 0.85630703)\n",
      "pred pont\n",
      "(1596.1036377, 264.56231689, True, False, 0.91199476)\n",
      "pred pont\n",
      "(1690.86413574, 313.30636597, True, False, 0.65735209)\n",
      "pred pont\n",
      "(743.22363281, 156.28239441, True, False, 0.77068573)\n",
      "pred pont\n",
      "(910.71044922, 323.40975952, True, False, 0.75684589)\n",
      "pred pont\n",
      "(815.1628418, 253.15003967, True, False, 0.49234152)\n",
      "pred pont\n",
      "(899.52020264, 371.08218384, True, False, 0.66055989)\n",
      "pred pont\n",
      "(937.85791016, 322.5921936, True, False, 0.66102421)\n",
      "pred pont\n",
      "(455.35015869, 229.2353363, True, False, 0.91734558)\n",
      "pred pont\n",
      "(551.79199219, 17.56526947, True, False, 0.71048033)\n",
      "pred pont\n",
      "(493.08242798, 96.0515213, True, False, 0.53467804)\n",
      "pred pont\n",
      "(589.91503906, 24.64179993, True, False, 0.45158345)\n",
      "pred pont\n",
      "(563.21710205, 10.64141655, True, False, 0.38202003)\n",
      "pred pont\n",
      "(1465.16650391, 240.25595093, True, False, 1.0217638)\n",
      "pred pont\n",
      "(1226.33679199, 178.99676514, True, False, 0.83774978)\n",
      "pred pont\n",
      "(1309.41491699, 192.69259644, True, False, 0.79663742)\n",
      "pred pont\n",
      "(1188.40991211, 120.99558258, True, False, 0.77014726)\n",
      "pred pont\n",
      "(1209.18261719, 205.05371094, True, False, 0.33441907)\n",
      "pred pont\n",
      "(226.73121643, 288.04962158, True, False, 0.66988492)\n",
      "pred pont\n",
      "(262.72235107, 60.24220276, True, False, 0.64923745)\n",
      "pred pont\n",
      "(250.11062622, 156.82563782, True, False, 0.61493528)\n",
      "pred pont\n",
      "(297.75701904, 60.01070786, True, False, 0.40577894)\n",
      "pred pont\n",
      "(241.5002594, 12.71857166, True, False, 0.22765642)\n",
      "pred pont\n",
      "(1033.93945312, 324.17605591, True, False, 0.95645994)\n",
      "pred pont\n",
      "(934.67913818, 551.39880371, True, False, 0.92058498)\n",
      "pred pont\n",
      "(959.33044434, 467.72079468, True, False, 0.79772443)\n",
      "pred pont\n",
      "(900.16912842, 563.66522217, True, False, 0.65613604)\n",
      "pred pont\n",
      "(960.2131958, 575.52478027, True, False, 0.59947538)\n",
      "pred pont\n",
      "(814.76306152, 370.84460449, True, False, 0.91516304)\n",
      "pred pont\n",
      "(803.50164795, 613.7164917, True, False, 0.9264589)\n",
      "pred pont\n",
      "(805.23516846, 540.3269043, True, False, 0.80049086)\n",
      "pred pont\n",
      "(733.37438965, 647.64294434, True, False, 0.96667129)\n",
      "pred pont\n",
      "(841.44384766, 659.59539795, True, False, 0.82575333)\n",
      "pred pont\n",
      "(1933.67114258, 383.34939575, True, False, 0.94585711)\n",
      "pred pont\n",
      "(1885.3840332, 133.10165405, True, False, 0.77535611)\n",
      "pred pont\n",
      "(1907.60925293, 227.46151733, True, False, 0.67990696)\n",
      "pred pont\n",
      "(1918.73742676, 109.83596039, True, False, 0.73811847)\n",
      "pred pont\n",
      "(1860.45349121, 131.59942627, True, False, 0.67256808)\n",
      "pred pont\n",
      "(1236.21179199, 431.05108643, True, False, 0.98838598)\n",
      "pred pont\n",
      "(1116.02612305, 215.55589294, True, False, 0.81758791)\n",
      "pred pont\n",
      "(1152.80297852, 277.340271, True, False, 0.72434723)\n",
      "pred pont\n",
      "(1129.44726562, 156.32800293, True, False, 0.79330099)\n",
      "pred pont\n",
      "(1057.75500488, 215.41374207, True, False, 0.60890985)\n",
      "pred pont\n",
      "(1019.6114502, 707.96160889, True, False, 0.92736328)\n",
      "pred pont\n",
      "(1080.47106934, 478.61355591, True, False, 0.85575658)\n",
      "pred pont\n",
      "(1056.03283691, 553.53729248, True, False, 0.80289209)\n",
      "pred pont\n",
      "(1151.03076172, 456.31472778, True, False, 0.86637193)\n",
      "pred pont\n",
      "(1045.62963867, 455.10836792, True, False, 0.76201659)\n",
      "pred pont\n",
      "(1646.08972168, 720.70367432, True, False, 0.96654409)\n",
      "pred pont\n",
      "(1752.89135742, 457.35244751, True, False, 0.8420189)\n",
      "pred pont\n",
      "(1728.56860352, 539.49005127, True, False, 0.74391276)\n",
      "pred pont\n",
      "(1800.66491699, 433.37979126, True, False, 0.79300278)\n",
      "pred pont\n",
      "(1728.07873535, 431.92907715, True, False, 0.5655216)\n",
      "pred pont\n",
      "(1523.91357422, 731.43920898, True, False, 0.97793269)\n",
      "pred pont\n",
      "(1452.98681641, 491.24133301, True, False, 0.83651239)\n",
      "pred pont\n",
      "(1477.28942871, 564.15557861, True, False, 0.81659466)\n",
      "pred pont\n",
      "(1475.29846191, 419.39276123, True, False, 0.89717406)\n",
      "pred pont\n",
      "(1378.85571289, 467.96990967, True, False, 0.88144356)\n",
      "pred pont\n",
      "(539.06732178, 865.57531738, True, False, 0.92473853)\n",
      "pred pont\n",
      "(374.14584351, 706.07684326, True, False, 0.77391511)\n",
      "pred pont\n",
      "(421.21157837, 743.71936035, True, False, 0.73472083)\n",
      "pred pont\n",
      "(406.96679688, 636.29864502, True, False, 0.67278022)\n",
      "pred pont\n",
      "(301.11749268, 731.87115479, True, False, 0.85230559)\n",
      "pred pont\n",
      "(887.74847412, 1019.98944092, True, False, 0.93686008)\n",
      "pred pont\n",
      "(721.99334717, 829.26379395, True, False, 0.78268421)\n",
      "pred pont\n",
      "(757.60559082, 888.29663086, True, False, 0.8731668)\n",
      "pred pont\n",
      "(745.56097412, 767.5489502, True, False, 0.85881883)\n",
      "pred pont\n",
      "(670.78985596, 817.79229736, True, False, 0.72726476)\n",
      "pred pont\n",
      "(503.43890381, 1056.40014648, True, False, 0.94921476)\n",
      "pred pont\n",
      "(696.14782715, 1200.40087891, True, False, 0.77072769)\n",
      "pred pont\n",
      "(624.16833496, 1140.22924805, True, False, 0.7428984)\n",
      "pred pont\n",
      "(708.62402344, 1225.78369141, True, False, 0.48470467)\n",
      "pred pont\n",
      "(732.23468018, 1202.33862305, True, False, 0.47787148)\n",
      "pred pont\n",
      "(698.35308838, 17.85642242, True, False, 0.80398363)\n",
      "pred pont\n",
      "(635.65527344, 287.762146, True, False, 0.95627183)\n",
      "pred pont\n",
      "(658.94830322, 204.78315735, True, False, 0.8586998)\n",
      "pred pont\n",
      "(553.79382324, 301.71484375, True, False, 0.74559122)\n",
      "pred pont\n",
      "(672.59484863, 347.9793396, True, False, 0.81860018)\n",
      "pred pont\n",
      "(841.22247314, 120.79285431, True, False, 0.89327222)\n",
      "pred pont\n",
      "(826.94732666, 396.98684692, True, False, 0.93990701)\n",
      "pred pont\n",
      "(825.8918457, 323.5612793, True, False, 0.79984403)\n",
      "pred pont\n",
      "(767.38427734, 444.67181396, True, False, 0.89995766)\n",
      "pred pont\n",
      "(877.41369629, 444.69921875, True, False, 0.9730522)\n",
      "pred pont\n",
      "(325.11868286, 214.67098999, True, False, 0.87961084)\n",
      "pred pont\n",
      "(264.36343384, 457.68902588, True, False, 0.85581535)\n",
      "pred pont\n",
      "(311.2635498, 383.19000244, True, False, 0.77961177)\n",
      "pred pont\n",
      "(226.62930298, 457.29797363, True, False, 0.7222513)\n",
      "pred pont\n",
      "(262.31698608, 492.7137146, True, False, 0.62118781)\n",
      "pred pont\n",
      "(1714.93859863, 287.46438599, True, False, 0.89927238)\n",
      "pred pont\n",
      "(1909.73181152, 157.0677948, True, False, 0.88446587)\n",
      "pred pont\n",
      "(1836.34521484, 203.13969421, True, False, 0.69279277)\n",
      "pred pont\n",
      "(1932.4519043, 180.8868866, True, False, 0.75762796)\n",
      "pred pont\n",
      "(1907.43835449, 130.9818573, True, False, 0.83155227)\n",
      "pred pont\n",
      "(1560.56408691, 432.37139893, True, False, 0.89026666)\n",
      "pred pont\n",
      "(1631.79455566, 216.07949829, True, False, 0.89602143)\n",
      "pred pont\n",
      "(1597.20654297, 299.35601807, True, False, 0.81081772)\n",
      "pred pont\n",
      "(1656.92932129, 216.40151978, True, False, 0.78452122)\n",
      "pred pont\n",
      "(1607.54174805, 192.80332947, True, False, 0.71590316)\n",
      "pred pont\n",
      "(1057.56787109, 587.15966797, True, False, 0.95410943)\n",
      "pred pont\n",
      "(1021.88769531, 335.50881958, True, False, 0.87208617)\n",
      "pred pont\n",
      "(1042.63977051, 409.07315063, True, False, 0.80981648)\n",
      "pred pont\n",
      "(1079.32873535, 287.07449341, True, False, 0.89060587)\n",
      "pred pont\n",
      "(972.97491455, 289.7756958, True, False, 0.86287439)\n",
      "pred pont\n",
      "(1357.55651855, 636.87310791, True, False, 0.98973268)\n",
      "pred pont\n",
      "(1394.40905762, 372.51220703, True, False, 0.86658853)\n",
      "pred pont\n",
      "(1392.09301758, 445.18005371, True, False, 0.86069661)\n",
      "pred pont\n",
      "(1463.21130371, 336.21850586, True, False, 0.995897)\n",
      "pred pont\n",
      "(1345.6998291, 312.3243103, True, False, 1.02303612)\n",
      "pred pont\n",
      "(1680.84838867, 696.18231201, True, False, 1.02234066)\n",
      "pred pont\n",
      "(1776.1920166, 431.62722778, True, False, 0.94749492)\n",
      "pred pont\n",
      "(1753.30480957, 503.77914429, True, False, 0.80739152)\n",
      "pred pont\n",
      "(1847.1484375, 395.07177734, True, False, 1.07030678)\n",
      "pred pont\n",
      "(1740.73083496, 361.074646, True, False, 0.95333725)\n",
      "pred pont\n",
      "(455.8454895, 851.60888672, True, False, 0.93301862)\n",
      "pred pont\n",
      "(553.37036133, 1081.36486816, True, False, 0.86197871)\n",
      "pred pont\n",
      "(515.88946533, 1006.7010498, True, False, 0.6578998)\n",
      "pred pont\n",
      "(540.63830566, 1117.46459961, True, False, 0.70404887)\n",
      "pred pont\n",
      "(598.69421387, 1103.31970215, True, False, 0.60532582)\n",
      "pred pont\n",
      "(803.43737793, 937.5166626, True, False, 0.97700328)\n",
      "pred pont\n",
      "(733.33636475, 1163.01171875, True, False, 0.66519547)\n",
      "pred pont\n",
      "(758.01367188, 1068.62854004, True, False, 0.66914427)\n",
      "pred pont\n",
      "(706.95330811, 1164.89770508, True, False, 0.39937815)\n",
      "pred pont\n",
      "(755.27612305, 1187.81994629, True, False, 0.42768675)\n",
      "pred pont\n",
      "(1955.46569824, 959.41351318, True, False, 0.89761555)\n",
      "pred pont\n",
      "(1907.97583008, 709.34710693, True, False, 0.75732058)\n",
      "pred pont\n",
      "(1922.26306152, 781.93914795, True, False, 0.78574836)\n",
      "pred pont\n",
      "(1922.40893555, 636.56915283, True, False, 0.93589801)\n",
      "pred pont\n",
      "(1813.85070801, 683.70050049, True, False, 0.74321568)\n",
      "pred pont\n",
      "(1812.14660645, 1127.0604248, True, False, 0.98812258)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1765.13342285, 1212.42553711, True, False, 0.33206993)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(443.71362305, 1200.25866699, True, False, 0.43657228)\n",
      "pred pont\n",
      "(253.5456543, 1070.38061523, True, False, 0.28072751)\n",
      "pred pont\n",
      "(336.45089722, 1130.69287109, True, False, 0.39966327)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(875.4241333, 1249.87145996, True, False, 0.70981073)\n",
      "pred pont\n",
      "(995.64117432, 1078.52038574, True, False, 0.77265352)\n",
      "pred pont\n",
      "(948.67120361, 1128.80065918, True, False, 0.86440444)\n",
      "pred pont\n",
      "(960.67498779, 1020.72625732, True, False, 0.7817809)\n",
      "pred pont\n",
      "(1055.53051758, 1067.09667969, True, False, 0.87384552)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1596.4041748, 47.74931335, True, False, 0.9637782)\n",
      "pred pont\n",
      "(1559.36853027, 9.73648262, True, False, 0.43865609)\n",
      "pred pont\n",
      "(1571.17211914, 96.00338745, True, False, 0.85364217)\n",
      "pred pont\n",
      "(1656.57897949, 59.17061234, True, False, 0.825688)\n",
      "pred pont\n",
      "(516.79412842, 10.65248013, True, False, 0.54504818)\n",
      "pred pont\n",
      "(384.31005859, 205.32911682, True, False, 0.98981476)\n",
      "pred pont\n",
      "(421.91424561, 144.60848999, True, False, 0.90789294)\n",
      "pred pont\n",
      "(312.94760132, 218.69429016, True, False, 0.87334031)\n",
      "pred pont\n",
      "(417.68701172, 277.00119019, True, False, 0.71184838)\n",
      "pred pont\n",
      "(1980.96899414, 17.83932877, True, False, 0.54688239)\n",
      "pred pont\n",
      "(1993.05773926, 203.31027222, True, False, 0.91743064)\n",
      "pred pont\n",
      "(1982.47192383, 108.16837311, True, False, 0.79950899)\n",
      "pred pont\n",
      "(1956.95922852, 229.48606873, True, False, 0.71738267)\n",
      "pred pont\n",
      "(2050.75390625, 239.98638916, True, False, 0.82660055)\n",
      "pred pont\n",
      "(1514.7869873, 119.31399536, True, False, 0.80059916)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1573.28320312, 16.53815842, True, False, 0.44006428)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1573.69299316, 277.55419922, True, False, 0.98917466)\n",
      "pred pont\n",
      "(1368.92724609, 120.63244629, True, False, 0.85072476)\n",
      "pred pont\n",
      "(1427.78320312, 169.13746643, True, False, 0.76669139)\n",
      "pred pont\n",
      "(1382.21569824, 83.82769012, True, False, 0.64134085)\n",
      "pred pont\n",
      "(1333.0501709, 133.2959137, True, False, 0.64366651)\n",
      "pred pont\n",
      "(1729.36376953, 347.7387085, True, False, 0.95532745)\n",
      "pred pont\n",
      "(1860.77294922, 130.02632141, True, False, 0.96539766)\n",
      "pred pont\n",
      "(1824.6348877, 192.6539917, True, False, 0.81710267)\n",
      "pred pont\n",
      "(1906.80444336, 131.10360718, True, False, 0.76210874)\n",
      "pred pont\n",
      "(1836.81591797, 98.00176239, True, False, 0.6610539)\n",
      "pred pont\n",
      "(468.32376099, 372.91625977, True, False, 0.94397515)\n",
      "pred pont\n",
      "(492.38986206, 637.53320312, True, False, 0.86383748)\n",
      "pred pont\n",
      "(481.36584473, 565.112854, True, False, 0.85908461)\n",
      "pred pont\n",
      "(432.18508911, 670.75579834, True, False, 0.52870506)\n",
      "pred pont\n",
      "(552.02435303, 673.0020752, True, False, 0.83856517)\n",
      "pred pont\n",
      "(719.51135254, 371.0178833, True, False, 0.6523692)\n",
      "pred pont\n",
      "(732.70373535, 623.67150879, True, False, 0.91806316)\n",
      "pred pont\n",
      "(733.7364502, 516.12835693, True, False, 0.51350474)\n",
      "pred pont\n",
      "(745.32244873, 683.41339111, True, False, 0.63043427)\n",
      "pred pont\n",
      "(766.24194336, 658.96832275, True, False, 0.4658142)\n",
      "pred pont\n",
      "(1031.54858398, 564.10284424, True, False, 1.06232893)\n",
      "pred pont\n",
      "(1068.25341797, 322.64819336, True, False, 0.85785043)\n",
      "pred pont\n",
      "(1055.9901123, 395.47106934, True, False, 0.85207421)\n",
      "pred pont\n",
      "(1151.64440918, 300.46087646, True, False, 0.90671676)\n",
      "pred pont\n",
      "(1055.18200684, 241.49050903, True, False, 0.7946527)\n",
      "pred pont\n",
      "(348.27319336, 612.84356689, True, False, 0.93159783)\n",
      "pred pont\n",
      "(323.33587646, 371.6362915, True, False, 0.89161795)\n",
      "pred pont\n",
      "(324.59194946, 456.15011597, True, False, 0.91503352)\n",
      "pred pont\n",
      "(371.49887085, 335.25753784, True, False, 0.76846117)\n",
      "pred pont\n",
      "(276.87863159, 312.25039673, True, False, 0.82883656)\n",
      "pred pont\n",
      "(1703.48583984, 673.97833252, True, False, 1.0012356)\n",
      "pred pont\n",
      "(1754.15710449, 407.20123291, True, False, 0.88430345)\n",
      "pred pont\n",
      "(1741.84216309, 491.8828125, True, False, 0.77646208)\n",
      "pred pont\n",
      "(1812.10266113, 382.70193481, True, False, 0.82239294)\n",
      "pred pont\n",
      "(1738.37438965, 372.48706055, True, False, 0.69280803)\n",
      "pred pont\n",
      "(1163.60961914, 827.09375, True, False, 0.89207631)\n",
      "pred pont\n",
      "(1116.37402344, 565.00976562, True, False, 0.91444606)\n",
      "pred pont\n",
      "(1128.96228027, 660.20690918, True, False, 0.7776947)\n",
      "pred pont\n",
      "(1153.87365723, 514.76519775, True, False, 0.92245084)\n",
      "pred pont\n",
      "(1081.71435547, 553.14044189, True, False, 0.64110863)\n",
      "pred pont\n",
      "(1584.2421875, 877.21130371, True, False, 0.93250626)\n",
      "pred pont\n",
      "(1595.81945801, 625.96911621, True, False, 0.93877107)\n",
      "pred pont\n",
      "(1595.24560547, 698.05786133, True, False, 0.83603454)\n",
      "pred pont\n",
      "(1643.0567627, 599.95672607, True, False, 0.85684246)\n",
      "pred pont\n",
      "(1536.61523438, 576.47796631, True, False, 0.93266201)\n",
      "pred pont\n",
      "(457.67321777, 1092.45825195, True, False, 0.8213309)\n",
      "pred pont\n",
      "(552.24884033, 889.63262939, True, False, 0.89667851)\n",
      "pred pont\n",
      "(515.78546143, 972.26989746, True, False, 0.87475753)\n",
      "pred pont\n",
      "(625.32879639, 875.60754395, True, False, 0.98776823)\n",
      "pred pont\n",
      "(529.42840576, 817.06292725, True, False, 0.83355415)\n",
      "pred pont\n",
      "(888.0289917, 1248.0559082, True, False, 0.53520477)\n",
      "pred pont\n",
      "(972.34039307, 1069.10620117, True, False, 0.74593061)\n",
      "pred pont\n",
      "(947.07617188, 1140.74169922, True, False, 0.83566403)\n",
      "pred pont\n",
      "(946.45776367, 1020.38043213, True, False, 0.90858501)\n",
      "pred pont\n",
      "(1033.08190918, 1033.57250977, True, False, 0.78443223)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(612.77246094, 431.90057373, True, False, 0.88936454)\n",
      "pred pont\n",
      "(624.98303223, 527.56280518, True, False, 0.67892867)\n",
      "pred pont\n",
      "(648.58624268, 394.8687439, True, False, 0.4793179)\n",
      "pred pont\n",
      "(588.03717041, 373.57894897, True, False, 0.62274301)\n",
      "pred pont\n",
      "(1583.55432129, 143.62051392, True, False, 0.97891778)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1572.64135742, 23.09626007, True, False, 0.54823565)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(670.31695557, 191.87008667, True, False, 0.99149007)\n",
      "pred pont\n",
      "(575.34301758, 457.89831543, True, False, 0.82401699)\n",
      "pred pont\n",
      "(600.43048096, 395.9604187, True, False, 0.90589797)\n",
      "pred pont\n",
      "(516.22650146, 491.80862427, True, False, 0.85566455)\n",
      "pred pont\n",
      "(612.05749512, 518.11364746, True, False, 0.57322776)\n",
      "pred pont\n",
      "(216.67811584, 215.80133057, True, False, 0.62007046)\n",
      "pred pont\n",
      "(262.99829102, 16.15448761, True, False, 0.2885032)\n",
      "pred pont\n",
      "(241.13310242, 84.74043274, True, False, 0.48488888)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1622.32019043, 250.94328308, True, False, 0.92140138)\n",
      "pred pont\n",
      "(1427.42553711, 408.43096924, True, False, 0.97596842)\n",
      "pred pont\n",
      "(1476.62072754, 360.54962158, True, False, 0.93139553)\n",
      "pred pont\n",
      "(1345.78356934, 410.21362305, True, False, 0.92178202)\n",
      "pred pont\n",
      "(1418.79382324, 491.83825684, True, False, 0.91844785)\n",
      "pred pont\n",
      "(1778.16821289, 313.13946533, True, False, 0.91620296)\n",
      "pred pont\n",
      "(1860.94311523, 61.22260666, True, False, 0.95084375)\n",
      "pred pont\n",
      "(1837.82104492, 133.37335205, True, False, 0.72893113)\n",
      "pred pont\n",
      "(1919.44787598, 35.54201508, True, False, 0.90299577)\n",
      "pred pont\n",
      "(1845.52807617, 25.05660057, True, False, 0.65332669)\n",
      "pred pont\n",
      "(299.31430054, 456.18615723, True, False, 0.86146909)\n",
      "pred pont\n",
      "(299.16329956, 719.20648193, True, False, 0.91546685)\n",
      "pred pont\n",
      "(301.02212524, 635.18139648, True, False, 0.80487293)\n",
      "pred pont\n",
      "(263.94671631, 767.00866699, True, False, 0.54351652)\n",
      "pred pont\n",
      "(324.21731567, 766.18200684, True, False, 0.51831746)\n",
      "pred pont\n",
      "(456.24456787, 479.08401489, True, False, 0.98014998)\n",
      "pred pont\n",
      "(444.99005127, 767.60339355, True, False, 1.00141764)\n",
      "pred pont\n",
      "(455.20391846, 673.2767334, True, False, 0.77226919)\n",
      "pred pont\n",
      "(408.31860352, 816.54260254, True, False, 0.58082676)\n",
      "pred pont\n",
      "(492.21133423, 793.72247314, True, False, 0.48777562)\n",
      "pred pont\n",
      "(1033.39355469, 526.84552002, True, False, 0.89242762)\n",
      "pred pont\n",
      "(1019.88201904, 265.32894897, True, False, 0.89218622)\n",
      "pred pont\n",
      "(1031.40478516, 347.50039673, True, False, 0.81230116)\n",
      "pred pont\n",
      "(1066.65930176, 214.73696899, True, False, 0.84325105)\n",
      "pred pont\n",
      "(959.64654541, 217.32043457, True, False, 0.89477372)\n",
      "pred pont\n",
      "(817.0513916, 566.33209229, True, False, 0.83883035)\n",
      "pred pont\n",
      "(742.88067627, 347.4201355, True, False, 0.81263226)\n",
      "pred pont\n",
      "(767.8862915, 420.89306641, True, False, 0.74770552)\n",
      "pred pont\n",
      "(779.0144043, 276.456604, True, False, 0.88893813)\n",
      "pred pont\n",
      "(696.24468994, 325.12338257, True, False, 0.68420637)\n",
      "pred pont\n",
      "(563.58935547, 600.3192749, True, False, 0.85834366)\n",
      "pred pont\n",
      "(600.17126465, 839.92340088, True, False, 0.84607428)\n",
      "pred pont\n",
      "(599.39550781, 743.50311279, True, False, 0.70997125)\n",
      "pred pont\n",
      "(587.6697998, 864.81854248, True, False, 0.4809739)\n",
      "pred pont\n",
      "(600.97344971, 855.09210205, True, False, 0.44619268)\n",
      "pred pont\n",
      "(1705.71081543, 683.55950928, True, False, 0.94150138)\n",
      "pred pont\n",
      "(1753.51733398, 396.33151245, True, False, 0.87692028)\n",
      "pred pont\n",
      "(1741.32995605, 479.81558228, True, False, 0.78507054)\n",
      "pred pont\n",
      "(1789.70507812, 372.53787231, True, False, 0.67139697)\n",
      "pred pont\n",
      "(1716.42456055, 370.96429443, True, False, 0.66397613)\n",
      "pred pont\n",
      "(1548.81237793, 780.04180908, True, False, 0.85242414)\n",
      "pred pont\n",
      "(1609.05456543, 550.46478271, True, False, 0.93585318)\n",
      "pred pont\n",
      "(1583.53271484, 625.19464111, True, False, 0.76559144)\n",
      "pred pont\n",
      "(1656.56408691, 540.70825195, True, False, 0.87066501)\n",
      "pred pont\n",
      "(1583.75708008, 516.58532715, True, False, 0.68203354)\n",
      "pred pont\n",
      "(540.47979736, 959.21569824, True, False, 0.89780468)\n",
      "pred pont\n",
      "(744.9553833, 862.92834473, True, False, 0.91160679)\n",
      "pred pont\n",
      "(684.31500244, 899.39117432, True, False, 0.86797422)\n",
      "pred pont\n",
      "(805.13494873, 900.05700684, True, False, 0.94744962)\n",
      "pred pont\n",
      "(743.38494873, 781.83410645, True, False, 0.72854722)\n",
      "pred pont\n",
      "(1055.75671387, 1031.19616699, True, False, 0.91025108)\n",
      "pred pont\n",
      "(1068.60058594, 768.45147705, True, False, 0.84203726)\n",
      "pred pont\n",
      "(1068.69299316, 840.73901367, True, False, 0.86799192)\n",
      "pred pont\n",
      "(1116.99523926, 719.18988037, True, False, 0.89714462)\n",
      "pred pont\n",
      "(1021.41656494, 731.003479, True, False, 0.58181775)\n",
      "pred pont\n",
      "(1501.73156738, 1166.09301758, True, False, 0.80970901)\n",
      "pred pont\n",
      "(1428.18261719, 924.35186768, True, False, 0.80266416)\n",
      "pred pont\n",
      "(1440.72167969, 997.64239502, True, False, 0.77698594)\n",
      "pred pont\n",
      "(1465.97436523, 863.88085938, True, False, 0.92137212)\n",
      "pred pont\n",
      "(1369.43701172, 876.1473999, True, False, 0.70617294)\n",
      "pred pont\n",
      "(1921.86889648, 1199.26916504, True, False, 0.7914272)\n",
      "pred pont\n",
      "(1715.8918457, 1092.98254395, True, False, 0.6218136)\n",
      "pred pont\n",
      "(1776.99560547, 1139.10351562, True, False, 0.76909721)\n",
      "pred pont\n",
      "(1702.52380371, 1018.36730957, True, False, 0.85814935)\n",
      "pred pont\n",
      "(1642.51281738, 1094.02575684, True, False, 0.92248964)\n",
      "pred pont\n",
      "(889.36077881, 1249.8380127, True, False, 0.62762624)\n",
      "pred pont\n",
      "(888.11114502, 1056.63623047, True, False, 0.80041438)\n",
      "pred pont\n",
      "(877.06054688, 1139.87182617, True, False, 0.83090663)\n",
      "pred pont\n",
      "(853.5425415, 996.61322021, True, False, 0.83437365)\n",
      "pred pont\n",
      "(937.00604248, 1030.40966797, True, False, 0.6977132)\n",
      "pred pont\n",
      "(1573.40209961, 70.57931519, True, False, 0.91192591)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1559.66760254, 9.04120731, True, False, 0.24648845)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(565.40478516, 228.43623352, True, False, 0.87652916)\n",
      "pred pont\n",
      "(710.19390869, 418.18136597, True, False, 0.98411006)\n",
      "pred pont\n",
      "(660.49267578, 336.69500732, True, False, 0.7899189)\n",
      "pred pont\n",
      "(684.8873291, 456.77236938, True, False, 0.73961776)\n",
      "pred pont\n",
      "(745.82019043, 409.83374023, True, False, 0.65475422)\n",
      "pred pont\n",
      "(1837.65930176, 241.37748718, True, False, 0.87292606)\n",
      "pred pont\n",
      "(1872.93078613, 22.63454247, True, False, 0.72654718)\n",
      "pred pont\n",
      "(1861.38439941, 95.95462036, True, False, 0.74818403)\n",
      "pred pont\n",
      "(1909.88366699, 16.86107445, True, False, 0.46113014)\n",
      "pred pont\n",
      "(1859.51928711, 10.52058411, True, False, 0.36135781)\n",
      "pred pont\n",
      "(1596.09716797, 372.7428894, True, False, 0.59490144)\n",
      "pred pont\n",
      "(1415.60852051, 468.20355225, True, False, 0.63339108)\n",
      "pred pont\n",
      "(1488.68774414, 419.43511963, True, False, 0.55681241)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1418.73352051, 502.6184082, True, False, 0.61248446)\n",
      "pred pont\n",
      "(562.92254639, 397.44723511, True, False, 0.99554926)\n",
      "pred pont\n",
      "(600.03527832, 660.7800293, True, False, 1.06131518)\n",
      "pred pont\n",
      "(600.71484375, 576.33551025, True, False, 0.87204194)\n",
      "pred pont\n",
      "(575.01879883, 683.80175781, True, False, 0.45577544)\n",
      "pred pont\n",
      "(624.24237061, 744.84155273, True, False, 0.70831102)\n",
      "pred pont\n",
      "(1032.49133301, 503.76596069, True, False, 0.9080826)\n",
      "pred pont\n",
      "(1033.60437012, 263.58093262, True, False, 0.869178)\n",
      "pred pont\n",
      "(1032.9552002, 336.92498779, True, False, 0.8069824)\n",
      "pred pont\n",
      "(1115.453125, 240.03285217, True, False, 0.87490684)\n",
      "pred pont\n",
      "(984.0614624, 204.78469849, True, False, 0.89340097)\n",
      "pred pont\n",
      "(1284.22045898, 537.99377441, True, False, 0.75378144)\n",
      "pred pont\n",
      "(1319.70788574, 289.52453613, True, False, 0.79391259)\n",
      "pred pont\n",
      "(1297.10974121, 395.82553101, True, False, 0.81199998)\n",
      "pred pont\n",
      "(1345.27087402, 276.55307007, True, False, 0.534311)\n",
      "pred pont\n",
      "(1295.94091797, 239.89627075, True, False, 0.70579457)\n",
      "pred pont\n",
      "(503.2460022, 601.33648682, True, False, 0.85683781)\n",
      "pred pont\n",
      "(455.30172729, 842.00970459, True, False, 1.0093652)\n",
      "pred pont\n",
      "(480.75320435, 779.6463623, True, False, 0.82744384)\n",
      "pred pont\n",
      "(384.77148438, 876.54614258, True, False, 0.95432591)\n",
      "pred pont\n",
      "(503.69961548, 900.12872314, True, False, 0.78877205)\n",
      "pred pont\n",
      "(1705.35168457, 672.74383545, True, False, 0.94603842)\n",
      "pred pont\n",
      "(1776.75695801, 396.49987793, True, False, 0.85090071)\n",
      "pred pont\n",
      "(1753.40625, 480.7807312, True, False, 0.71486497)\n",
      "pred pont\n",
      "(1823.75793457, 395.06417847, True, False, 0.74312776)\n",
      "pred pont\n",
      "(1752.63037109, 372.0362854, True, False, 0.67402959)\n",
      "pred pont\n",
      "(901.92010498, 695.70098877, True, False, 0.92090803)\n",
      "pred pont\n",
      "(841.19628906, 467.64953613, True, False, 0.81616688)\n",
      "pred pont\n",
      "(863.63989258, 552.83703613, True, False, 0.78404731)\n",
      "pred pont\n",
      "(877.56817627, 443.09985352, True, False, 0.81092489)\n",
      "pred pont\n",
      "(815.52484131, 466.44415283, True, False, 0.70969903)\n",
      "pred pont\n",
      "(1165.14550781, 695.918396, True, False, 0.97033376)\n",
      "pred pont\n",
      "(1116.82592773, 467.91781616, True, False, 0.78398269)\n",
      "pred pont\n",
      "(1129.86645508, 564.265625, True, False, 0.6760729)\n",
      "pred pont\n",
      "(1152.1328125, 443.47109985, True, False, 0.81478345)\n",
      "pred pont\n",
      "(1091.48864746, 466.49325562, True, False, 0.76470459)\n",
      "pred pont\n",
      "(275.31686401, 709.48907471, True, False, 0.86729521)\n",
      "pred pont\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(276.09182739, 958.89172363, True, False, 0.9483394)\n",
      "pred pont\n",
      "(287.29388428, 875.57440186, True, False, 0.75730133)\n",
      "pred pont\n",
      "(228.62261963, 972.94262695, True, False, 0.74476856)\n",
      "pred pont\n",
      "(289.13824463, 984.27697754, True, False, 0.66532934)\n",
      "pred pont\n",
      "(1441.02954102, 733.39935303, True, False, 0.76470464)\n",
      "pred pont\n",
      "(1357.40698242, 526.71960449, True, False, 0.67471594)\n",
      "pred pont\n",
      "(1391.80554199, 612.26367188, True, False, 0.63954949)\n",
      "pred pont\n",
      "(1381.34936523, 502.34637451, True, False, 0.87267423)\n",
      "pred pont\n",
      "(1343.42321777, 514.93481445, True, False, 0.62446308)\n",
      "pred pont\n",
      "(1584.11560059, 731.55108643, True, False, 0.90086991)\n",
      "pred pont\n",
      "(1501.49780273, 516.42950439, True, False, 0.89168173)\n",
      "pred pont\n",
      "(1546.71105957, 601.16851807, True, False, 0.70589447)\n",
      "pred pont\n",
      "(1523.27319336, 490.19451904, True, False, 0.73553663)\n",
      "pred pont\n",
      "(1464.2109375, 514.58764648, True, False, 0.70117611)\n",
      "pred pont\n",
      "(563.65100098, 949.01861572, True, False, 0.90594298)\n",
      "pred pont\n",
      "(577.19537354, 1174.56945801, True, False, 0.85758621)\n",
      "pred pont\n",
      "(565.89782715, 1081.50170898, True, False, 0.75393331)\n",
      "pred pont\n",
      "(542.15441895, 1199.6361084, True, False, 0.55021179)\n",
      "pred pont\n",
      "(613.0513916, 1198.01806641, True, False, 0.57662678)\n",
      "pred pont\n",
      "(1176.09460449, 1115.31152344, True, False, 0.92797047)\n",
      "pred pont\n",
      "(1116.17041016, 853.98638916, True, False, 0.83488852)\n",
      "pred pont\n",
      "(1129.50964355, 925.37811279, True, False, 0.82279021)\n",
      "pred pont\n",
      "(1153.11535645, 791.97973633, True, False, 0.98074371)\n",
      "pred pont\n",
      "(1045.2130127, 816.09619141, True, False, 0.86865532)\n",
      "pred pont\n",
      "(1332.78137207, 1234.82446289, True, False, 0.63694972)\n",
      "pred pont\n",
      "(1331.5690918, 995.00616455, True, False, 0.84100336)\n",
      "pred pont\n",
      "(1319.99401855, 1069.83959961, True, False, 0.84176552)\n",
      "pred pont\n",
      "(1308.12512207, 934.63946533, True, False, 0.54422742)\n",
      "pred pont\n",
      "(1332.25976562, 937.28649902, True, False, 0.51414573)\n",
      "pred pont\n",
      "(877.4006958, 1249.03869629, True, False, 0.56096452)\n",
      "pred pont\n",
      "(934.13928223, 1057.609375, True, False, 0.76562923)\n",
      "pred pont\n",
      "(913.17407227, 1129.47070312, True, False, 0.76061374)\n",
      "pred pont\n",
      "(887.59136963, 1020.04217529, True, False, 0.80115777)\n",
      "pred pont\n",
      "(959.83099365, 1020.80175781, True, False, 0.74519646)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1511.89916992, 312.23800659, True, False, 0.66523892)\n",
      "pred pont\n",
      "(1417.88244629, 241.95959473, True, False, 0.68361717)\n",
      "pred pont\n",
      "(1502.22680664, 348.8114624, True, False, 0.5472427)\n",
      "pred pont\n",
      "(1549.53027344, 312.1416626, True, False, 0.54790217)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1343.02685547, 181.33473206, True, False, 0.61770362)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1345.33081055, 167.57121277, True, False, 0.49804419)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1200.41564941, 131.11659241, True, False, 0.92127675)\n",
      "pred pont\n",
      "(1069.1817627, 10.02839947, True, False, 0.22197011)\n",
      "pred pont\n",
      "(1115.43774414, 24.20892334, True, False, 0.69676387)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1862.1953125, 191.63673401, True, False, 0.85870773)\n",
      "pred pont\n",
      "(1885.68481445, 15.80963039, True, False, 0.37742376)\n",
      "pred pont\n",
      "(1883.11828613, 72.76490784, True, False, 0.61049187)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1582.97424316, 204.0632019, True, False, 0.87200361)\n",
      "pred pont\n",
      "(1645.06054688, 454.90426636, True, False, 0.9151206)\n",
      "pred pont\n",
      "(1643.11682129, 371.1940918, True, False, 0.76317716)\n",
      "pred pont\n",
      "(1585.82714844, 458.03427124, True, False, 0.52717036)\n",
      "pred pont\n",
      "(1681.23217773, 479.81661987, True, False, 0.71263134)\n",
      "pred pont\n",
      "(662.16290283, 288.60882568, True, False, 0.96892411)\n",
      "pred pont\n",
      "(515.78363037, 504.91305542, True, False, 0.95899981)\n",
      "pred pont\n",
      "(552.50384521, 454.4909668, True, False, 0.90180147)\n",
      "pred pont\n",
      "(465.29150391, 526.9831543, True, False, 0.86392492)\n",
      "pred pont\n",
      "(552.6585083, 576.1395874, True, False, 0.55631149)\n",
      "pred pont\n",
      "(876.81884766, 383.12255859, True, False, 0.86762369)\n",
      "pred pont\n",
      "(828.97369385, 119.89099121, True, False, 0.89305741)\n",
      "pred pont\n",
      "(850.17498779, 204.51679993, True, False, 0.79886615)\n",
      "pred pont\n",
      "(854.41546631, 60.11651993, True, False, 0.84901124)\n",
      "pred pont\n",
      "(757.49786377, 73.7862854, True, False, 0.84939224)\n",
      "pred pont\n",
      "(1452.21838379, 469.23825073, True, False, 0.88710397)\n",
      "pred pont\n",
      "(1378.42907715, 719.93835449, True, False, 0.89066952)\n",
      "pred pont\n",
      "(1393.72363281, 649.54632568, True, False, 0.8908484)\n",
      "pred pont\n",
      "(1320.18359375, 744.87219238, True, False, 1.08904636)\n",
      "pred pont\n",
      "(1403.77770996, 780.46899414, True, False, 0.80257124)\n",
      "pred pont\n",
      "(1067.08850098, 503.16738892, True, False, 0.88837546)\n",
      "pred pont\n",
      "(1019.75512695, 275.80160522, True, False, 0.88536733)\n",
      "pred pont\n",
      "(1033.88867188, 349.35244751, True, False, 0.78673488)\n",
      "pred pont\n",
      "(1043.60144043, 205.47401428, True, False, 0.80422395)\n",
      "pred pont\n",
      "(960.42163086, 275.79812622, True, False, 0.59293818)\n",
      "pred pont\n",
      "(636.50366211, 553.37994385, True, False, 1.01530886)\n",
      "pred pont\n",
      "(708.87579346, 816.67163086, True, False, 0.85378397)\n",
      "pred pont\n",
      "(695.77581787, 744.10980225, True, False, 0.86016315)\n",
      "pred pont\n",
      "(660.06384277, 875.38684082, True, False, 0.37719354)\n",
      "pred pont\n",
      "(672.89996338, 876.48486328, True, False, 0.6102742)\n",
      "pred pont\n",
      "(936.80285645, 552.11877441, True, False, 0.89508587)\n",
      "pred pont\n",
      "(720.83093262, 564.5300293, True, False, 0.92391819)\n",
      "pred pont\n",
      "(815.78771973, 565.03338623, True, False, 0.83007014)\n",
      "pred pont\n",
      "(695.78753662, 504.67883301, True, False, 0.72409284)\n",
      "pred pont\n",
      "(718.08734131, 600.51495361, True, False, 0.77086669)\n",
      "pred pont\n",
      "(1729.43505859, 671.82537842, True, False, 0.90493065)\n",
      "pred pont\n",
      "(1799.86901855, 406.02755737, True, False, 0.78940409)\n",
      "pred pont\n",
      "(1776.69226074, 469.71252441, True, False, 0.74707365)\n",
      "pred pont\n",
      "(1838.46618652, 372.85064697, True, False, 0.75081754)\n",
      "pred pont\n",
      "(1763.62634277, 373.39300537, True, False, 0.58111453)\n",
      "pred pont\n",
      "(1211.01721191, 684.07019043, True, False, 0.90077657)\n",
      "pred pont\n",
      "(1154.2064209, 455.72024536, True, False, 0.86248928)\n",
      "pred pont\n",
      "(1175.578125, 541.58776855, True, False, 0.75577718)\n",
      "pred pont\n",
      "(1187.49047852, 430.80059814, True, False, 0.72862828)\n",
      "pred pont\n",
      "(1126.86303711, 454.74847412, True, False, 0.78739709)\n",
      "pred pont\n",
      "(540.77398682, 733.18035889, True, False, 0.95049733)\n",
      "pred pont\n",
      "(468.28192139, 1007.40606689, True, False, 0.95513421)\n",
      "pred pont\n",
      "(503.4151001, 924.87036133, True, False, 0.75576657)\n",
      "pred pont\n",
      "(408.74813843, 1008.58251953, True, False, 0.81795925)\n",
      "pred pont\n",
      "(481.21374512, 1044.4786377, True, False, 0.84293914)\n",
      "pred pont\n",
      "(1499.59729004, 780.60296631, True, False, 0.94417298)\n",
      "pred pont\n",
      "(1657.87805176, 625.39538574, True, False, 0.92461628)\n",
      "pred pont\n",
      "(1597.31201172, 684.43847656, True, False, 0.78031057)\n",
      "pred pont\n",
      "(1683.78149414, 636.99859619, True, False, 0.84224379)\n",
      "pred pont\n",
      "(1644.7442627, 589.22698975, True, False, 0.80584776)\n",
      "pred pont\n",
      "(384.82330322, 1041.57373047, True, False, 0.62801862)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(383.68652344, 1175.42504883, True, False, 0.4767631)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1249.77209473, 1055.92907715, True, False, 0.96816117)\n",
      "pred pont\n",
      "(1284.62390137, 790.82653809, True, False, 0.77369714)\n",
      "pred pont\n",
      "(1272.79785156, 853.63519287, True, False, 0.8357982)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1239.16918945, 723.75952148, True, False, 0.60272634)\n",
      "pred pont\n",
      "(671.76068115, 1248.45507812, True, False, 0.58400172)\n",
      "pred pont\n",
      "(625.237854, 1056.71105957, True, False, 0.84394127)\n",
      "pred pont\n",
      "(636.18505859, 1140.84863281, True, False, 0.87338996)\n",
      "pred pont\n",
      "(674.215271, 997.79095459, True, False, 0.77681559)\n",
      "pred pont\n",
      "(600.35620117, 996.61083984, True, False, 0.73959124)\n",
      "pred pont\n",
      "(890.80853271, 1248.39794922, True, False, 0.56175554)\n",
      "pred pont\n",
      "(913.22692871, 1056.71948242, True, False, 0.85121375)\n",
      "pred pont\n",
      "(912.81274414, 1130.02807617, True, False, 0.80858588)\n",
      "pred pont\n",
      "(853.85290527, 1009.418396, True, False, 0.69655663)\n",
      "pred pont\n",
      "(949.67956543, 1009.23040771, True, False, 0.68374532)\n",
      "pred pont\n",
      "(1862.31970215, 181.56524658, True, False, 0.88621408)\n",
      "pred pont\n",
      "(1873.2923584, 15.58801746, True, False, 0.50336617)\n",
      "pred pont\n",
      "(1872.27355957, 72.11373138, True, False, 0.67878747)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1859.11315918, 8.68550301, True, False, 0.20128067)\n",
      "pred pont\n",
      "(1284.70361328, 312.71603394, True, False, 0.82147247)\n",
      "pred pont\n",
      "(1260.07385254, 47.83464432, True, False, 0.82230276)\n",
      "pred pont\n",
      "(1273.05358887, 120.2845993, True, False, 0.82183498)\n",
      "pred pont\n",
      "(1285.96789551, 10.627985, True, False, 0.60979503)\n",
      "pred pont\n",
      "(1177.80407715, 17.98389244, True, False, 0.81157446)\n",
      "pred pont\n",
      "(1514.10900879, 346.93603516, True, False, 1.01661599)\n",
      "pred pont\n",
      "(1379.95959473, 576.0256958, True, False, 0.92281193)\n",
      "pred pont\n",
      "(1426.99865723, 503.80447388, True, False, 0.81736624)\n",
      "pred pont\n",
      "(1332.50610352, 586.80432129, True, False, 0.64501792)\n",
      "pred pont\n",
      "(1405.04077148, 611.64355469, True, False, 0.79139942)\n",
      "pred pont\n",
      "(995.85528564, 466.94641113, True, False, 0.80560184)\n",
      "pred pont\n",
      "(1081.62365723, 251.37580872, True, False, 0.88738024)\n",
      "pred pont\n",
      "(1045.70166016, 313.05218506, True, False, 0.75768363)\n",
      "pred pont\n",
      "(1153.99780273, 264.47149658, True, False, 0.80628794)\n",
      "pred pont\n",
      "(1090.9197998, 168.88983154, True, False, 0.92042834)\n",
      "pred pont\n",
      "(877.14782715, 501.5737915, True, False, 0.9319306)\n",
      "pred pont\n",
      "(648.35595703, 503.33398438, True, False, 0.84689838)\n",
      "pred pont\n",
      "(732.47869873, 515.21832275, True, False, 0.72004187)\n",
      "pred pont\n",
      "(599.22790527, 445.80444336, True, False, 0.75600767)\n",
      "pred pont\n",
      "(601.20141602, 540.46343994, True, False, 0.38066775)\n",
      "pred pont\n",
      "(1464.85107422, 612.70336914, True, False, 1.0295496)\n",
      "pred pont\n",
      "(1535.27612305, 863.27197266, True, False, 0.94351941)\n",
      "pred pont\n",
      "(1501.90771484, 792.20001221, True, False, 0.83503246)\n",
      "pred pont\n",
      "(1489.16833496, 900.66638184, True, False, 0.54229337)\n",
      "pred pont\n",
      "(1585.64233398, 888.42260742, True, False, 1.00978923)\n",
      "pred pont\n",
      "(1166.07116699, 636.03509521, True, False, 0.93716443)\n",
      "pred pont\n",
      "(961.48284912, 518.27648926, True, False, 0.8730014)\n",
      "pred pont\n",
      "(1031.66687012, 563.74847412, True, False, 0.84316605)\n",
      "pred pont\n",
      "(971.66900635, 479.49987793, True, False, 0.73609954)\n",
      "pred pont\n",
      "(911.51190186, 551.090271, True, False, 0.99256808)\n",
      "pred pont\n",
      "(1727.88330078, 673.3225708, True, False, 0.96660829)\n",
      "pred pont\n",
      "(1788.53710938, 396.82049561, True, False, 0.85919863)\n",
      "pred pont\n",
      "(1765.57983398, 479.43740845, True, False, 0.7547555)\n",
      "pred pont\n",
      "(1834.81872559, 395.13613892, True, False, 0.67458463)\n",
      "pred pont\n",
      "(1752.60192871, 372.3420105, True, False, 0.65055013)\n",
      "pred pont\n",
      "(456.3500061, 683.94299316, True, False, 0.96444494)\n",
      "pred pont\n",
      "(337.58029175, 900.5836792, True, False, 0.93809813)\n",
      "pred pont\n",
      "(372.20666504, 815.93249512, True, False, 0.79924113)\n",
      "pred pont\n",
      "(300.28564453, 924.39154053, True, False, 0.76946169)\n",
      "pred pont\n",
      "(383.2796936, 936.24829102, True, False, 0.63645542)\n",
      "pred pont\n",
      "(1597.70690918, 779.46508789, True, False, 0.97310036)\n",
      "pred pont\n",
      "(1826.42834473, 804.30645752, True, False, 0.96528345)\n",
      "pred pont\n",
      "(1751.80786133, 793.25610352, True, False, 0.80134726)\n",
      "pred pont\n",
      "(1861.82531738, 852.67791748, True, False, 0.81641275)\n",
      "pred pont\n",
      "(1884.16650391, 779.20001221, True, False, 0.63159221)\n",
      "pred pont\n",
      "(443.78179932, 899.19573975, True, False, 0.76644939)\n",
      "pred pont\n",
      "(479.67007446, 1104.0670166, True, False, 0.92508513)\n",
      "pred pont\n",
      "(493.51144409, 1020.84265137, True, False, 0.80118573)\n",
      "pred pont\n",
      "(430.40658569, 1137.70300293, True, False, 0.66669601)\n",
      "pred pont\n",
      "(479.48031616, 1153.27453613, True, False, 0.50753784)\n",
      "pred pont\n",
      "(743.35308838, 1008.27801514, True, False, 1.07844388)\n",
      "pred pont\n",
      "(706.74273682, 745.79168701, True, False, 0.98453844)\n",
      "pred pont\n",
      "(719.11877441, 829.84924316, True, False, 0.83049798)\n",
      "pred pont\n",
      "(732.30981445, 733.05328369, True, False, 0.57791239)\n",
      "pred pont\n",
      "(660.39416504, 698.45178223, True, False, 0.74046099)\n",
      "pred pont\n",
      "(2040.0020752, 1020.98168945, True, False, 0.8483184)\n",
      "pred pont\n",
      "(1920.97680664, 802.06738281, True, False, 0.62095773)\n",
      "pred pont\n",
      "(1957.07458496, 864.66027832, True, False, 0.78480995)\n",
      "pred pont\n",
      "(1909.51281738, 731.77227783, True, False, 0.79282111)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(876.04040527, 1236.88903809, True, False, 0.39857465)\n",
      "pred pont\n",
      "(960.71594238, 1078.7800293, True, False, 0.76998121)\n",
      "pred pont\n",
      "(936.83990479, 1151.46240234, True, False, 0.69103473)\n",
      "pred pont\n",
      "(913.5791626, 1021.22113037, True, False, 0.73817259)\n",
      "pred pont\n",
      "(1009.46813965, 1042.8359375, True, False, 0.75709701)\n",
      "pred pont\n",
      "(1417.96948242, 1239.77233887, True, False, 0.23808092)\n",
      "pred pont\n",
      "(1463.78112793, 1127.8527832, True, False, 0.78943735)\n",
      "pred pont\n",
      "(1429.70617676, 1199.45788574, True, False, 0.69005877)\n",
      "pred pont\n",
      "(1427.00146484, 1069.58679199, True, False, 0.61972725)\n",
      "pred pont\n",
      "(1489.26757812, 1102.79931641, True, False, 0.8106755)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(829.08410645, 721.18658447, True, False, 0.96366912)\n",
      "pred pont\n",
      "(758.27008057, 671.47412109, True, False, 0.71061933)\n",
      "pred pont\n",
      "(851.76068115, 791.58233643, True, False, 0.78758484)\n",
      "pred pont\n",
      "(865.17547607, 695.54101562, True, False, 0.53305715)\n",
      "pred pont\n",
      "(540.32421875, 70.6109848, True, False, 0.77938348)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(468.99673462, 16.54839134, True, False, 0.34230426)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(829.63867188, 144.79272461, True, False, 0.88206381)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(865.9597168, 17.49942207, True, False, 0.41988844)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1873.6484375, 168.72364807, True, False, 0.87710088)\n",
      "pred pont\n",
      "(1894.04956055, 14.25489426, True, False, 0.31897083)\n",
      "pred pont\n",
      "(1884.99279785, 60.46625137, True, False, 0.61633146)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1044.94921875, 383.81176758, True, False, 0.88385612)\n",
      "pred pont\n",
      "(1043.30310059, 133.94555664, True, False, 0.8447991)\n",
      "pred pont\n",
      "(1043.62670898, 215.95883179, True, False, 0.83446342)\n",
      "pred pont\n",
      "(1080.52087402, 82.99256897, True, False, 0.84216839)\n",
      "pred pont\n",
      "(974.07318115, 96.22793579, True, False, 0.8376202)\n",
      "pred pont\n",
      "(299.21536255, 491.33828735, True, False, 0.91012353)\n",
      "pred pont\n",
      "(371.93597412, 733.52624512, True, False, 0.92246109)\n",
      "pred pont\n",
      "(347.61422729, 661.12493896, True, False, 0.8499797)\n",
      "pred pont\n",
      "(311.59866333, 779.41589355, True, False, 0.67734963)\n",
      "pred pont\n",
      "(432.9805603, 769.13476562, True, False, 1.09310508)\n",
      "pred pont\n",
      "(1836.66662598, 637.88348389, True, False, 0.77511382)\n",
      "pred pont\n",
      "(1933.79516602, 419.98770142, True, False, 0.87939173)\n",
      "pred pont\n",
      "(1909.13183594, 481.58239746, True, False, 0.82907343)\n",
      "pred pont\n",
      "(1993.3951416, 397.12533569, True, False, 0.92455024)\n",
      "pred pont\n",
      "(1897.4309082, 349.52987671, True, False, 0.89902127)\n",
      "pred pont\n",
      "(1679.90429688, 661.37823486, True, False, 0.88618642)\n",
      "pred pont\n",
      "(1790.18383789, 408.19650269, True, False, 0.90487462)\n",
      "pred pont\n",
      "(1754.32458496, 480.20458984, True, False, 0.71910864)\n",
      "pred pont\n",
      "(1824.65917969, 408.99133301, True, False, 0.6365993)\n",
      "pred pont\n",
      "(1776.03503418, 370.74868774, True, False, 0.79775488)\n",
      "pred pont\n",
      "(1393.71972656, 768.61120605, True, False, 0.99594826)\n",
      "pred pont\n",
      "(1477.8269043, 1019.51580811, True, False, 0.90840894)\n",
      "pred pont\n",
      "(1450.90002441, 948.15960693, True, False, 0.81154269)\n",
      "pred pont\n",
      "(1464.31555176, 1058.05517578, True, False, 0.64923334)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1835.85021973, 780.69854736, True, False, 1.02032888)\n",
      "pred pont\n",
      "(2005.41357422, 936.52911377, True, False, 1.05425799)\n",
      "pred pont\n",
      "(1944.94152832, 899.10058594, True, False, 0.79995716)\n",
      "pred pont\n",
      "(2017.6998291, 996.91223145, True, False, 0.74724483)\n",
      "pred pont\n",
      "(2077.32250977, 922.96429443, True, False, 0.76190817)\n",
      "pred pont\n",
      "(684.31188965, 805.15466309, True, False, 0.85626501)\n",
      "pred pont\n",
      "(865.30731201, 959.62750244, True, False, 0.72100085)\n",
      "pred pont\n",
      "(852.93603516, 875.86785889, True, False, 0.71590781)\n",
      "pred pont\n",
      "(851.55877686, 1020.5178833, True, False, 0.73233372)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(1717.09240723, 959.57562256, True, False, 0.74948865)\n",
      "pred pont\n",
      "(1729.91442871, 709.61315918, True, False, 0.80625385)\n",
      "pred pont\n",
      "(1729.01806641, 792.84332275, True, False, 0.76399529)\n",
      "pred pont\n",
      "(1787.4407959, 684.74822998, True, False, 0.56551874)\n",
      "pred pont\n",
      "(1715.85168457, 673.86022949, True, False, 0.52566195)\n",
      "pred pont\n",
      "(865.3614502, 1238.33190918, True, False, 0.50549042)\n",
      "pred pont\n",
      "(934.96606445, 1068.42150879, True, False, 0.84651154)\n",
      "pred pont\n",
      "(911.2255249, 1151.27905273, True, False, 0.75848198)\n",
      "pred pont\n",
      "(984.55834961, 1019.99383545, True, False, 0.80170053)\n",
      "pred pont\n",
      "(935.67028809, 1020.38873291, True, False, 0.6438725)\n",
      "pred pont\n",
      "(1465.7442627, 1246.83764648, True, False, 0.55383158)\n",
      "pred pont\n",
      "(1561.30126953, 1068.40026855, True, False, 0.76392108)\n",
      "pred pont\n",
      "(1535.94702148, 1141.19274902, True, False, 0.77371377)\n",
      "pred pont\n",
      "(1548.61206055, 1020.06671143, True, False, 0.58185923)\n",
      "pred pont\n",
      "(1595.38977051, 1031.43078613, True, False, 0.86588365)\n",
      "pred pont\n",
      "(nan, nan, True, False, 0.)\n",
      "pred pont\n",
      "(694.44207764, 109.22511292, True, False, 0.8687731)\n",
      "pred pont\n",
      "(683.93249512, 24.7903614, True, False, 0.56689864)\n",
      "pred pont\n",
      "(720.35247803, 145.01742554, True, False, 0.59684515)\n",
      "pred pont\n",
      "(683.60229492, 145.16648865, True, False, 0.39797446)\n",
      "pred pont\n",
      "(526.52197266, 191.23435974, True, False, 0.94717741)\n",
      "pred pont\n",
      "(622.46807861, 432.28009033, True, False, 0.91803461)\n",
      "pred pont\n",
      "(587.65002441, 347.92355347, True, False, 0.78631759)\n",
      "pred pont\n",
      "(598.96838379, 467.63824463, True, False, 0.64904851)\n",
      "pred pont\n",
      "(670.01477051, 468.54522705, True, False, 0.86647832)\n",
      "pred pont\n",
      "(1056.6854248, 346.40661621, True, False, 0.8957541)\n",
      "pred pont\n",
      "(1058.21557617, 95.29347992, True, False, 0.80539101)\n",
      "pred pont\n",
      "(1057.6328125, 167.45549011, True, False, 0.78638458)\n",
      "pred pont\n",
      "(1116.65881348, 37.39013672, True, False, 0.84564835)\n",
      "pred pont\n",
      "(1008.21276855, 35.70765305, True, False, 0.84894198)\n",
      "pred pont\n",
      "(598.64904785, 479.57876587, True, False, 0.88281065)\n",
      "pred pont\n",
      "(589.87585449, 742.91882324, True, False, 0.89831716)\n",
      "pred pont\n",
      "(599.9743042, 661.37744141, True, False, 0.88280207)\n",
      "pred pont\n",
      "(528.8493042, 780.81878662, True, False, 0.75723386)\n",
      "pred pont\n",
      "(637.13995361, 792.26086426, True, False, 0.81639922)\n",
      "pred pont\n",
      "(1451.61413574, 552.99829102, True, False, 1.07703447)\n",
      "pred pont\n",
      "(1262.21008301, 347.30117798, True, False, 0.77612251)\n",
      "pred pont\n",
      "(1309.39953613, 408.64208984, True, False, 0.85938966)\n",
      "pred pont\n",
      "(1274.09765625, 275.24951172, True, False, 0.84278554)\n",
      "pred pont\n",
      "(1210.97229004, 348.57217407, True, False, 0.79154342)\n",
      "pred pont\n",
      "(2087.90844727, 599.09051514, True, False, 0.95531064)\n",
      "pred pont\n",
      "(2075.93603516, 349.12298584, True, False, 0.86270994)\n",
      "pred pont\n",
      "(2078.44458008, 421.20825195, True, False, 0.77702045)\n",
      "pred pont\n",
      "(2114.33691406, 298.90560913, True, False, 0.93564826)\n",
      "pred pont\n",
      "(2015.68701172, 311.35836792, True, False, 0.88302708)\n",
      "pred pont\n",
      "(1715.92077637, 613.49945068, True, False, 1.05286264)\n",
      "pred pont\n",
      "(1634.55517578, 336.21853638, True, False, 0.88194114)\n",
      "pred pont\n",
      "(1667.13525391, 408.67364502, True, False, 0.79066634)\n",
      "pred pont\n",
      "(1656.98425293, 287.48455811, True, False, 0.76572335)\n",
      "pred pont\n",
      "(1560.11865234, 323.29391479, True, False, 0.92460132)\n",
      "pred pont\n",
      "(1151.62866211, 815.85107422, True, False, 0.93147916)\n",
      "pred pont\n",
      "(1153.33288574, 551.77563477, True, False, 0.83168644)\n",
      "pred pont\n",
      "(1151.96838379, 625.29858398, True, False, 0.80615181)\n",
      "pred pont\n",
      "(1202.57006836, 504.29135132, True, False, 0.93222362)\n",
      "pred pont\n",
      "(1094.56103516, 504.43832397, True, False, 0.81612921)\n",
      "pred pont\n",
      "(1487.04345703, 851.5513916, True, False, 0.96982938)\n",
      "pred pont\n",
      "(1355.64318848, 626.06134033, True, False, 0.87077147)\n",
      "pred pont\n",
      "(1403.18261719, 708.16461182, True, False, 0.74706167)\n",
      "pred pont\n",
      "(1369.47912598, 588.89434814, True, False, 0.70457816)\n",
      "pred pont\n",
      "(1309.29907227, 625.16790771, True, False, 0.70542377)\n",
      "pred pont\n",
      "(889.71813965, 863.96063232, True, False, 0.93167716)\n",
      "pred pont\n",
      "(973.6494751, 1129.56616211, True, False, 0.97070163)\n",
      "pred pont\n",
      "(949.57830811, 1057.6270752, True, False, 0.8502866)\n",
      "pred pont\n",
      "(948.00610352, 1187.77697754, True, False, 0.60920471)\n",
      "pred pont\n",
      "(1031.30688477, 1154.95825195, True, False, 0.6026507)\n",
      "pred pont\n",
      "(1285.05065918, 1033.22937012, True, False, 0.90833139)\n",
      "pred pont\n",
      "(1344.89672852, 816.62109375, True, False, 0.93693137)\n",
      "pred pont\n",
      "(1320.22668457, 912.47387695, True, False, 0.84590775)\n",
      "pred pont\n",
      "(1390.17871094, 804.96240234, True, False, 0.72259867)\n",
      "pred pont\n",
      "(1308.38378906, 768.85198975, True, False, 0.6119864)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "filename = 'sleap_bottomup.slp'\n",
    "with h5py.File(filename, 'r') as f:\n",
    "    # List all groups\n",
    "    print('Keys: %s' % f.keys())\n",
    "    a_group_key = list(f.keys())\n",
    "    print('a_group_key:', a_group_key)\n",
    "    # Get the datapo\n",
    "    frames = f['frames'][()]\n",
    "    instances = f['instances'][()]\n",
    "    points = f['points'][()]\n",
    "    pred_points = f['pred_points'][()]\n",
    "\n",
    "for f in frames:\n",
    "    print('frames')\n",
    "    print(f)\n",
    "    \n",
    "for f in instances:\n",
    "    print('ints')\n",
    "    print(f)\n",
    "    \n",
    "for f in points:\n",
    "    print('ponts')\n",
    "    print(f)\n",
    "    \n",
    "for f in pred_points:\n",
    "    print('pred pont')\n",
    "    print(f)\n",
    "    \n",
    "print(frames[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before\n",
      "nc_250_10.py\n",
      "combined\n",
      "version:  0\n",
      "alpha:  .5\n",
      "loss type:  combined\n",
      ".5\n",
      "250\n",
      "after\n",
      "2021-05-27 06:46:02,068 maskrcnn_benchmark INFO: Using 1 GPUs\n",
      "2021-05-27 06:46:02,068 maskrcnn_benchmark INFO: AMP_VERBOSE: False\n",
      "DATALOADER:\n",
      "  ASPECT_RATIO_GROUPING: True\n",
      "  NUM_WORKERS: 2\n",
      "  SIZE_DIVISIBILITY: 32\n",
      "DATASETS:\n",
      "  TEST: ('keypoints_coco_2014_minival',)\n",
      "  TRAIN: ('bee_train_cocostyle_small', 'bee_train_cocostyle_small')\n",
      "DATATYPE: bee\n",
      "DTYPE: float32\n",
      "INPUT:\n",
      "  BRIGHTNESS: 0.0\n",
      "  CONTRAST: 0.0\n",
      "  CROP_SIZE: 800\n",
      "  FLIP_PROB_TRAIN: 0.5\n",
      "  HUE: 0.0\n",
      "  MAX_SIZE_TEST: 1333\n",
      "  MAX_SIZE_TRAIN: 2666\n",
      "  MIN_SIZE_RANGE_TRAIN: (480, 1600)\n",
      "  MIN_SIZE_TEST: 800\n",
      "  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)\n",
      "  PIXEL_MEAN: [102.9801, 115.9465, 122.7717]\n",
      "  PIXEL_STD: [1.0, 1.0, 1.0]\n",
      "  SATURATION: 0.0\n",
      "  TO_BGR255: True\n",
      "MODEL:\n",
      "  ANIMAL_ON: False\n",
      "  BACKBONE:\n",
      "    CONV_BODY: R-50-FPN-RETINANET\n",
      "    FREEZE_CONV_BODY_AT: 2\n",
      "    USE_GN: False\n",
      "  CENTROID: False\n",
      "  CLS_AGNOSTIC_BBOX_REG: False\n",
      "  DEVICE: cuda\n",
      "  FBNET:\n",
      "    ARCH: default\n",
      "    ARCH_DEF: \n",
      "    BN_TYPE: bn\n",
      "    DET_HEAD_BLOCKS: []\n",
      "    DET_HEAD_LAST_SCALE: 1.0\n",
      "    DET_HEAD_STRIDE: 0\n",
      "    DW_CONV_SKIP_BN: True\n",
      "    DW_CONV_SKIP_RELU: True\n",
      "    KPTS_HEAD_BLOCKS: []\n",
      "    KPTS_HEAD_LAST_SCALE: 0.0\n",
      "    KPTS_HEAD_STRIDE: 0\n",
      "    MASK_HEAD_BLOCKS: []\n",
      "    MASK_HEAD_LAST_SCALE: 0.0\n",
      "    MASK_HEAD_STRIDE: 0\n",
      "    RPN_BN_TYPE: \n",
      "    RPN_HEAD_BLOCKS: 0\n",
      "    SCALE_FACTOR: 1.0\n",
      "    WIDTH_DIVISOR: 1\n",
      "  FCOS:\n",
      "    FPN_STRIDES: [8, 16, 32, 64, 128]\n",
      "    INFERENCE_TH: 0.05\n",
      "    LOSS_ALPHA: 0.25\n",
      "    LOSS_GAMMA: 2.0\n",
      "    NMS_TH: 0.4\n",
      "    NUM_CLASSES: 2\n",
      "    NUM_CONVS: 4\n",
      "    PRE_NMS_TOP_N: 1000\n",
      "    PRIOR_PROB: 0.01\n",
      "  FCOS_ON: True\n",
      "  FPN:\n",
      "    USE_GN: False\n",
      "    USE_RELU: False\n",
      "  GROUP_NORM:\n",
      "    DIM_PER_GP: -1\n",
      "    EPSILON: 1e-05\n",
      "    NUM_GROUPS: 32\n",
      "  HEATMAPS_LOSS_WEIGHT: 4.0\n",
      "  KEYPOINT_ON: True\n",
      "  MASK_ON: False\n",
      "  META_ARCHITECTURE: GeneralizedRCNN\n",
      "  RESNETS:\n",
      "    BACKBONE_OUT_CHANNELS: 256\n",
      "    DEFORMABLE_GROUPS: 1\n",
      "    NUM_GROUPS: 1\n",
      "    RES2_OUT_CHANNELS: 256\n",
      "    RES5_DILATION: 1\n",
      "    STAGE_WITH_DCN: (False, False, False, False)\n",
      "    STEM_FUNC: StemWithFixedBatchNorm\n",
      "    STEM_OUT_CHANNELS: 64\n",
      "    STRIDE_IN_1X1: True\n",
      "    TRANS_FUNC: BottleneckWithFixedBatchNorm\n",
      "    WIDTH_PER_GROUP: 64\n",
      "    WITH_MODULATED_DCN: False\n",
      "  RETINANET:\n",
      "    ANCHOR_SIZES: (32, 64, 128, 256, 512)\n",
      "    ANCHOR_STRIDES: (8, 16, 32, 64, 128)\n",
      "    ASPECT_RATIOS: (0.5, 1.0, 2.0)\n",
      "    BBOX_REG_BETA: 0.11\n",
      "    BBOX_REG_WEIGHT: 4.0\n",
      "    BG_IOU_THRESHOLD: 0.4\n",
      "    FG_IOU_THRESHOLD: 0.5\n",
      "    INFERENCE_TH: 0.05\n",
      "    LOSS_ALPHA: 0.25\n",
      "    LOSS_GAMMA: 2.0\n",
      "    NMS_TH: 0.4\n",
      "    NUM_CLASSES: 81\n",
      "    NUM_CONVS: 4\n",
      "    OCTAVE: 2.0\n",
      "    PRE_NMS_TOP_N: 1000\n",
      "    PRIOR_PROB: 0.01\n",
      "    SCALES_PER_OCTAVE: 3\n",
      "    STRADDLE_THRESH: 0\n",
      "    USE_C5: False\n",
      "  RETINANET_ON: False\n",
      "  ROI_BOX_HEAD:\n",
      "    CONV_HEAD_DIM: 256\n",
      "    DILATION: 1\n",
      "    FEATURE_EXTRACTOR: ResNet50Conv5ROIFeatureExtractor\n",
      "    MLP_HEAD_DIM: 1024\n",
      "    NUM_CLASSES: 81\n",
      "    NUM_STACKED_CONVS: 4\n",
      "    POOLER_RESOLUTION: 14\n",
      "    POOLER_SAMPLING_RATIO: 0\n",
      "    POOLER_SCALES: (0.0625,)\n",
      "    PREDICTOR: FastRCNNPredictor\n",
      "    USE_GN: False\n",
      "  ROI_HEADS:\n",
      "    BATCH_SIZE_PER_IMAGE: 1\n",
      "    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)\n",
      "    BG_IOU_THRESHOLD: 0.5\n",
      "    DETECTIONS_PER_IMG: 100\n",
      "    FG_IOU_THRESHOLD: 0.5\n",
      "    NMS: 0.5\n",
      "    POSITIVE_FRACTION: 0.25\n",
      "    SCORE_THRESH: 0.05\n",
      "    USE_FPN: False\n",
      "  ROI_KEYPOINT_HEAD:\n",
      "    CONV_LAYERS: (512, 512, 512, 512, 512, 512, 512, 512)\n",
      "    FEATURE_EXTRACTOR: KeypointRCNNFeatureExtractor\n",
      "    MLP_HEAD_DIM: 1024\n",
      "    NUM_CLASSES: 17\n",
      "    POOLER_RESOLUTION: 14\n",
      "    POOLER_SAMPLING_RATIO: 0\n",
      "    POOLER_SCALES: (0.0625,)\n",
      "    PREDICTOR: KeypointRCNNPredictor\n",
      "    RESOLUTION: 14\n",
      "    SHARE_BOX_FEATURE_EXTRACTOR: True\n",
      "  ROI_MASK_HEAD:\n",
      "    CONV_LAYERS: (256, 256, 256, 256)\n",
      "    DILATION: 1\n",
      "    FEATURE_EXTRACTOR: ResNet50Conv5ROIFeatureExtractor\n",
      "    MLP_HEAD_DIM: 1024\n",
      "    POOLER_RESOLUTION: 14\n",
      "    POOLER_SAMPLING_RATIO: 0\n",
      "    POOLER_SCALES: (0.0625,)\n",
      "    POSTPROCESS_MASKS: False\n",
      "    POSTPROCESS_MASKS_THRESHOLD: 0.5\n",
      "    PREDICTOR: MaskRCNNC4Predictor\n",
      "    RESOLUTION: 14\n",
      "    SHARE_BOX_FEATURE_EXTRACTOR: True\n",
      "    USE_GN: False\n",
      "  RPN:\n",
      "    ANCHOR_SIZES: (32, 64, 128, 256, 512)\n",
      "    ANCHOR_STRIDE: (16,)\n",
      "    ASPECT_RATIOS: (0.5, 1.0, 2.0)\n",
      "    BATCH_SIZE_PER_IMAGE: 1\n",
      "    BG_IOU_THRESHOLD: 0.3\n",
      "    FG_IOU_THRESHOLD: 0.7\n",
      "    FPN_POST_NMS_PER_BATCH: True\n",
      "    FPN_POST_NMS_TOP_N_TEST: 2000\n",
      "    FPN_POST_NMS_TOP_N_TRAIN: 2000\n",
      "    MIN_SIZE: 0\n",
      "    NMS_THRESH: 0.7\n",
      "    POSITIVE_FRACTION: 0.5\n",
      "    POST_NMS_TOP_N_TEST: 1000\n",
      "    POST_NMS_TOP_N_TRAIN: 2000\n",
      "    PRE_NMS_TOP_N_TEST: 6000\n",
      "    PRE_NMS_TOP_N_TRAIN: 12000\n",
      "    RPN_HEAD: SingleConvRPNHead\n",
      "    STRADDLE_THRESH: 0\n",
      "    USE_FPN: False\n",
      "  RPN_ONLY: True\n",
      "  WEIGHT: training_dir/250/standard_res0/0/fcos_kps_ms_training_R_50_FPN_1x_bee/model_0002000.pth\n",
      "OUTPUT_DIR: training_dir/250/newloss/p2combined_res/0/fcos_kps_ms_training_R_50_FPN_1x_bee\n",
      "PATHS_CATALOG: /share/ctn/users/bsb2144/directpose/maskrcnn_benchmark/config/paths_catalog.py\n",
      "SOLVER:\n",
      "  BASE_LR: 0.01\n",
      "  BIAS_LR_FACTOR: 2\n",
      "  CHECKPOINT_PERIOD: 2500\n",
      "  GAMMA: 0.1\n",
      "  IMS_PER_BATCH: 3\n",
      "  KPS_GRAD_MULT: 10.0\n",
      "  MAX_GRAD_NORM: 5.0\n",
      "  MAX_ITER: 180000\n",
      "  MOMENTUM: 0.9\n",
      "  POWER: 1.0\n",
      "  STEPS: (60000, 80000)\n",
      "  WARMUP_FACTOR: 0.3333333333333333\n",
      "  WARMUP_ITERS: 500\n",
      "  WARMUP_METHOD: constant\n",
      "  WEIGHT_DECAY: 0.0001\n",
      "  WEIGHT_DECAY_BIAS: 0\n",
      "TEST:\n",
      "  DETECTIONS_PER_IMG: 100\n",
      "  EXPECTED_RESULTS: []\n",
      "  EXPECTED_RESULTS_SIGMA_TOL: 4\n",
      "  IMS_PER_BATCH: 8\n",
      "2021-05-27 06:46:02,070 maskrcnn_benchmark INFO: Collecting env info (might take some time)\n",
      "2021-05-27 06:46:04,746 maskrcnn_benchmark INFO: \n",
      "PyTorch version: 1.0.0.dev20190328\n",
      "Is debug build: No\n",
      "CUDA used to build PyTorch: 10.0.130\n",
      "\n",
      "OS: CentOS Linux 7 (Core)\n",
      "GCC version: (GCC) 5.2.0\n",
      "CMake version: Could not collect\n",
      "\n",
      "Python version: 3.7\n",
      "Is CUDA available: Yes\n",
      "CUDA runtime version: Could not collect\n",
      "GPU models and configuration: GPU 0: GeForce GTX 1080 Ti\n",
      "Nvidia driver version: 460.27.04\n",
      "cuDNN version: Could not collect\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] numpy==1.19.2\n",
      "[pip3] torch==1.0.0.dev20190328\n",
      "[pip3] torchvision==0.2.2\n",
      "[conda] blas                      1.0                         mkl  \n",
      "[conda] mkl                       2020.2                      256  \n",
      "[conda] mkl-service               2.3.0            py37he8ac12f_0  \n",
      "[conda] mkl_fft                   1.2.0            py37h23d657b_0  \n",
      "[conda] mkl_random                1.1.1            py37h0573a6f_0  \n",
      "[conda] pytorch-nightly           1.0.0.dev20190328 py3.7_cuda10.0.130_cudnn7.4.2_0    pytorch\n",
      "[conda] torchvision               0.2.2                    pypi_0    pypi\n",
      "        Pillow (8.0.1)\n",
      "2021-05-27 06:46:04,747 maskrcnn_benchmark INFO: Loaded configuration file /home/bsb2144/directpose/configs/fcos/fcos_kps_ms_training_R_50_FPN_1x.yaml\n",
      "2021-05-27 06:46:04,747 maskrcnn_benchmark INFO: \n",
      "MODEL:\n",
      "  META_ARCHITECTURE: \"GeneralizedRCNN\"\n",
      "  WEIGHT: \"catalog://ImageNetPretrained/MSRA/R-50\"\n",
      "  RPN_ONLY: True\n",
      "  FCOS_ON: True\n",
      "  KEYPOINT_ON: True\n",
      "  BACKBONE:\n",
      "    CONV_BODY: \"R-50-FPN-RETINANET\"\n",
      "  RESNETS:\n",
      "    BACKBONE_OUT_CHANNELS: 256\n",
      "    STAGE_WITH_DCN: (False, False, False, False)\n",
      "    WITH_MODULATED_DCN: False\n",
      "    DEFORMABLE_GROUPS: 1\n",
      "  RETINANET:\n",
      "    USE_C5: False # FCOS uses P5 instead of C5\n",
      "  FCOS:\n",
      "    NUM_CLASSES: 2\n",
      "DATASETS:\n",
      "  TRAIN: (\"keypoints_coco_2014_train\", \"keypoints_coco_2014_valminusminival\")\n",
      "  TEST: (\"keypoints_coco_2014_minival\",)\n",
      "INPUT:\n",
      "  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)\n",
      "  MAX_SIZE_TRAIN: 1333\n",
      "  MIN_SIZE_TEST: 800\n",
      "  MAX_SIZE_TEST: 1333\n",
      "DATALOADER:\n",
      "  SIZE_DIVISIBILITY: 32\n",
      "SOLVER:\n",
      "  BASE_LR: 0.01\n",
      "  WEIGHT_DECAY: 0.0001\n",
      "  STEPS: (60000, 80000)\n",
      "  MAX_ITER: 90000\n",
      "  IMS_PER_BATCH: 16\n",
      "  WARMUP_METHOD: \"constant\"\n",
      "\n",
      "2021-05-27 06:46:04,748 maskrcnn_benchmark INFO: Running with config:\n",
      "AMP_VERBOSE: False\n",
      "DATALOADER:\n",
      "  ASPECT_RATIO_GROUPING: True\n",
      "  NUM_WORKERS: 2\n",
      "  SIZE_DIVISIBILITY: 32\n",
      "DATASETS:\n",
      "  TEST: ('keypoints_coco_2014_minival',)\n",
      "  TRAIN: ('bee_train_cocostyle_small', 'bee_train_cocostyle_small')\n",
      "DATATYPE: bee\n",
      "DTYPE: float32\n",
      "INPUT:\n",
      "  BRIGHTNESS: 0.0\n",
      "  CONTRAST: 0.0\n",
      "  CROP_SIZE: 800\n",
      "  FLIP_PROB_TRAIN: 0.5\n",
      "  HUE: 0.0\n",
      "  MAX_SIZE_TEST: 1333\n",
      "  MAX_SIZE_TRAIN: 2666\n",
      "  MIN_SIZE_RANGE_TRAIN: (480, 1600)\n",
      "  MIN_SIZE_TEST: 800\n",
      "  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)\n",
      "  PIXEL_MEAN: [102.9801, 115.9465, 122.7717]\n",
      "  PIXEL_STD: [1.0, 1.0, 1.0]\n",
      "  SATURATION: 0.0\n",
      "  TO_BGR255: True\n",
      "MODEL:\n",
      "  ANIMAL_ON: False\n",
      "  BACKBONE:\n",
      "    CONV_BODY: R-50-FPN-RETINANET\n",
      "    FREEZE_CONV_BODY_AT: 2\n",
      "    USE_GN: False\n",
      "  CENTROID: False\n",
      "  CLS_AGNOSTIC_BBOX_REG: False\n",
      "  DEVICE: cuda\n",
      "  FBNET:\n",
      "    ARCH: default\n",
      "    ARCH_DEF: \n",
      "    BN_TYPE: bn\n",
      "    DET_HEAD_BLOCKS: []\n",
      "    DET_HEAD_LAST_SCALE: 1.0\n",
      "    DET_HEAD_STRIDE: 0\n",
      "    DW_CONV_SKIP_BN: True\n",
      "    DW_CONV_SKIP_RELU: True\n",
      "    KPTS_HEAD_BLOCKS: []\n",
      "    KPTS_HEAD_LAST_SCALE: 0.0\n",
      "    KPTS_HEAD_STRIDE: 0\n",
      "    MASK_HEAD_BLOCKS: []\n",
      "    MASK_HEAD_LAST_SCALE: 0.0\n",
      "    MASK_HEAD_STRIDE: 0\n",
      "    RPN_BN_TYPE: \n",
      "    RPN_HEAD_BLOCKS: 0\n",
      "    SCALE_FACTOR: 1.0\n",
      "    WIDTH_DIVISOR: 1\n",
      "  FCOS:\n",
      "    FPN_STRIDES: [8, 16, 32, 64, 128]\n",
      "    INFERENCE_TH: 0.05\n",
      "    LOSS_ALPHA: 0.25\n",
      "    LOSS_GAMMA: 2.0\n",
      "    NMS_TH: 0.4\n",
      "    NUM_CLASSES: 2\n",
      "    NUM_CONVS: 4\n",
      "    PRE_NMS_TOP_N: 1000\n",
      "    PRIOR_PROB: 0.01\n",
      "  FCOS_ON: True\n",
      "  FPN:\n",
      "    USE_GN: False\n",
      "    USE_RELU: False\n",
      "  GROUP_NORM:\n",
      "    DIM_PER_GP: -1\n",
      "    EPSILON: 1e-05\n",
      "    NUM_GROUPS: 32\n",
      "  HEATMAPS_LOSS_WEIGHT: 4.0\n",
      "  KEYPOINT_ON: True\n",
      "  MASK_ON: False\n",
      "  META_ARCHITECTURE: GeneralizedRCNN\n",
      "  RESNETS:\n",
      "    BACKBONE_OUT_CHANNELS: 256\n",
      "    DEFORMABLE_GROUPS: 1\n",
      "    NUM_GROUPS: 1\n",
      "    RES2_OUT_CHANNELS: 256\n",
      "    RES5_DILATION: 1\n",
      "    STAGE_WITH_DCN: (False, False, False, False)\n",
      "    STEM_FUNC: StemWithFixedBatchNorm\n",
      "    STEM_OUT_CHANNELS: 64\n",
      "    STRIDE_IN_1X1: True\n",
      "    TRANS_FUNC: BottleneckWithFixedBatchNorm\n",
      "    WIDTH_PER_GROUP: 64\n",
      "    WITH_MODULATED_DCN: False\n",
      "  RETINANET:\n",
      "    ANCHOR_SIZES: (32, 64, 128, 256, 512)\n",
      "    ANCHOR_STRIDES: (8, 16, 32, 64, 128)\n",
      "    ASPECT_RATIOS: (0.5, 1.0, 2.0)\n",
      "    BBOX_REG_BETA: 0.11\n",
      "    BBOX_REG_WEIGHT: 4.0\n",
      "    BG_IOU_THRESHOLD: 0.4\n",
      "    FG_IOU_THRESHOLD: 0.5\n",
      "    INFERENCE_TH: 0.05\n",
      "    LOSS_ALPHA: 0.25\n",
      "    LOSS_GAMMA: 2.0\n",
      "    NMS_TH: 0.4\n",
      "    NUM_CLASSES: 81\n",
      "    NUM_CONVS: 4\n",
      "    OCTAVE: 2.0\n",
      "    PRE_NMS_TOP_N: 1000\n",
      "    PRIOR_PROB: 0.01\n",
      "    SCALES_PER_OCTAVE: 3\n",
      "    STRADDLE_THRESH: 0\n",
      "    USE_C5: False\n",
      "  RETINANET_ON: False\n",
      "  ROI_BOX_HEAD:\n",
      "    CONV_HEAD_DIM: 256\n",
      "    DILATION: 1\n",
      "    FEATURE_EXTRACTOR: ResNet50Conv5ROIFeatureExtractor\n",
      "    MLP_HEAD_DIM: 1024\n",
      "    NUM_CLASSES: 81\n",
      "    NUM_STACKED_CONVS: 4\n",
      "    POOLER_RESOLUTION: 14\n",
      "    POOLER_SAMPLING_RATIO: 0\n",
      "    POOLER_SCALES: (0.0625,)\n",
      "    PREDICTOR: FastRCNNPredictor\n",
      "    USE_GN: False\n",
      "  ROI_HEADS:\n",
      "    BATCH_SIZE_PER_IMAGE: 1\n",
      "    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)\n",
      "    BG_IOU_THRESHOLD: 0.5\n",
      "    DETECTIONS_PER_IMG: 100\n",
      "    FG_IOU_THRESHOLD: 0.5\n",
      "    NMS: 0.5\n",
      "    POSITIVE_FRACTION: 0.25\n",
      "    SCORE_THRESH: 0.05\n",
      "    USE_FPN: False\n",
      "  ROI_KEYPOINT_HEAD:\n",
      "    CONV_LAYERS: (512, 512, 512, 512, 512, 512, 512, 512)\n",
      "    FEATURE_EXTRACTOR: KeypointRCNNFeatureExtractor\n",
      "    MLP_HEAD_DIM: 1024\n",
      "    NUM_CLASSES: 17\n",
      "    POOLER_RESOLUTION: 14\n",
      "    POOLER_SAMPLING_RATIO: 0\n",
      "    POOLER_SCALES: (0.0625,)\n",
      "    PREDICTOR: KeypointRCNNPredictor\n",
      "    RESOLUTION: 14\n",
      "    SHARE_BOX_FEATURE_EXTRACTOR: True\n",
      "  ROI_MASK_HEAD:\n",
      "    CONV_LAYERS: (256, 256, 256, 256)\n",
      "    DILATION: 1\n",
      "    FEATURE_EXTRACTOR: ResNet50Conv5ROIFeatureExtractor\n",
      "    MLP_HEAD_DIM: 1024\n",
      "    POOLER_RESOLUTION: 14\n",
      "    POOLER_SAMPLING_RATIO: 0\n",
      "    POOLER_SCALES: (0.0625,)\n",
      "    POSTPROCESS_MASKS: False\n",
      "    POSTPROCESS_MASKS_THRESHOLD: 0.5\n",
      "    PREDICTOR: MaskRCNNC4Predictor\n",
      "    RESOLUTION: 14\n",
      "    SHARE_BOX_FEATURE_EXTRACTOR: True\n",
      "    USE_GN: False\n",
      "  RPN:\n",
      "    ANCHOR_SIZES: (32, 64, 128, 256, 512)\n",
      "    ANCHOR_STRIDE: (16,)\n",
      "    ASPECT_RATIOS: (0.5, 1.0, 2.0)\n",
      "    BATCH_SIZE_PER_IMAGE: 1\n",
      "    BG_IOU_THRESHOLD: 0.3\n",
      "    FG_IOU_THRESHOLD: 0.7\n",
      "    FPN_POST_NMS_PER_BATCH: True\n",
      "    FPN_POST_NMS_TOP_N_TEST: 2000\n",
      "    FPN_POST_NMS_TOP_N_TRAIN: 2000\n",
      "    MIN_SIZE: 0\n",
      "    NMS_THRESH: 0.7\n",
      "    POSITIVE_FRACTION: 0.5\n",
      "    POST_NMS_TOP_N_TEST: 1000\n",
      "    POST_NMS_TOP_N_TRAIN: 2000\n",
      "    PRE_NMS_TOP_N_TEST: 6000\n",
      "    PRE_NMS_TOP_N_TRAIN: 12000\n",
      "    RPN_HEAD: SingleConvRPNHead\n",
      "    STRADDLE_THRESH: 0\n",
      "    USE_FPN: False\n",
      "  RPN_ONLY: True\n",
      "  WEIGHT: training_dir/250/standard_res0/0/fcos_kps_ms_training_R_50_FPN_1x_bee/model_0002000.pth\n",
      "OUTPUT_DIR: training_dir/250/newloss/p2combined_res/0/fcos_kps_ms_training_R_50_FPN_1x_bee\n",
      "PATHS_CATALOG: /share/ctn/users/bsb2144/directpose/maskrcnn_benchmark/config/paths_catalog.py\n",
      "SOLVER:\n",
      "  BASE_LR: 0.01\n",
      "  BIAS_LR_FACTOR: 2\n",
      "  CHECKPOINT_PERIOD: 2500\n",
      "  GAMMA: 0.1\n",
      "  IMS_PER_BATCH: 3\n",
      "  KPS_GRAD_MULT: 10.0\n",
      "  MAX_GRAD_NORM: 5.0\n",
      "  MAX_ITER: 180000\n",
      "  MOMENTUM: 0.9\n",
      "  POWER: 1.0\n",
      "  STEPS: (60000, 80000)\n",
      "  WARMUP_FACTOR: 0.3333333333333333\n",
      "  WARMUP_ITERS: 500\n",
      "  WARMUP_METHOD: constant\n",
      "  WEIGHT_DECAY: 0.0001\n",
      "  WEIGHT_DECAY_BIAS: 0\n",
      "TEST:\n",
      "  DETECTIONS_PER_IMG: 100\n",
      "  EXPECTED_RESULTS: []\n",
      "  EXPECTED_RESULTS_SIGMA_TOL: 4\n",
      "  IMS_PER_BATCH: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.cls_tower.0.weight\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.cls_tower.0.bias\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.cls_tower.1.weight\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.cls_tower.1.bias\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.cls_tower.3.weight\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.cls_tower.3.bias\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.cls_tower.4.weight\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.cls_tower.4.bias\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.cls_tower.6.weight\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.cls_tower.6.bias\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.cls_tower.7.weight\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.cls_tower.7.bias\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.cls_tower.9.weight\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.cls_tower.9.bias\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.cls_tower.10.weight\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.cls_tower.10.bias\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.kps_tower.0.weight\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.kps_tower.0.bias\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.kps_tower.1.weight\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.kps_tower.1.bias\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.kps_tower.3.weight\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.kps_tower.3.bias\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.kps_tower.4.weight\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.kps_tower.4.bias\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.kps_tower.6.weight\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.kps_tower.6.bias\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.kps_tower.7.weight\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.kps_tower.7.bias\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.kps_tower.9.weight\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.kps_tower.9.bias\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.kps_tower.10.weight\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.kps_tower.10.bias\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.kps_bases.weight\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.kps_bases.bias\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.sample_features_conv.0.weight\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.sample_features_conv.0.bias\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.sample_features_conv.1.weight\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.sample_features_conv.1.bias\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.kps_offsets.0.kps_offsets.weight\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.kps_offsets.0.kps_offsets.bias\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.cls_logits.weight\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.cls_logits.bias\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.centerness.weight\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to rpn.head.centerness.bias\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to heatmaps.deeplab.seg_head.0.weight\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to heatmaps.deeplab.seg_head.1.weight\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to heatmaps.deeplab.seg_head.1.bias\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to heatmaps.deeplab.seg_head.3.weight\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to heatmaps.deeplab.seg_head.4.weight\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to heatmaps.deeplab.seg_head.4.bias\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to heatmaps.deeplab.heatmaps.weight\r\n",
      "in starts with\r\n",
      "apply SOLVER.KPS_GRAD_MULT to heatmaps.deeplab.heatmaps.bias\r\n",
      "IN LR SCHEDULER\r\n",
      "2021-05-27 06:46:07,917 maskrcnn_benchmark.utils.checkpoint INFO: Loading checkpoint from training_dir/250/standard_res0/0/fcos_kps_ms_training_R_50_FPN_1x_bee/model_0002000.pth\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"nc_250_10.py\", line 197, in <module>\r\n",
      "    model = train(cfg, local_rank, distributed, loss_type, alpha,version)\r\n",
      "  File \"nc_250_10.py\", line 64, in train\r\n",
      "    extra_checkpoint_data = checkpointer.load(cfg.MODEL.WEIGHT)\r\n",
      "  File \"/share/ctn/users/bsb2144/directpose/maskrcnn_benchmark/utils/checkpoint.py\", line 62, in load\r\n",
      "    checkpoint = self._load_file(f)\r\n",
      "  File \"/share/ctn/users/bsb2144/directpose/maskrcnn_benchmark/utils/checkpoint.py\", line 142, in _load_file\r\n",
      "    loaded = super(DetectronCheckpointer, self)._load_file(f)\r\n",
      "  File \"/share/ctn/users/bsb2144/directpose/maskrcnn_benchmark/utils/checkpoint.py\", line 101, in _load_file\r\n",
      "    return torch.load(f, map_location=torch.device(\"cpu\"))\r\n",
      "  File \"/home/bsb2144/miniconda3/envs/directpose/lib/python3.7/site-packages/torch/serialization.py\", line 383, in load\r\n",
      "    f = open(f, 'rb')\r\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'training_dir/250/standard_res0/0/fcos_kps_ms_training_R_50_FPN_1x_bee/model_0002000.pth'\r\n"
     ]
    }
   ],
   "source": [
    "# run with scores\n",
    "alphas = [0.0001, 0.001, 0.01, 0.1, 0.5, 1]\n",
    "#for alpha in alphas:\n",
    "!python nc_250_10.py \"combined\" \"250\" \"0\" \".5\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.9.2\n",
      "  latest version: 4.10.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/bsb2144/miniconda3/envs/directpose\n",
      "\n",
      "  added / updated specs:\n",
      "    - h5py\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    cached-property-1.5.2      |             py_0          11 KB\n",
      "    h5py-3.2.1                 |   py37h6c542dc_0         1.0 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         1.0 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  cached-property    pkgs/main/noarch::cached-property-1.5.2-py_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  h5py                                2.10.0-py37hd6299e0_1 --> 3.2.1-py37h6c542dc_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "h5py-3.2.1           | 1.0 MB    | ##################################### | 100% \n",
      "cached-property-1.5. | 11 KB     | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "#!ml load cuda/10.0.130\n",
    "#!nvidia-smi\n",
    "#!export PYTORCH_NO_CUDA_MEMORY_CACHING=1\n",
    "!conda install --yes h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"/home/bsb2144/directpose/configs/fcos/fcos_kps_ms_training_R_50_FPN_1x.yaml\"\n",
    "\n",
    "cfg.merge_from_file(\"/home/bsb2144/directpose/configs/fcos/fcos_kps_ms_training_R_50_FPN_1x.yaml\")\n",
    "cfg.merge_from_list(['DATALOADER.NUM_WORKERS', '2', \\\n",
    "                     'DATATYPE', 'bee', \\\n",
    "                     'OUTPUT_DIR', 'training_dir_v2/fcos_kps_ms_training_R_50_FPN_1x_bee1', \\\n",
    "                     'MODEL.WEIGHT', '/home/bsb2144/directpose/tools/training_dir_v2/fcos_kps_ms_training_R_50_FPN_1x_bee1/model_final.pth',\\\n",
    "                     'DATASETS.TEST', \"('bee_val_cocostyle',)\",\\\n",
    "                     'DATASETS.TRAIN', \"('bee_train_cocostyle', )\",\\\n",
    "                    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
